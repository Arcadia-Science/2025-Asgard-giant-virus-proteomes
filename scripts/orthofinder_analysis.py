#!/usr/bin/env python3

"""
Analyzes OrthoFinder results to provide summary statistics, identify conserved
orthogroups (both hypothetical and annotated), generate basic visualizations,
and optionally incorporate phylum-level analysis.

This script integrates OrthoFinder output with metadata parsed from the input
FASTA files used for the OrthoFinder run and an optional phylum mapping file.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import sys
import argparse
import logging
import re
import glob
from pathlib import Path
from Bio import SeqIO

# --- Setup Logging ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')

# --- Helper Functions ---

def parse_fasta_header(record, default_phylum='UnknownPhylum'):
    """
    Parses protein metadata from a Biopython SeqRecord header.

    *** EDIT THIS FUNCTION *** if your FASTA header format differs significantly
    between datasets (e.g., Asgard vs. GV).

    Tries to parse common NCBI formats and the custom format generated by
    `filter_proteomes.py`: >ProteinID|GenomeAssemblyID|Phylum|AnnotationType|CleanedName

    Args:
        record (SeqRecord): A Biopython SeqRecord object.
        default_phylum (str): Default value if phylum cannot be parsed.

    Returns:
        dict: A dictionary containing parsed metadata fields
              ('ProteinID', 'GenomeID', 'Phylum', 'AnnotationType', 'CleanedAnnotationName'),
              or None if parsing fails critically.
    """
    protein_id = 'UnknownID'; genome_id = 'UnknownGenome'; phylum = default_phylum
    annotation_type = 'hypothetical'; cleaned_annotation = None
    description = record.description # Full description line
    header_id = record.id # Part before first space

    try:
        # Attempt to parse custom format first
        parts = header_id.split('|')
        if len(parts) >= 5: # >ProteinID|GenomeID|Phylum|AnnotationType|CleanedName
            protein_id = parts[0]
            genome_id = parts[1]
            phylum = parts[2] if parts[2] else default_phylum
            annotation_type = parts[3].lower() if parts[3] else 'hypothetical'
            cleaned_annotation = parts[4] if parts[4] and annotation_type == 'annotated' else None
            # Basic validation on type
            if annotation_type not in ['annotated', 'hypothetical']:
                 annotation_type = 'hypothetical'
                 cleaned_annotation = None # Assume hypothetical if type is weird

        elif len(parts) >= 4: # >ProteinID|GenomeID|Phylum|AnnotationType (Name missing)
            protein_id = parts[0]
            genome_id = parts[1]
            phylum = parts[2] if parts[2] else default_phylum
            annotation_type = parts[3].lower() if parts[3] else 'hypothetical'
            if annotation_type == 'annotated':
                # Try to parse name from description if type is annotated but name missing
                desc_only = description.replace(header_id, '', 1).strip()
                cleaned_annotation = clean_annotation_name(desc_only) # Use helper for cleaning
                if cleaned_annotation is None: annotation_type = 'hypothetical' # Revert if parsing fails
            else:
                 cleaned_annotation = None
                 if annotation_type not in ['annotated', 'hypothetical']: annotation_type = 'hypothetical'

        else: # Fallback for simpler NCBI-like headers >ProteinID Description
            protein_id = header_id
            desc_only = description.replace(header_id, '', 1).strip()
            cleaned_annotation = clean_annotation_name(desc_only)
            # Infer type based on common keywords in description
            generic_terms = r'(hypothetical|unknown|predicted|uncharacterized|domain_of_unknown_function)'
            if re.search(generic_terms, desc_only, flags=re.IGNORECASE):
                annotation_type = 'hypothetical'
                cleaned_annotation = None
            elif cleaned_annotation:
                annotation_type = 'annotated'
            else:
                annotation_type = 'hypothetical'
                cleaned_annotation = None
            # GenomeID and Phylum remain defaults unless overridden by map later

        # Final check: ensure name is None if type is hypothetical
        if annotation_type == 'hypothetical':
            cleaned_annotation = None

        return {
            'ProteinID': protein_id,
            'GenomeID': genome_id, # May be 'UnknownGenome' in fallback
            'Phylum': phylum, # May be default_phylum
            'AnnotationType': annotation_type,
            'CleanedAnnotationName': cleaned_annotation
        }

    except Exception as e:
        logging.warning(f"Could not parse header for record starting with '{record.id[:50]}...': {e}")
        # Return defaults on error
        return {
            'ProteinID': header_id, 'GenomeID': 'ParseError', 'Phylum': default_phylum,
            'AnnotationType': 'hypothetical', 'CleanedAnnotationName': None
        }

def clean_annotation_name(annotation_string):
    """Cleans a raw annotation string."""
    if pd.isnull(annotation_string) or not isinstance(annotation_string, str): return None
    cleaned = annotation_string.strip()
    cleaned = re.sub(r'\\s*\\\[.*?\\\]\\s*', '', cleaned).strip() # Remove bracketed text
    cleaned = re.sub(r'[\\s,;()\\[\\]{}:/\\\\]+', '_', cleaned)
    cleaned = re.sub(r'[^a-zA-Z0-9_.-]', '', cleaned)
    cleaned = re.sub(r'_+', '_', cleaned).strip('_')
    if not cleaned: return None
    # Check against generic terms AFTER cleaning
    generic_terms = r'^(hypothetical_protein|unknown|predicted_protein|uncharacterized_protein|protein_of_unknown_function|possible_protein|orf|DUF.*)$'
    if re.match(generic_terms, cleaned, flags=re.IGNORECASE): return None
    return cleaned

def load_phylum_map(map_file):
    """Loads a phylum map (GenomeID -> Phylum) from a CSV/TSV file."""
    logging.info(f"Loading phylum map from: {map_file}")
    phylum_map = {}
    try:
        # Try tab separator first, then comma
        try:
            phylum_df_map = pd.read_csv(map_file, sep='\\t')
        except pd.errors.ParserError:
             logging.debug("Tab separator failed, trying comma...")
             phylum_df_map = pd.read_csv(map_file, sep=',')

        # Standardize column names (case-insensitive)
        phylum_df_map.columns = [col.lower().strip() for col in phylum_df_map.columns]

        # Check for required columns (allow variations like 'gca' or 'genomeid')
        accession_col = None
        phylum_col = None
        for col in phylum_df_map.columns:
            if col in ['accession', 'genomeid', 'gca_accession', 'assembly_accession']:
                accession_col = col
            if col in ['updated_phylum', 'phylum', 'taxon_phylum']:
                phylum_col = col

        if accession_col and phylum_col:
             phylum_map = phylum_df_map.set_index(accession_col)[phylum_col].astype(str).to_dict()
             logging.info(f"Loaded phylum information for {len(phylum_map)} accessions.")
        else:
            logging.error(f"Phylum map file '{map_file}' missing required columns (need ~'accession' and ~'phylum'). Found: {list(phylum_df_map.columns)}")
            return None # Return None to indicate failure
    except FileNotFoundError:
        logging.error(f"Phylum map file not found: '{map_file}'.")
        return None
    except Exception as e:
        logging.error(f"Error reading phylum map file '{map_file}': {e}")
        return None
    return phylum_map


def load_protein_metadata(fasta_dir, phylum_map=None):
    """
    Loads protein metadata by parsing FASTA headers from files in a directory.
    Optionally overrides parsed phylum with map lookup.
    """
    logging.info(f"Loading protein metadata from FASTA files in: {fasta_dir}")
    input_path = Path(fasta_dir)
    if not input_path.is_dir():
        logging.error(f"FASTA input directory not found: {fasta_dir}")
        return None

    protein_metadata = []
    fasta_files = list(input_path.glob('*.fasta')) + \
                  list(input_path.glob('*.faa')) + \
                  list(input_path.glob('*.fa'))
    fasta_files = sorted(list(set(fasta_files)))

    if not fasta_files:
        logging.error(f"No FASTA files (.fasta, .faa, .fa) found in {fasta_dir}")
        return None

    logging.info(f"Found {len(fasta_files)} FASTA files to process.")
    parsed_proteins_count = 0
    processed_files_count = 0
    for f_path in fasta_files:
        processed_files_count += 1
        logging.debug(f"Parsing FASTA file: {f_path.name}")
        # Try to guess GenomeID from filename if needed later
        genome_id_from_fname = f_path.stem # e.g., GCA_0012345.1 from GCA_0012345.1.fasta

        try:
            for record in SeqIO.parse(f_path, "fasta"):
                parsed_proteins_count += 1
                metadata = parse_fasta_header(record) # Get initial metadata from header

                if metadata:
                    # If GenomeID wasn't parsed from header, try using filename
                    if metadata['GenomeID'] in ['UnknownGenome', 'ParseError']:
                        metadata['GenomeID'] = genome_id_from_fname

                    # *** Override Phylum using the map if provided ***
                    if phylum_map and metadata['GenomeID'] in phylum_map:
                        metadata['Phylum'] = phylum_map[metadata['GenomeID']]
                    elif metadata['Phylum'] == 'UnknownPhylum' and phylum_map:
                         # If header didn't have phylum, but map exists, check map
                         metadata['Phylum'] = phylum_map.get(metadata['GenomeID'], 'UnknownPhylum')

                    protein_metadata.append(metadata)

            if processed_files_count % 50 == 0 or processed_files_count == len(fasta_files):
                 logging.info(f"  ...parsed {processed_files_count}/{len(fasta_files)} files ({parsed_proteins_count:,} proteins)...")

        except Exception as e:
            logging.warning(f"Error parsing file {f_path.name}: {e}. Skipping.")
            continue

    if not protein_metadata:
        logging.error("Failed to load any protein metadata. Check FASTA headers and parsing function.")
        return None

    protein_meta_df = pd.DataFrame(protein_metadata)
    logging.info(f"Loaded metadata for {len(protein_meta_df):,} proteins from {processed_files_count} files.")

    # Check for duplicate ProteinIDs which could indicate issues
    if protein_meta_df['ProteinID'].duplicated().any():
        duplicate_count = protein_meta_df['ProteinID'].duplicated().sum()
        logging.warning(f"{duplicate_count} duplicate ProteinIDs found in metadata. Using first occurrence. Ensure ProteinIDs are unique across all input FASTAs if this is unexpected.")
        protein_meta_df = protein_meta_df.drop_duplicates(subset=['ProteinID'], keep='first')
        logging.info(f"  DataFrame reduced to {len(protein_meta_df)} rows after dropping duplicate ProteinIDs.")

    return protein_meta_df

# --- Other functions (load_orthogroups, calculate_og_stats, merge_and_annotate,
#                      filter_and_save_ogs, write_summary_stats_file, generate_plots)
#     remain largely the same as in orthofinder_analysis_generic_v1, but need
#     minor adjustments for phylum handling ---

def load_orthogroups(orthofinder_results_dir):
    """Loads the Orthogroups.tsv file."""
    orthogroups_file = os.path.join(orthofinder_results_dir, 'Orthogroups', 'Orthogroups.tsv')
    logging.info(f"Loading orthogroups from: {orthogroups_file}")
    try:
        og_df_wide = pd.read_csv(orthogroups_file, sep='\\t', index_col=0, keep_default_na=True, na_values=[''])
        logging.info(f"Successfully loaded {len(og_df_wide)} orthogroups.")
        if 'HOG' in og_df_wide.columns:
            og_df_wide = og_df_wide.drop(columns=['HOG'])
            logging.debug("Dropped 'HOG' column.")
        return og_df_wide
    except FileNotFoundError:
        logging.error(f"Orthogroups file not found: {orthogroups_file}")
        return None
    except Exception as e:
        logging.error(f"Error reading {orthogroups_file}: {e}")
        return None

def calculate_og_stats(og_df_wide):
    """Calculates statistics (protein count, species count) per orthogroup."""
    logging.info("Calculating statistics per orthogroup...")
    num_species = len(og_df_wide.columns)
    if num_species == 0:
        logging.error("No species columns found in Orthogroups data.")
        return None, None, 0

    try:
        logging.debug("Stacking dataframe...")
        og_df_long_with_str = og_df_wide.stack().reset_index()
        og_df_long_with_str.columns = ['Orthogroup', 'GenomeID', 'ProteinString']

        logging.debug("Splitting protein strings...")
        og_df_long_with_str = og_df_long_with_str.dropna(subset=['ProteinString'])
        og_df_long_with_str['ProteinID_list'] = og_df_long_with_str['ProteinString'].str.split(', ')

        logging.debug("Exploding dataframe...")
        og_df_long = og_df_long_with_str.explode('ProteinID_list').drop(columns=['ProteinString'])
        og_df_long.rename(columns={'ProteinID_list': 'ProteinID'}, inplace=True)
        og_df_long['ProteinID'] = og_df_long['ProteinID'].str.strip()
        og_df_long = og_df_long.dropna(subset=['ProteinID'])
        og_df_long = og_df_long[og_df_long['ProteinID'] != '']
        logging.info(f"Reshaped orthogroup data into {len(og_df_long):,} protein assignments.")

    except Exception as e:
        logging.error(f"Error reshaping orthogroup data: {e}")
        return None, None, num_species

    logging.debug("Aggregating stats...")
    try:
        og_stats = og_df_long.groupby('Orthogroup').agg(
            num_proteins=('ProteinID', 'size'),
            num_species=('GenomeID', 'nunique')
        ).reset_index()
        og_stats['species_pct'] = (og_stats['num_species'] / num_species) * 100
    except Exception as e:
        logging.error(f"Error aggregating orthogroup stats: {e}")
        return None, None, num_species

    return og_stats, og_df_long, num_species


def merge_and_annotate(og_df_long, protein_meta_df, phylum_col_name):
    """Merges orthogroup assignments with protein metadata and calculates hypothetical percentage."""
    logging.info("Merging orthogroup assignments with protein metadata...")
    if og_df_long is None or protein_meta_df is None:
        logging.error("Cannot merge data, input dataframes are missing.")
        return None, None

    # Merge on ProteinID only, as GenomeID might not be reliable in metadata fallback
    merge_key = ['ProteinID']
    logging.debug(f"Using merge key: {merge_key}")

    try:
        # Ensure ProteinID columns have the same type if necessary
        # og_df_long['ProteinID'] = og_df_long['ProteinID'].astype(str)
        # protein_meta_df['ProteinID'] = protein_meta_df['ProteinID'].astype(str)
        merged_data = pd.merge(og_df_long, protein_meta_df, on=merge_key, how='left')
        # Drop potential duplicates introduced if a ProteinID exists in multiple OGs (shouldn't happen with OrthoFinder)
        # or if metadata had duplicates not fully removed earlier. Check based on OG and ProteinID.
        merged_data = merged_data.drop_duplicates(subset=['Orthogroup', 'ProteinID'])
    except KeyError as e:
         logging.error(f"Merge failed. Missing key column: {e}. Check ProteinID column name in both datasets.")
         return None, None
    except Exception as e:
        logging.error(f"Error merging orthogroup and metadata: {e}")
        return None

    # Check for proteins in OGs that didn't get metadata
    missing_meta_mask = merged_data['AnnotationType'].isnull()
    missing_meta_count = missing_meta_mask.sum()
    if missing_meta_count > 0:
        logging.warning(f"{missing_meta_count:,} protein assignments lack metadata after merge.")
        missing_ids = merged_data.loc[missing_meta_mask, 'ProteinID'].unique()
        logging.warning(f"  Example ProteinIDs missing metadata: {list(missing_ids[:10])}")
        logging.warning("  This might happen if FASTA files used for OrthoFinder differ from those used for metadata extraction, or due to header parsing issues.")
        # Fill missing AnnotationType for calculation purposes
        merged_data['AnnotationType'].fillna('unknown', inplace=True)
        # Also fill Phylum if it's being used
        if phylum_col_name in merged_data.columns:
             merged_data[phylum_col_name].fillna('UnknownPhylum', inplace=True)

    # --- Calculate hypothetical percentage per OG ---
    logging.info("Calculating hypothetical percentage per orthogroup...")
    try:
        og_annot_summary = merged_data.groupby('Orthogroup')['AnnotationType'].value_counts().unstack(fill_value=0)
        if 'hypothetical' not in og_annot_summary.columns: og_annot_summary['hypothetical'] = 0
        if 'annotated' not in og_annot_summary.columns: og_annot_summary['annotated'] = 0
        og_annot_summary['total_annot_types'] = og_annot_summary['hypothetical'] + og_annot_summary['annotated']

        og_annot_summary['pct_hypothetical'] = 0.0
        valid_totals = og_annot_summary['total_annot_types'] > 0
        og_annot_summary.loc[valid_totals, 'pct_hypothetical'] = \
            (og_annot_summary['hypothetical'][valid_totals] / og_annot_summary['total_annot_types'][valid_totals]) * 100

    except Exception as e:
        logging.error(f"Error calculating hypothetical percentages: {e}")
        return merged_data, None # Return merged data even if summary fails

    # --- Calculate Phylum stats per OG (if Phylum column exists) ---
    og_phylum_stats = None
    if phylum_col_name in merged_data.columns:
        logging.info(f"Calculating {phylum_col_name} representation per orthogroup...")
        try:
            # Count number of unique phyla per OG
            og_phylum_counts = merged_data.groupby('Orthogroup')[phylum_col_name].nunique()
            og_phylum_stats = og_phylum_counts.reset_index(name=f'num_{phylum_col_name.lower()}')

            # Calculate percentage of proteins from each phylum within each OG (optional, can be large)
            # og_phylum_pct = merged_data.groupby(['Orthogroup', phylum_col_name]).size().unstack(fill_value=0)
            # og_phylum_pct = og_phylum_pct.apply(lambda x: x / x.sum() * 100, axis=1)
            # og_phylum_stats = pd.merge(og_phylum_stats, og_phylum_pct, on='Orthogroup', how='left')

        except Exception as e:
            logging.error(f"Error calculating phylum statistics: {e}")
            # Continue without phylum stats if calculation fails

    # Combine annotation summary and phylum stats (if available)
    og_summary_stats = og_annot_summary[['pct_hypothetical']]
    if og_phylum_stats is not None:
        og_summary_stats = pd.merge(og_summary_stats, og_phylum_stats, on='Orthogroup', how='left')

    return merged_data, og_summary_stats


def filter_and_save_ogs(og_stats_full, merged_data, num_species, conservation_threshold_pct,
                        hypothetical_threshold_pct, annotated_threshold_pct, output_prefix, phylum_col_name):
    """Filters OGs based on conservation and annotation status, saves lists including phylum if available."""
    logging.info(f"Identifying and saving conserved OGs (>= {conservation_threshold_pct}% species)...")

    conservation_threshold_count = int(np.ceil(num_species * (conservation_threshold_pct / 100.0)))
    if conservation_threshold_pct > 0 and conservation_threshold_count == 0: conservation_threshold_count = 1
    logging.info(f"  Absolute species count threshold for conservation: {conservation_threshold_count}/{num_species}")

    conserved_ogs = og_stats_full[og_stats_full['num_species'] >= conservation_threshold_count].copy()
    logging.info(f"Found {len(conserved_ogs):,} OGs conserved at >= {conservation_threshold_pct}% level.")

    if conserved_ogs.empty:
        logging.warning("No OGs met the conservation threshold. Skipping detailed list saving.")
        return

    # Define base columns for output TSV files
    og_output_cols = ['Orthogroup', 'num_species', 'species_pct', 'num_proteins', 'pct_hypothetical']
    protein_output_cols = ['Orthogroup', 'GenomeID', 'ProteinID', 'AnnotationType', 'CleanedAnnotationName']
    if phylum_col_name in conserved_ogs.columns:
        og_output_cols.append(f'num_{phylum_col_name.lower()}') # Add phylum count column if present
    if phylum_col_name in merged_data.columns:
        protein_output_cols.insert(3, phylum_col_name) # Insert phylum before annotation type

    # --- Conserved Hypothetical ---
    hypo_pct_filter = hypothetical_threshold_pct
    logging.info(f"Filtering for Conserved Hypothetical (>= {hypo_pct_filter}% hypothetical)...")
    conserved_hypothetical_ogs = conserved_ogs[conserved_ogs['pct_hypothetical'] >= hypo_pct_filter]
    logging.info(f"  Found {len(conserved_hypothetical_ogs):,} conserved hypothetical orthogroups.")

    if not conserved_hypothetical_ogs.empty:
        conserved_hypothetical_ogs_sorted = conserved_hypothetical_ogs.sort_values('num_species', ascending=False)
        hypo_ogs_file = f'{output_prefix}_conserved{conservation_threshold_pct}pct_hypothetical_ogs.tsv'
        hypo_proteins_file = f'{output_prefix}_conserved{conservation_threshold_pct}pct_hypothetical_proteins.tsv'
        try:
            conserved_hypothetical_ogs_sorted[og_output_cols].to_csv(hypo_ogs_file, sep='\\t', index=False, float_format='%.1f')
            logging.info(f"  Saved conserved hypothetical OG stats to: {hypo_ogs_file}")
            conserved_hypo_proteins = merged_data[merged_data['Orthogroup'].isin(conserved_hypothetical_ogs['Orthogroup'])]
            conserved_hypo_proteins[protein_output_cols].to_csv(hypo_proteins_file, sep='\\t', index=False)
            logging.info(f"  Saved conserved hypothetical protein list ({len(conserved_hypo_proteins):,} proteins) to: {hypo_proteins_file}")
        except IOError as e:
            logging.error(f"  Failed to write conserved hypothetical output files: {e}")
        except KeyError as e:
             logging.error(f"  Missing column for saving conserved hypothetical data: {e}. Columns available: {list(conserved_hypothetical_ogs_sorted.columns)} / {list(conserved_hypo_proteins.columns)}")
    else:
        logging.info("  No conserved orthogroups met the hypothetical threshold.")

    # --- Conserved Annotated ---
    anno_pct_filter = 100.0 - annotated_threshold_pct
    logging.info(f"Filtering for Conserved Annotated (>= {annotated_threshold_pct}% annotated, i.e., <= {anno_pct_filter:.1f}% hypothetical)...")
    conserved_annotated_ogs = conserved_ogs[conserved_ogs['pct_hypothetical'] <= anno_pct_filter]
    logging.info(f"  Found {len(conserved_annotated_ogs):,} conserved annotated orthogroups.")

    if not conserved_annotated_ogs.empty:
        conserved_annotated_ogs_sorted = conserved_annotated_ogs.sort_values('num_species', ascending=False)
        anno_ogs_file = f'{output_prefix}_conserved{conservation_threshold_pct}pct_annotated_ogs.tsv'
        anno_proteins_file = f'{output_prefix}_conserved{conservation_threshold_pct}pct_annotated_proteins.tsv'
        try:
            # Display top 5 for verification
            logging.info("    --- Top Conserved Annotated OGs ---")
            logging.info(conserved_annotated_ogs_sorted[og_output_cols].round(1).head().to_string())
            conserved_annotated_ogs_sorted[og_output_cols].to_csv(anno_ogs_file, sep='\\t', index=False, float_format='%.1f')
            logging.info(f"   Saved OG stats to: {anno_ogs_file}")
            conserved_anno_proteins = merged_data[merged_data['Orthogroup'].isin(conserved_annotated_ogs['Orthogroup'])]
            conserved_anno_proteins[protein_output_cols].to_csv(anno_proteins_file, sep='\\t', index=False)
            logging.info(f"   Saved conserved annotated protein list ({len(conserved_anno_proteins):,} proteins) to: {anno_proteins_file}")
        except IOError as e:
            logging.error(f"  Failed to write conserved annotated output files: {e}")
        except KeyError as e:
             logging.error(f"  Missing column for saving conserved annotated data: {e}. Columns available: {list(conserved_annotated_ogs_sorted.columns)} / {list(conserved_anno_proteins.columns)}")
    else:
        logging.info("  No conserved orthogroups met the annotated threshold.")


def write_summary_stats_file(og_stats, merged_data, num_species, num_orthogroups, core_percentages,
                             output_file, results_dir_info, phylum_col_name):
    """Writes a text file summarizing the OrthoFinder statistics, including phylum info if available."""
    logging.info(f"Calculating Core OG counts and writing summary to: {output_file}")
    core_og_counts = {}

    try:
        with open(output_file, 'w') as f:
            f.write(f"OrthoFinder Summary Statistics\n")
            f.write(f"Source Results Directory: {results_dir_info}\n")
            f.write(f"Based on {num_species} input species.\n")
            f.write("="*60 + "\n\n")
            f.write("--- Overall Summary ---\n")
            f.write(f"Total Orthogroups (OGs) Found: {num_orthogroups:,}\n\n")
            f.write("--- Orthogroup Size (Number of Species) ---\n")
            f.write(og_stats['num_species'].describe().round(1).to_string())
            f.write("\n\n")
            f.write("--- Orthogroup Size (Number of Proteins) ---\n")
            f.write(og_stats['num_proteins'].describe().round(1).to_string())
            f.write("\n\n")
            f.write("--- Core & Single-Copy Orthogroups at Different Thresholds ---\n")
            for p in sorted(core_percentages, reverse=True):
                threshold_count = int(np.ceil(num_species * (p / 100.0)))
                if p > 0 and threshold_count == 0: threshold_count = 1
                core_ogs_at_p = og_stats[og_stats['num_species'] >= threshold_count]
                num_core_ogs_at_p = len(core_ogs_at_p)
                single_copy_at_p = core_ogs_at_p[core_ogs_at_p['num_proteins'] == core_ogs_at_p['num_species']]
                num_single_copy_at_p = len(single_copy_at_p)
                num_strict_single_copy = 0
                if p == 100 and threshold_count == num_species:
                     strict_single_copy_ogs = single_copy_at_p[single_copy_at_p['num_species'] == num_species]
                     num_strict_single_copy = len(strict_single_copy_ogs)
                f.write(f"  Threshold >= {p}% ({threshold_count} / {num_species} species):\n")
                f.write(f"    'Core' OGs: {num_core_ogs_at_p:,}\n")
                f.write(f"    Single-Copy OGs in Core: {num_single_copy_at_p:,}\n")
                if p == 100: f.write(f"      (Strict Single-Copy): {num_strict_single_copy:,}\\n")
                f.write("\n")
                core_og_counts[f">= {p}%"] = num_core_ogs_at_p # Use string key for plotting

            # --- Add Phylum Summary Section (if phylum data exists) ---
            if phylum_col_name in merged_data.columns:
                f.write(f"--- {phylum_col_name} Representation Summary ---\n")
                try:
                    phylum_protein_counts = merged_data[phylum_col_name].value_counts()
                    f.write(f"\nTotal Protein Count per {phylum_col_name} (across all OGs):\n")
                    f.write(phylum_protein_counts.to_string())
                    f.write("\n\n")

                    # Count OGs where each phylum is present
                    og_phylum_presence = merged_data.drop_duplicates(subset=['Orthogroup', phylum_col_name])[phylum_col_name].value_counts()
                    f.write(f"Number of OGs Containing at least one Protein from {phylum_col_name}:\n")
                    f.write(og_phylum_presence.to_string())
                    f.write("\n\n")
                except Exception as phylum_e:
                     logging.warning(f"Could not calculate or write phylum summary statistics: {phylum_e}")
                     f.write(" (Error calculating phylum statistics)\n\n")

        logging.info(f"Successfully wrote statistics to {output_file}")
        return core_og_counts

    except Exception as e:
        logging.error(f"Error writing summary file '{output_file}': {e}")
        return None

def generate_plots(og_stats, core_og_counts, num_orthogroups, num_species, output_prefix, plot_output_dir):
    """Generates and saves/shows summary plots."""
    logging.info("Generating visualizations...")
    plt.style.use('seaborn-v0_8-talk')

    # Plot 1: Species per OG
    try:
        plt.figure(figsize=(10, 6))
        sns.histplot(og_stats['num_species'], bins=min(50, num_species), kde=False)
        plt.xlabel('Number of Species per Orthogroup')
        plt.ylabel('Number of Orthogroups')
        plt.title(f'{output_prefix}: Distribution of Species per OG (Total OGs = {num_orthogroups:,})')
        plt.tight_layout()
        if plot_output_dir:
            plot_path = os.path.join(plot_output_dir, f'{output_prefix}_species_per_og_dist.png')
            plt.savefig(plot_path, dpi=300)
            logging.info(f"Saved species distribution plot to: {plot_path}")
        plt.show()
        plt.close()
    except Exception as e: logging.error(f"Error generating species distribution plot: {e}")

    # Plot 2: Proteins per OG
    try:
        plt.figure(figsize=(10, 6))
        plot_data = og_stats['num_proteins'][og_stats['num_proteins'] > 0]
        if not plot_data.empty:
            min_val, max_val = plot_data.min(), plot_data.max()
            if min_val > 0 and max_val > min_val:
                log_bins = np.logspace(np.log10(min_val), np.log10(max_val), 50)
                sns.histplot(plot_data, bins=log_bins, kde=False)
                plt.xscale('log')
                plt.xlabel('Number of Proteins per Orthogroup (Log Scale)')
            else:
                sns.histplot(plot_data, bins=50, kde=False)
                plt.xlabel('Number of Proteins per Orthogroup')
            plt.ylabel('Number of Orthogroups')
            plt.title(f'{output_prefix}: Distribution of Proteins per OG (Total OGs = {num_orthogroups:,})')
            plt.tight_layout()
            if plot_output_dir:
                plot_path = os.path.join(plot_output_dir, f'{output_prefix}_proteins_per_og_dist.png')
                plt.savefig(plot_path, dpi=300)
                logging.info(f"Saved protein distribution plot to: {plot_path}")
            plt.show()
            plt.close()
        else: logging.warning("Skipping protein distribution plot (no data).")
    except Exception as e: logging.error(f"Error generating protein distribution plot: {e}")

    # Plot 3: Core OG Counts
    if core_og_counts:
        try:
            # Convert dict keys (like '>= 90%') to numeric for sorting if needed
            core_og_counts_df = pd.DataFrame(list(core_og_counts.items()), columns=['Threshold_Label', 'Count'])
            core_og_counts_df['Threshold_Num'] = core_og_counts_df['Threshold_Label'].str.extract(r'(\d+\.?\d*)').astype(float)
            core_og_counts_df = core_og_counts_df.sort_values(by='Threshold_Num', ascending=False)

            plt.figure(figsize=(12, 7))
            sns.barplot(x='Threshold_Label', y='Count', data=core_og_counts_df, color='skyblue', order=core_og_counts_df['Threshold_Label'])
            plt.xlabel('Minimum Conservation Threshold (% Species)')
            plt.ylabel('Number of Orthogroups')
            plt.title(f'{output_prefix}: Number of Orthogroups vs. Conservation Threshold')
            plt.xticks(rotation=45, ha='right')
            plt.tight_layout()
            if plot_output_dir:
                plot_path = os.path.join(plot_output_dir, f'{output_prefix}_core_og_counts_by_threshold.png')
                plt.savefig(plot_path, dpi=300)
                logging.info(f"Saved core OG count plot to: {plot_path}")
            plt.show()
            plt.close()
        except Exception as e: logging.error(f"Error generating core OG count plot: {e}")
    else: logging.warning("Skipping core OG count plot (no data calculated).")


# --- Main Execution ---
def main():
    parser = argparse.ArgumentParser(description="Analyze OrthoFinder results, integrate with protein metadata, and optionally include phylum analysis.")

    # Input Args
    parser.add_argument("-r", "--orthofinder_dir", required=True, help="Path to OrthoFinder results directory.")
    parser.add_argument("-f", "--fasta_dir", required=True, help="Path to directory containing input FASTA files used for OrthoFinder.")
    parser.add_argument("--phylum_map", default=None, help="Optional path to CSV/TSV file mapping GenomeID (col 'accession') to Phylum (col 'updated_phylum').")

    # Output Args
    parser.add_argument("-p", "--output_prefix", required=True, help="Prefix for output files (e.g., 'GV_Analysis').")
    parser.add_argument("--plot_dir", default=None, help="Directory to save plots (optional).")
    parser.add_argument("--summary_file", default=None, help="File path for summary statistics text (optional, uses prefix otherwise).")
    parser.add_argument("--phylum_column_name", default="Phylum", help="Name to use for the phylum column in outputs (default: Phylum).")

    # Analysis Threshold Args
    parser.add_argument("--conservation_threshold", type=float, default=80.0, help="Conservation threshold (%% species) for saving detailed OG lists (default: 80.0).")
    parser.add_argument("--hypothetical_threshold", type=float, default=75.0, help="Threshold (%%) for 'mostly hypothetical' OGs (default: 75.0).")
    parser.add_argument("--annotated_threshold", type=float, default=75.0, help="Threshold (%%) for 'mostly annotated' OGs (default: 75.0).")
    parser.add_argument("--core_pcts", nargs='+', type=float, default=[100, 99, 95, 90, 80, 70, 50], help="Percentages for core OG counts (default: 100 99 95 90 80 70 50).")

    args = parser.parse_args()

    # Set Output Filenames
    if args.summary_file is None:
        args.summary_file = f"{args.output_prefix}_orthofinder_summary_stats_{int(args.conservation_threshold)}pct.txt"

    # Create Plot Directory
    if args.plot_dir:
        try: Path(args.plot_dir).mkdir(parents=True, exist_ok=True); logging.info(f"Plots will be saved in: {args.plot_dir}")
        except OSError as e: logging.warning(f"Could not create plot directory {args.plot_dir}: {e}. Plots will only be displayed."); args.plot_dir = None

    # --- Run Analysis ---
    logging.info(f"--- Starting Analysis for Prefix: {args.output_prefix} ---")

    phylum_map_data = None
    if args.phylum_map:
        phylum_map_data = load_phylum_map(args.phylum_map)
        if phylum_map_data is None:
             logging.warning("Phylum map loading failed. Proceeding without phylum analysis.")

    og_df_wide = load_orthogroups(args.orthofinder_dir)
    if og_df_wide is None: sys.exit(1)
    num_orthogroups_loaded = len(og_df_wide)

    protein_meta_df = load_protein_metadata(args.fasta_dir, phylum_map_data)
    if protein_meta_df is None: sys.exit(1)

    og_stats, og_df_long, num_species = calculate_og_stats(og_df_wide)
    if og_stats is None or og_df_long is None: sys.exit(1)

    # Pass the desired phylum column name for consistency
    merged_data, og_summary_stats = merge_and_annotate(og_df_long, protein_meta_df, args.phylum_column_name)
    if merged_data is None or og_summary_stats is None: sys.exit(1)

    # Add summary stats (hypothetical pct, phylum counts) back to main stats df
    og_stats_full = pd.merge(og_stats, og_summary_stats, on='Orthogroup', how='left')
    # Fill NaN for pct_hypothetical if calculation failed for some OGs
    og_stats_full['pct_hypothetical'] = og_stats_full['pct_hypothetical'].fillna(0)
    # Fill NaN for phylum counts if phylum analysis wasn't done or failed
    phylum_count_col = f'num_{args.phylum_column_name.lower()}'
    if phylum_count_col in og_stats_full.columns:
         og_stats_full[phylum_count_col] = og_stats_full[phylum_count_col].fillna(0).astype(int)

    core_og_counts = write_summary_stats_file(
        og_stats_full, merged_data, num_species, num_orthogroups_loaded,
        args.core_pcts, args.summary_file, args.orthofinder_dir, args.phylum_column_name
    )

    filter_and_save_ogs(
        og_stats_full, merged_data, num_species,
        args.conservation_threshold, args.hypothetical_threshold, args.annotated_threshold,
        args.output_prefix, args.phylum_column_name
    )

    generate_plots(og_stats_full, core_og_counts, num_orthogroups_loaded, num_species, args.output_prefix, args.plot_dir)

    logging.info(f"--- Analysis for Prefix: {args.output_prefix} Complete ---")

if __name__ == "__main__":
    main()