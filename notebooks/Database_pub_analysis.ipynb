{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bb5a54-2db7-4a54-9904-d7d30d0efb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup, Data Loading, and Phase 1.1 Analysis (Hit Quality Review - Evalue/Pident)\n",
    "# (This cell remains the same - ensure it's run first)\n",
    "# NOTE: This cell should load the database that HAS the Euk Hit info but LACKS coverage, \n",
    "# which is 'proteome_database_combined_v2.0.csv' based on user feedback.\n",
    "\n",
    "# --- Standard Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from collections import Counter\n",
    "import glob # For finding files in a folder\n",
    "import re # For parsing organism names\n",
    "import sys # For exit\n",
    "import logging # For detailed logging\n",
    "from pathlib import Path # For handling paths\n",
    "# Ensure Biopython is installed: pip install biopython\n",
    "try:\n",
    "    from Bio import SeqIO \n",
    "except ImportError:\n",
    "    print(\"ERROR: Biopython is required for this script. Please install it: pip install biopython\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# --- Plotly Imports ---\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "# --- Setup Logging ---\n",
    "log_formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "logger = logging.getLogger()\n",
    "# Prevent adding handlers multiple times if script is re-run in interactive session\n",
    "if not logger.hasHandlers():\n",
    "    logger.setLevel(logging.INFO) \n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setFormatter(log_formatter)\n",
    "    logger.addHandler(console_handler)\n",
    "else:\n",
    "    # Ensure level is set if handlers already exist (e.g., in notebook re-run)\n",
    "    logger.setLevel(logging.INFO) \n",
    "\n",
    "print(\"--- Notebook Setup: Asgard Eukaryotic Hit Validation - Phase 1 ---\")\n",
    "\n",
    "# --- Configuration ---\n",
    "# INPUT file path: Use the database version that already has Euk annotations\n",
    "# but is missing coverage columns, as indicated by the user.\n",
    "db_with_annots_path = 'proteome_database_combined_v2.0.csv' \n",
    "\n",
    "# INPUT: Eukaryotic proteome FASTA file (needed for helper function definition in this cell)\n",
    "euk_fasta_path = Path('euk63_proteomes_final.fasta') \n",
    "\n",
    "# Directory to save plots and summary data for this analysis phase\n",
    "output_plot_dir_phase1 = 'output_plots_hit_validation_phase1'\n",
    "output_summary_dir_phase1 = 'output_summary_data_hit_validation_phase1'\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "# Use Path objects here for consistency\n",
    "Path(output_plot_dir_phase1).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Ensured plot output directory exists: {output_plot_dir_phase1}\")\n",
    "Path(output_summary_dir_phase1).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Ensured summary data directory exists: {output_summary_dir_phase1}\")\n",
    "\n",
    "\n",
    "# Path to InterPro entry list file (ensure this file is in the correct location)\n",
    "interpro_entry_path = 'interpro_entry.txt' \n",
    "\n",
    "# DIAMOND parsing parameters (for re-parsing in Cell 2)\n",
    "diamond_results_folder_for_coverage = 'euk_diamond_search_results'\n",
    "e_value_threshold_for_coverage = 1e-10 # Should match initial filtering used to generate hits in DB\n",
    "diamond_col_names_for_coverage = [\n",
    "    'qseqid', 'sseqid_full', 'pident', 'length', 'mismatch', 'gapopen',\n",
    "    'qstart', 'qend', 'sstart', 'send', 'evalue', 'bitscore', 'qlen', 'slen'\n",
    "]\n",
    "\n",
    "\n",
    "# --- Define Key Column Names (Based on user provided list for v2.0) ---\n",
    "protein_id_col = 'ProteinID'\n",
    "# sequence_col = 'Sequence' # Not needed for this analysis\n",
    "# length_col = 'Length' # Use Original_Seq_Length\n",
    "source_dataset_col = 'Source_Dataset'\n",
    "source_genome_accession_col = 'Source_Genome_Assembly_Accession'\n",
    "source_protein_annotation_col = 'Source_Protein_Annotation'\n",
    "ncbi_taxid_col = 'NCBI_TaxID'\n",
    "asgard_phylum_col = 'Asgard_Phylum' \n",
    "virus_family_col = 'Virus_Family'\n",
    "virus_name_col = 'Virus_Name'\n",
    "orthogroup_col = 'Orthogroup'\n",
    "ipr_col = 'IPR_Signatures' \n",
    "ipr_go_terms_col = 'IPR_GO_Terms'\n",
    "uniprotkb_ac_col = 'UniProtKB_AC'\n",
    "num_domains_col = 'Num_Domains'\n",
    "domain_arch_col = 'Domain_Architecture' \n",
    "type_col = 'Type' # As in 'Annotated', 'Uncharacterized'\n",
    "is_hypothetical_col = 'Is_Hypothetical'\n",
    "has_known_structure_col = 'Has_Known_Structure' # Not used directly, Is_Structurally_Dark is derived\n",
    "percent_disorder_col = 'Percent_Disorder'\n",
    "specific_func_cat_col = 'Specific_Functional_Category'\n",
    "broad_func_cat_col = 'Broad_Functional_Category' \n",
    "category_trigger_col = 'Category_Trigger'\n",
    "signal_peptide_col = 'Signal_Peptide_USPNet'\n",
    "sp_cleavage_site_col = 'SP_Cleavage_Site_USPNet'\n",
    "original_seq_length_col = 'Original_Seq_Length' # Use this as the definitive query length\n",
    "group_col = 'Group'\n",
    "seqsearch_pdb_hit_col = 'SeqSearch_PDB_Hit'\n",
    "seqsearch_afdb_hit_col = 'SeqSearch_AFDB_Hit'\n",
    "has_reference_structure_col = 'Has_Reference_Structure'\n",
    "localization_col = 'Predicted_Subcellular_Localization' \n",
    "mature_protein_sequence_col = 'Mature_Protein_Sequence'\n",
    "mature_seq_length_col = 'Mature_Seq_Length'\n",
    "seqsearch_mgnify_hit_col = 'SeqSearch_MGnify_Hit'\n",
    "seqsearch_esma_hit_col = 'SeqSearch_ESMA_Hit'\n",
    "structurally_dark_col = 'Is_Structurally_Dark'\n",
    "esp_col = 'Is_ESP'\n",
    "\n",
    "# DIAMOND Hit Columns (expected to be ALREADY in db_with_annots_path)\n",
    "hit_flag_col = 'Has_Euk_DIAMOND_Hit'\n",
    "euk_hit_sseqid_col = 'Euk_Hit_SSEQID'\n",
    "euk_hit_organism_col = 'Euk_Hit_Organism' \n",
    "euk_hit_pident_col = 'Euk_Hit_PIDENT'\n",
    "euk_hit_evalue_col = 'Euk_Hit_EVALUE'\n",
    "euk_hit_protein_name_col = 'Euk_Hit_Protein_Name' \n",
    "\n",
    "# New DIAMOND Hit Detail Columns (to be ADDED in Cell 2)\n",
    "euk_hit_qstart_col = 'Euk_Hit_Qstart'\n",
    "euk_hit_qend_col = 'Euk_Hit_Qend'\n",
    "euk_hit_sstart_col = 'Euk_Hit_Sstart'\n",
    "euk_hit_send_col = 'Euk_Hit_Send'\n",
    "# euk_hit_qlen_diamond_col = 'Euk_Hit_Qlen_Diamond' # We'll use Original_Seq_Length instead\n",
    "euk_hit_slen_diamond_col = 'Euk_Hit_Slen_Diamond' # Length of Euk hit from DIAMOND\n",
    "query_coverage_col = 'Query_Coverage' # (qend-qstart+1)/Original_Seq_Length\n",
    "subject_coverage_col = 'Subject_Coverage' # (send-sstart+1)/slen\n",
    "\n",
    "# Filtering criteria for high-quality hits (defined here for use in Cell 2)\n",
    "min_query_coverage_filter = 0.70 \n",
    "min_subject_coverage_filter = 0.50\n",
    "min_pident_filter = 30.0 \n",
    "\n",
    "# Output file for Cell 2 (DB with coverage added) - Use Path object\n",
    "output_db_with_coverage_path = Path(output_summary_dir_phase1) / 'proteome_database_combined_v2.1_coverage.csv'\n",
    "\n",
    "# --- Define Arcadia Color Palettes ---\n",
    "print(\"\\n--- Defining Manual Arcadia Color Palettes ---\")\n",
    "# (Color definitions remain the same as previous version)\n",
    "arcadia_colors_manual = {\n",
    "    \"aegean\": \"#5088C5\", \"amber\": \"#F28360\", \"seaweed\": \"#3B9886\", \"canary\": \"#F7B846\",\n",
    "    \"aster\": \"#7A77AB\", \"rose\": \"#F898AE\", \"vital\": \"#73B5E3\", \"tangerine\": \"#FFB984\",\n",
    "    \"oat\": \"#F5E4BE\", \"wish\": \"#BABEE0\", \"lime\": \"#97CD78\", \"dragon\": \"#C85152\",\n",
    "    \"sky\": \"#C6E7F4\", \"dress\": \"#F8C5C1\", \"taupe\": \"#DBD1C3\", \"denim\": \"#B6C8D4\",\n",
    "    \"sage\": \"#B5BEA4\", \"mars\": \"#DA9085\", \"marine\": \"#8A99AD\", \"shell\": \"#EDE0D6\",\n",
    "    \"white\": \"#FFFFFF\", \"gray\": \"#EBEDE8\", \"chateau\": \"#BAB0A8\", \"bark\": \"#8F8885\",\n",
    "    \"slate\": \"#43413F\", \"charcoal\": \"#484B50\", \"crow\": \"#292928\", \"black\": \"#09090A\",\n",
    "    \"forest\": \"#596F74\", \"parchment\": \"#FEF7F1\", \"zephyr\": \"#F4FBFF\",\n",
    "    \"lichen\": \"#F7FBEF\", \"dawn\": \"#F8F4F1\"\n",
    "}\n",
    "arcadia_primary_manual = [ arcadia_colors_manual[c] for c in [\"aegean\", \"amber\", \"seaweed\", \"canary\", \"aster\", \"rose\", \"vital\", \"tangerine\", \"oat\", \"wish\", \"lime\", \"dragon\"] ]\n",
    "arcadia_secondary_manual = [ arcadia_colors_manual[c] for c in [\"sky\", \"dress\", \"taupe\", \"denim\", \"sage\", \"mars\", \"marine\", \"shell\"] ]\n",
    "arcadia_neutrals_manual = [ arcadia_colors_manual[c] for c in [\"gray\", \"chateau\", \"bark\", \"slate\", \"charcoal\", \"forest\", \"crow\"] ]\n",
    "print(\"Manual Arcadia palettes created.\")\n",
    "\n",
    "# --- Configure Plotly Defaults ---\n",
    "print(\"\\n--- Configuring Plotly Defaults ---\")\n",
    "pio.templates.default = \"plotly_white\"\n",
    "print(\"Plotly default template set to 'plotly_white'.\")\n",
    "\n",
    "# --- Load InterPro Entry Data ---\n",
    "print(f\"\\n--- Loading InterPro Entry Data from '{interpro_entry_path}' ---\")\n",
    "ipr_lookup = {} \n",
    "start_time_ipr = time.time()\n",
    "# (IPR loading logic remains the same as previous version)\n",
    "try:\n",
    "    ipr_info_df = pd.read_csv( interpro_entry_path, sep='\\t', usecols=[0, 1, 2], names=['IPR_ID', 'Type', 'Name'], header=0, comment='#', on_bad_lines='warn' )\n",
    "    if 'ENTRY_AC' in ipr_info_df.columns: ipr_info_df.rename(columns={'ENTRY_AC': 'IPR_ID'}, inplace=True)\n",
    "    if 'ENTRY_TYPE' in ipr_info_df.columns: ipr_info_df.rename(columns={'ENTRY_TYPE': 'Type'}, inplace=True)\n",
    "    if 'ENTRY_NAME' in ipr_info_df.columns: ipr_info_df.rename(columns={'ENTRY_NAME': 'Name'}, inplace=True)\n",
    "    if 'IPR_ID' in ipr_info_df.columns and 'Name' in ipr_info_df.columns and 'Type' in ipr_info_df.columns:\n",
    "        ipr_info_df['IPR_ID'] = ipr_info_df['IPR_ID'].astype(str).str.strip()\n",
    "        ipr_lookup = ipr_info_df.set_index('IPR_ID')[['Type', 'Name']].to_dict('index')\n",
    "        print(f\"Loaded InterPro entry data for {len(ipr_lookup)} entries in {time.time() - start_time_ipr:.2f} seconds.\")\n",
    "    else: print(f\"Warning: Expected columns for IPR lookup not all found in '{interpro_entry_path}'.\")\n",
    "except FileNotFoundError: print(f\"Warning: InterPro entry file not found at '{interpro_entry_path}'.\")\n",
    "except Exception as e: print(f\"Warning: An error occurred loading or processing '{interpro_entry_path}': {e}\")\n",
    "if not ipr_lookup: print(\"Warning: ipr_lookup is empty. Domain name translations will not be available.\")\n",
    "\n",
    "# --- Define Helper Functions ---\n",
    "print(\"\\n--- Defining Helper Functions ---\")\n",
    "MAX_RAW_ARCH_LEN_CSV = 500 \n",
    "print(f\"Raw architecture strings in CSVs will be truncated to {MAX_RAW_ARCH_LEN_CSV} characters.\")\n",
    "# (Helper functions: translate_architecture, truncate_string, get_ipr_counts, clean_protein_name remain the same)\n",
    "def translate_architecture(arch_string, lookup_dict=ipr_lookup): \n",
    "    if not isinstance(arch_string, str) or not arch_string or not lookup_dict: return arch_string\n",
    "    processed_arch_string = str(arch_string).replace('|', ';'); ipr_ids = processed_arch_string.split(';')\n",
    "    translated_parts = []\n",
    "    for ipr_id in ipr_ids:\n",
    "        ipr_id_clean = ipr_id.strip()\n",
    "        if ipr_id_clean in lookup_dict: name = lookup_dict[ipr_id_clean].get('Name', ipr_id_clean); name = name[:40] + '...' if len(name) > 43 else name; translated_parts.append(f\"{name} ({ipr_id_clean})\")\n",
    "        elif ipr_id_clean: translated_parts.append(ipr_id_clean)\n",
    "    full_translation = \"; \".join(translated_parts); max_len = 150 \n",
    "    if len(full_translation) > max_len: full_translation = full_translation[:max_len-3] + \"...\"\n",
    "    return full_translation\n",
    "def truncate_string(text, max_len):\n",
    "    if isinstance(text, str) and len(text) > max_len: return text[:max_len-3] + \"...\"\n",
    "    return text\n",
    "def get_ipr_counts(df, ipr_column_name):\n",
    "    if ipr_column_name not in df.columns: print(f\"Warning: IPR column '{ipr_column_name}' not found. Returning empty Counter.\"); return Counter()\n",
    "    all_iprs_list = []\n",
    "    for ipr_string_raw in df[ipr_column_name].dropna():\n",
    "        processed_ipr_string = str(ipr_string_raw).replace('|', ';'); individual_iprs = processed_ipr_string.split(';')\n",
    "        for ipr_item in individual_iprs:\n",
    "            stripped_ipr = ipr_item.strip()\n",
    "            if stripped_ipr and stripped_ipr.startswith(\"IPR\"): all_iprs_list.append(stripped_ipr)\n",
    "    return Counter(all_iprs_list)\n",
    "def clean_protein_name(name):\n",
    "    if pd.isna(name): return \"Unknown/Not Found\"; name = str(name).strip()\n",
    "    name = re.sub(r'\\s*\\|\\s*.*','', name); name = re.sub(r'\\bisoform\\s+[\\w-]+\\b', '', name, flags=re.IGNORECASE).strip()\n",
    "    name = re.sub(r'\\bpartial\\b', '', name, flags=re.IGNORECASE).strip(); name = re.sub(r'\\bputative\\b', '', name, flags=re.IGNORECASE).strip()\n",
    "    name = re.sub(r'\\bpredicted protein\\b', '', name, flags=re.IGNORECASE).strip(); name = re.sub(r'\\btype\\s+\\w+\\b', '', name, flags=re.IGNORECASE).strip()\n",
    "    name = re.sub(r'\\bprotein\\b', '', name, flags=re.IGNORECASE).strip(); name = re.sub(r'\\buncharacterized\\b', 'Uncharacterized', name, flags=re.IGNORECASE).strip() \n",
    "    name = re.sub(r'[;,]$', '', name).strip(); name = re.sub(r'\\s*\\(Fragment\\)$', '', name, flags=re.IGNORECASE).strip()\n",
    "    name = re.sub(r'\\s+', ' ', name).strip(); name = name[0].upper() + name[1:] if len(name) > 0 else name \n",
    "    return name if name else \"Unknown/Not Found\"\n",
    "# Helper function for parsing FASTA headers (from add_euk_fasta_annotations_script)\n",
    "def parse_euk_fasta_headers(fasta_file: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Parses a FASTA file to extract information from headers.\n",
    "    Extracts ID before the first pipe '|' or space ' '.\n",
    "    Extracts Genus species (first two words) for the organism name.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Parsing FASTA headers from: {fasta_file}...\")\n",
    "    header_info = {}\n",
    "    count = 0; skipped_count = 0\n",
    "    if not fasta_file.is_file(): logging.error(f\"FASTA file not found: {fasta_file}\"); return header_info\n",
    "    try:\n",
    "        for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "            count += 1; description = record.description \n",
    "            first_pipe=description.find('|'); first_space=description.find(' '); end_of_id=-1\n",
    "            if first_pipe!=-1 and first_space!=-1: end_of_id=min(first_pipe, first_space)\n",
    "            elif first_pipe!=-1: end_of_id=first_pipe\n",
    "            elif first_space!=-1: end_of_id=first_space\n",
    "            fasta_id_key = description[:end_of_id].strip() if end_of_id!=-1 else description.strip()\n",
    "            if not fasta_id_key: skipped_count+=1; logging.warning(f\"Could not extract valid ID Key from header: {description[:100]}...\"); continue\n",
    "            genus_species_raw=\"Unknown Species\"; protein_name=\"Unknown Protein\"; parts=description.split('|')\n",
    "            if len(parts)>=3: genus_species_raw=parts[1].strip(); protein_name=\" | \".join(p.strip() for p in parts[2:]) \n",
    "            elif len(parts)==2: \n",
    "                second_part=parts[1].strip()\n",
    "                if ' ' in second_part and not second_part.isupper(): genus_species_raw=second_part; protein_name=\"Unknown Protein (Genus/Species only in header)\"\n",
    "                else: genus_species_raw=\"Unknown Species (Name only in header)\"; protein_name=second_part\n",
    "            elif len(parts)==1: \n",
    "                 id_len = len(fasta_id_key)\n",
    "                 first_space_after_id = description.find(' ', id_len)\n",
    "                 if first_space_after_id != -1:\n",
    "                     protein_name = description[id_len+1:].strip()\n",
    "                     genus_species_raw = \"Unknown Species (Space delimited header)\"\n",
    "                 else:\n",
    "                     protein_name = \"Unknown Protein (ID only header)\"\n",
    "                     genus_species_raw = \"Unknown Species (ID only header)\"\n",
    "            genus_species_cleaned=\"Unknown Species\"\n",
    "            if genus_species_raw!=\"Unknown Species\": \n",
    "                temp_name=genus_species_raw.replace('_', ' ').split()\n",
    "                if len(temp_name)>=2: genus_species_cleaned=f\"{temp_name[0]} {temp_name[1]}\"\n",
    "                elif len(temp_name)==1: genus_species_cleaned=temp_name[0] \n",
    "                else: genus_species_cleaned=genus_species_raw \n",
    "            header_info[fasta_id_key]={'Genus_Species': genus_species_cleaned, 'Protein_Name': protein_name}\n",
    "            if count % 100000 == 0: logging.info(f\"  Processed {count:,} FASTA records...\")\n",
    "    except Exception as e: logging.error(f\"Error parsing FASTA file {fasta_file}: {e}\", exc_info=True) \n",
    "    logging.info(f\"Finished parsing. Extracted info for {len(header_info):,} sequences.\")\n",
    "    if skipped_count > 0: logging.warning(f\"Skipped {skipped_count} records due to missing/unparseable IDs.\")\n",
    "    return header_info\n",
    "# Helper function for plotting comparison bars (defined here for self-containment if needed)\n",
    "def plot_comparison_bar(df_subset, df_baseline, column_name, title_suffix, color_map=None, category_orders=None):\n",
    "    \"\"\"Generates a grouped bar chart comparing percentages in a subset vs baseline.\"\"\"\n",
    "    if column_name not in df_subset.columns or column_name not in df_baseline.columns: print(f\"  Skipping plot for '{column_name}': Column not found.\"); return\n",
    "    subset_counts = df_subset[column_name].value_counts(normalize=True, dropna=False).mul(100); subset_counts.name = 'High-Quality Hits (%)'\n",
    "    baseline_counts = df_baseline[column_name].value_counts(normalize=True, dropna=False).mul(100); baseline_counts.name = 'All Asgard (%)'\n",
    "    df_compare = pd.concat([subset_counts, baseline_counts], axis=1).fillna(0).reset_index(); df_compare.rename(columns={df_compare.columns[0]: column_name}, inplace=True)\n",
    "    df_melted = df_compare.melt(id_vars=column_name, var_name='Group', value_name='Percentage')\n",
    "    xaxis_categoryorder=None; xaxis_categoryarray=None\n",
    "    if category_orders and column_name in category_orders: cat_order = category_orders[column_name]; current_cats = df_melted[column_name].unique(); ordered_cats = [c for c in cat_order if c in current_cats]; missing_cats = [c for c in current_cats if c not in ordered_cats]; final_cat_order = ordered_cats + missing_cats; df_melted[column_name] = pd.Categorical(df_melted[column_name], categories=final_cat_order, ordered=True); df_melted = df_melted.sort_values(column_name); xaxis_categoryorder = 'array'; xaxis_categoryarray = final_cat_order\n",
    "    else: default_order = baseline_counts.sort_values(ascending=False).index.tolist(); df_melted[column_name] = pd.Categorical(df_melted[column_name], categories=default_order, ordered=True); df_melted = df_melted.sort_values(column_name); xaxis_categoryorder = 'array'; xaxis_categoryarray = default_order\n",
    "    print(f\"\\nComparison for '{column_name}':\"); print(df_compare.round(1).to_markdown(index=False))\n",
    "    fig = px.bar(df_melted, x=column_name, y='Percentage', color='Group', barmode='group', title=f'{column_name} Distribution: High-Quality Hits vs. All Asgard {title_suffix}', labels={'Percentage': '% of Proteins', column_name: column_name.replace('_', ' ').title()}, color_discrete_map={'High-Quality Hits (%)': arcadia_colors_manual.get('amber', '#F28360'), 'All Asgard (%)': arcadia_colors_manual.get('aegean', '#5088C5')})\n",
    "    if xaxis_categoryorder == 'array': fig.update_xaxes(categoryorder=xaxis_categoryorder, categoryarray=xaxis_categoryarray)\n",
    "    fig.show()\n",
    "\n",
    "# --- Load Data ---\n",
    "print(f\"\\n--- Loading Data from '{db_with_annots_path}' ---\")\n",
    "try:\n",
    "    df_full = pd.read_csv(db_with_annots_path, low_memory=False)\n",
    "    # Ensure ProteinID is string type right after loading\n",
    "    if protein_id_col in df_full.columns:\n",
    "         df_full[protein_id_col] = df_full[protein_id_col].astype(str)\n",
    "    print(f\"Successfully loaded database. Shape: {df_full.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Database file not found at '{db_with_annots_path}'.\")\n",
    "    raise \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the database: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Create Color Maps for Plotting (after df_full is loaded) ---\n",
    "print(\"\\n--- Creating Color Maps ---\")\n",
    "# (Color map definitions remain the same)\n",
    "# Asgard Phylum Colors\n",
    "asgard_phylum_color_map = {}\n",
    "if asgard_phylum_col in df_full.columns and group_col in df_full.columns:\n",
    "    asgard_phyla_cat = df_full[df_full[group_col] == 'Asgard'][asgard_phylum_col].dropna().unique(); asgard_phyla_cat.sort() \n",
    "    if len(asgard_phyla_cat) > 0: asgard_phylum_color_map = {phylum: arcadia_primary_manual[i % len(arcadia_primary_manual)] for i, phylum in enumerate(asgard_phyla_cat)}\n",
    "    asgard_phylum_color_map['Unknown Phylum'] = arcadia_colors_manual.get('gray', '#bdbdbd') \n",
    "print(f\"Asgard phylum color map created for {len(asgard_phylum_color_map)} phyla.\")\n",
    "# Localization Colors\n",
    "localization_color_map = {}\n",
    "if localization_col in df_full.columns:\n",
    "    all_localizations_cat = df_full[localization_col].astype('category').cat.categories.tolist(); localization_assignments = { 'Archaea: Cytoplasmic/Membrane (non-SP)': arcadia_colors_manual.get('aegean', '#5088C5'), 'Archaea: Membrane-associated (Lipoprotein/Pilin)': arcadia_colors_manual.get('amber', '#F28360'), 'Archaea: Secreted/Membrane (Sec/Tat pathway)': arcadia_colors_manual.get('seaweed', '#3B9886'), 'Host: Cytoplasm/Nucleus/Virus Factory': arcadia_colors_manual.get('vital', '#73B5E3'), 'Host: Membrane-associated (Lipoprotein/Pilin-like)': arcadia_colors_manual.get('tangerine', '#FFB984'), 'Host: Secretory Pathway (Secreted/Membrane/Organelle)': arcadia_colors_manual.get('lime', '#97CD78'), 'CYTOPLASMIC': arcadia_colors_manual.get('vital', '#73B5E3'), 'MEMBRANE': arcadia_colors_manual.get('tangerine', '#FFB984'), 'EXTRACELLULAR': arcadia_colors_manual.get('lime', '#97CD78'), 'Unknown': arcadia_colors_manual.get('gray', '#EBEDE8'),}; fallback_palette_loc = arcadia_neutrals_manual + arcadia_secondary_manual; fallback_idx_loc = 0\n",
    "    for loc in all_localizations_cat:\n",
    "        if loc not in localization_color_map: localization_color_map[loc] = localization_assignments.get(loc, fallback_palette_loc[fallback_idx_loc % len(fallback_palette_loc)])\n",
    "        if loc not in localization_assignments: fallback_idx_loc += 1\n",
    "print(f\"Localization color map created for {len(localization_color_map)} categories.\")\n",
    "# Broad Functional Category Colors\n",
    "broad_category_color_map = {}\n",
    "if broad_func_cat_col in df_full.columns:\n",
    "    all_broad_categories_cat = df_full[broad_func_cat_col].astype('category').cat.categories.tolist(); category_assignments = { 'Cytoskeleton': arcadia_colors_manual.get('aegean', '#5088C5'), 'Membrane Trafficking/Vesicles': arcadia_colors_manual.get('amber', '#F28360'), 'ESCRT/Endosomal Sorting': arcadia_colors_manual.get('seaweed', '#3B9886'), 'Ubiquitin System': arcadia_colors_manual.get('aster', '#7A77AB'), 'N-glycosylation': arcadia_colors_manual.get('rose', '#F898AE'), 'Nuclear Transport/Pore': arcadia_colors_manual.get('vital', '#73B5E3'), 'DNA Info Processing': arcadia_colors_manual.get('canary', '#F7B846'), 'RNA Info Processing': arcadia_colors_manual.get('lime', '#97CD78'), 'Translation': arcadia_colors_manual.get('tangerine', '#FFB984'), 'Signal Transduction': arcadia_colors_manual.get('aster', '#7A77AB'), 'Metabolism': arcadia_colors_manual.get('sky', '#C6E7F4'), 'Other Specific Annotation': arcadia_colors_manual.get('denim', '#B6C8D4'), 'Unknown/Unclassified': arcadia_colors_manual.get('gray', '#EBEDE8'),}; fallback_palette_cat = arcadia_primary_manual + arcadia_secondary_manual + arcadia_neutrals_manual; fallback_idx_cat = 0\n",
    "    for category in all_broad_categories_cat:\n",
    "        if category not in broad_category_color_map: broad_category_color_map[category] = category_assignments.get(category, fallback_palette_cat[fallback_idx_cat % len(fallback_palette_cat)])\n",
    "        if category not in category_assignments: fallback_idx_cat += 1\n",
    "print(f\"Broad functional category color map created for {len(broad_category_color_map)} categories.\")\n",
    "\n",
    "\n",
    "# --- Filter for Asgard Proteins with Eukaryotic Hits (Initial Filter) ---\n",
    "print(f\"\\n--- Filtering for Asgard Proteins with Eukaryotic DIAMOND Hits ---\")\n",
    "if group_col not in df_full.columns or hit_flag_col not in df_full.columns:\n",
    "    print(f\"ERROR: Required columns ('{group_col}' or '{hit_flag_col}') not found. Cannot proceed.\")\n",
    "    df_asgard_hits = pd.DataFrame()\n",
    "else:\n",
    "    df_asgard_hits = df_full[\n",
    "        (df_full[group_col] == 'Asgard') & (df_full[hit_flag_col] == True)\n",
    "    ].copy()\n",
    "    print(f\"Found {len(df_asgard_hits)} Asgard proteins with eukaryotic hits for initial review.\")\n",
    "    if df_asgard_hits.empty: print(\"No Asgard proteins with eukaryotic hits found.\")\n",
    "\n",
    "# --- Phase 1.1: Review E-value, Percent Identity ---\n",
    "if not df_asgard_hits.empty:\n",
    "    print(f\"\\n\\n--- Phase 1.1: Reviewing E-value and Percent Identity for Asgard Eukaryotic Hits ---\")\n",
    "    # (Plots are commented out for brevity, uncomment if needed)\n",
    "    print(f\"\\nAnalyzing distribution of '{euk_hit_evalue_col}'...\")\n",
    "    if euk_hit_evalue_col in df_asgard_hits.columns:\n",
    "        print(df_asgard_hits[euk_hit_evalue_col].describe())\n",
    "        # df_asgard_hits['log10_evalue'] = np.log10(df_asgard_hits[euk_hit_evalue_col] + 1e-200) \n",
    "        # fig_evalue_hist = px.histogram(...) fig_evalue_hist.show()\n",
    "        # fig_evalue_box = px.box(...) fig_evalue_box.show()\n",
    "    print(f\"\\nAnalyzing distribution of '{euk_hit_pident_col}'...\")\n",
    "    if euk_hit_pident_col in df_asgard_hits.columns:\n",
    "        print(df_asgard_hits[euk_hit_pident_col].describe())\n",
    "        # fig_pident_hist = px.histogram(...) fig_pident_hist.show()\n",
    "        # fig_pident_box = px.box(...) fig_pident_box.show()\n",
    "else:\n",
    "    print(\"\\nNo Asgard proteins with eukaryotic hits to analyze for Phase 1.1 (Evalue/Pident).\")\n",
    "\n",
    "print(\"\\n--- Cell 1 (Setup & Evalue/Pident Review) Complete ---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2417d5ac-fd90-4340-96ac-b434145f6f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Add Coverage & Define High-Quality Hits\n",
    "# UPDATED: Loads v2.0, adds only coverage columns, saves as v2.1\n",
    "print(\"\\n\\n--- Cell 2: Add Coverage & Define High-Quality Hits ---\")\n",
    "\n",
    "# --- Step 2.1: Re-parse DIAMOND results to get alignment coordinates and subject lengths ---\n",
    "print(f\"\\nRe-parsing DIAMOND results from folder '{diamond_results_folder_for_coverage}' to extract alignment details...\")\n",
    "best_hit_alignment_details = {} \n",
    "raw_diamond_files_coverage = glob.glob(os.path.join(diamond_results_folder_for_coverage, '*_diamond_hits.tsv'))\n",
    "\n",
    "if not raw_diamond_files_coverage:\n",
    "    logging.warning(f\"No '*_diamond_hits.tsv' files found in '{diamond_results_folder_for_coverage}'. Cannot calculate coverage.\")\n",
    "else:\n",
    "    logging.info(f\"Found {len(raw_diamond_files_coverage)} DIAMOND result files to process for coverage.\")\n",
    "    df_diamond_raw_list_coverage = []\n",
    "    # Use df_full (loaded in Cell 1) to get the list of ProteinIDs that need coverage info\n",
    "    # This avoids parsing hits for proteins not in our main DB (e.g., GVs if running only Asgard analysis)\n",
    "    target_protein_ids = set(df_full[protein_id_col].astype(str))\n",
    "    logging.info(f\"Will extract coverage details for up to {len(target_protein_ids)} target ProteinIDs.\")\n",
    "    \n",
    "    processed_files = 0\n",
    "    for f_path in raw_diamond_files_coverage:\n",
    "        try:\n",
    "            # Read only necessary columns\n",
    "            df_temp = pd.read_csv(f_path, sep='\\t', header=None, \n",
    "                                  names=diamond_col_names_for_coverage, \n",
    "                                  usecols=['qseqid', 'evalue', 'bitscore', 'qstart', 'qend', 'sstart', 'send', 'slen'])\n",
    "            df_temp['qseqid'] = df_temp['qseqid'].astype(str) # Ensure qseqid is string\n",
    "            # Filter early for relevant IDs and evalue threshold\n",
    "            df_temp = df_temp[df_temp['qseqid'].isin(target_protein_ids) & (df_temp['evalue'] <= e_value_threshold_for_coverage)]\n",
    "            if not df_temp.empty:\n",
    "                 df_diamond_raw_list_coverage.append(df_temp)\n",
    "            processed_files += 1\n",
    "            if processed_files % 500 == 0:\n",
    "                 logging.info(f\"  Processed {processed_files}/{len(raw_diamond_files_coverage)} DIAMOND files...\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"  Error reading or processing file {f_path} for coverage: {e}\")\n",
    "            continue\n",
    "\n",
    "    if df_diamond_raw_list_coverage:\n",
    "        df_diamond_all_hits_coverage = pd.concat(df_diamond_raw_list_coverage, ignore_index=True)\n",
    "        logging.info(f\"  Combined {len(df_diamond_all_hits_coverage)} relevant DIAMOND alignments passing e-value for coverage analysis.\")\n",
    "        \n",
    "        # No need to filter by e-value again, already done during read\n",
    "        \n",
    "        if not df_diamond_all_hits_coverage.empty:\n",
    "            # Sort to get the best hit (lowest e-value, highest bitscore)\n",
    "            df_diamond_all_hits_coverage.sort_values(by=['qseqid', 'evalue', 'bitscore'], ascending=[True, True, False], inplace=True)\n",
    "            df_best_hits_for_coverage = df_diamond_all_hits_coverage.drop_duplicates(subset=['qseqid'], keep='first').copy()\n",
    "            logging.info(f\"  Identified {len(df_best_hits_for_coverage)} unique query proteins with best hit details for coverage.\")\n",
    "            \n",
    "            # Store only the necessary details for coverage calculation\n",
    "            for _, row in df_best_hits_for_coverage.iterrows():\n",
    "                query_id = row['qseqid'] \n",
    "                # Convert coordinates to int, handle potential errors robustly\n",
    "                try:\n",
    "                    qstart = int(row['qstart'])\n",
    "                    qend = int(row['qend'])\n",
    "                    sstart = int(row['sstart'])\n",
    "                    send = int(row['send'])\n",
    "                    slen = int(row['slen'])\n",
    "                    \n",
    "                    best_hit_alignment_details[query_id] = {\n",
    "                        euk_hit_qstart_col: qstart, \n",
    "                        euk_hit_qend_col: qend,\n",
    "                        euk_hit_sstart_col: sstart, \n",
    "                        euk_hit_send_col: send,\n",
    "                        euk_hit_slen_diamond_col: slen\n",
    "                    }\n",
    "                except (ValueError, TypeError) as conv_err:\n",
    "                     logging.warning(f\"Could not convert alignment details to int for qseqid {query_id}. Skipping. Error: {conv_err}. Row data: {row.to_dict()}\")\n",
    "                     continue\n",
    "        else: \n",
    "            logging.info(\"  No DIAMOND hits passed the e-value threshold after combining files.\")\n",
    "    else: \n",
    "        logging.info(\"  No valid DIAMOND alignments loaded for coverage analysis.\")\n",
    "\n",
    "\n",
    "# --- Step 2.2: Merge Alignment Details into df_full (loaded from v2.0) ---\n",
    "logging.info(\"\\nMerging alignment details into main DataFrame...\")\n",
    "if best_hit_alignment_details:\n",
    "    df_alignment_details_to_merge = pd.DataFrame.from_dict(best_hit_alignment_details, orient='index')\n",
    "    df_alignment_details_to_merge.index.name = protein_id_col\n",
    "    # Ensure index is string type for merging\n",
    "    df_alignment_details_to_merge.index = df_alignment_details_to_merge.index.astype(str) \n",
    "    \n",
    "    # Define the columns to merge \n",
    "    cols_to_add_from_diamond = [\n",
    "        euk_hit_qstart_col, euk_hit_qend_col, euk_hit_sstart_col, euk_hit_send_col,\n",
    "        euk_hit_slen_diamond_col \n",
    "    ]\n",
    "    # Drop these columns from df_full if they already exist from a previous run\n",
    "    cols_exist = [col for col in cols_to_add_from_diamond if col in df_full.columns]\n",
    "    if cols_exist:\n",
    "        logging.info(f\"  Dropping existing alignment detail columns before merge: {cols_exist}\")\n",
    "        df_full = df_full.drop(columns=cols_exist)\n",
    "            \n",
    "    # Ensure ProteinID in df_full is also string before merging\n",
    "    if protein_id_col in df_full.columns:\n",
    "         df_full[protein_id_col] = df_full[protein_id_col].astype(str)\n",
    "    else:\n",
    "         logging.error(f\"Critical Error: '{protein_id_col}' not found in df_full. Cannot merge.\")\n",
    "         sys.exit(1) # Stop execution\n",
    "\n",
    "    df_full = df_full.merge(df_alignment_details_to_merge, on=protein_id_col, how='left')\n",
    "    logging.info(f\"  Merged alignment details. df_full shape: {df_full.shape}\")\n",
    "else:\n",
    "    logging.warning(\"No alignment details found to merge. Coverage calculation will fail.\")\n",
    "    # Add empty columns if they weren't added\n",
    "    for col in [euk_hit_qstart_col, euk_hit_qend_col, euk_hit_sstart_col, euk_hit_send_col, euk_hit_slen_diamond_col]:\n",
    "        if col not in df_full.columns: df_full[col] = np.nan\n",
    "\n",
    "# --- Step 2.3: Calculate Query and Subject Coverage ---\n",
    "logging.info(\"\\nCalculating Query and Subject Coverage...\")\n",
    "# Check if necessary columns exist\n",
    "can_calc_q_coverage = all(c in df_full.columns for c in [euk_hit_qstart_col, euk_hit_qend_col, original_seq_length_col])\n",
    "can_calc_s_coverage = all(c in df_full.columns for c in [euk_hit_sstart_col, euk_hit_send_col, euk_hit_slen_diamond_col])\n",
    "\n",
    "# Initialize coverage columns if they don't exist\n",
    "if query_coverage_col not in df_full.columns: df_full[query_coverage_col] = np.nan\n",
    "if subject_coverage_col not in df_full.columns: df_full[subject_coverage_col] = np.nan\n",
    "\n",
    "if can_calc_q_coverage:\n",
    "    # Convert columns to numeric, coercing errors\n",
    "    qstart = pd.to_numeric(df_full[euk_hit_qstart_col], errors='coerce')\n",
    "    qend = pd.to_numeric(df_full[euk_hit_qend_col], errors='coerce')\n",
    "    qlen = pd.to_numeric(df_full[original_seq_length_col], errors='coerce')\n",
    "    \n",
    "    # Calculate coverage only where all components are valid numbers and qlen > 0\n",
    "    valid_q_mask = qstart.notna() & qend.notna() & qlen.notna() & (qlen > 0)\n",
    "    # Ensure indices align before calculation using .loc\n",
    "    df_full.loc[valid_q_mask, query_coverage_col] = (qend[valid_q_mask] - qstart[valid_q_mask] + 1) / qlen[valid_q_mask]\n",
    "    logging.info(f\"  Calculated '{query_coverage_col}'. NaN count: {df_full[query_coverage_col].isna().sum()}\")\n",
    "else:\n",
    "    logging.warning(f\"  Cannot calculate '{query_coverage_col}' due to missing columns.\")\n",
    "    \n",
    "\n",
    "if can_calc_s_coverage:\n",
    "    sstart = pd.to_numeric(df_full[euk_hit_sstart_col], errors='coerce')\n",
    "    ssend = pd.to_numeric(df_full[euk_hit_send_col], errors='coerce')\n",
    "    slen = pd.to_numeric(df_full[euk_hit_slen_diamond_col], errors='coerce')\n",
    "\n",
    "    # Calculate coverage only where all components are valid numbers and slen > 0\n",
    "    valid_s_mask = sstart.notna() & ssend.notna() & slen.notna() & (slen > 0)\n",
    "    # Ensure indices align before calculation using .loc\n",
    "    df_full.loc[valid_s_mask, subject_coverage_col] = (ssend[valid_s_mask] - sstart[valid_s_mask] + 1) / slen[valid_s_mask]\n",
    "    logging.info(f\"  Calculated '{subject_coverage_col}'. NaN count: {df_full[subject_coverage_col].isna().sum()}\")\n",
    "else:\n",
    "    logging.warning(f\"  Cannot calculate '{subject_coverage_col}' due to missing columns.\")\n",
    "    \n",
    "\n",
    "# --- Step 2.4: Define High-Quality Hits DataFrame ---\n",
    "logging.info(f\"\\n--- Defining High-Quality Hits DataFrame ---\")\n",
    "# Use the filter criteria defined in Cell 1\n",
    "required_filter_cols = [group_col, hit_flag_col, query_coverage_col, subject_coverage_col, euk_hit_pident_col]\n",
    "if all(col in df_full.columns for col in required_filter_cols):\n",
    "     # Ensure coverage columns are numeric before filtering (might be redundant but safe)\n",
    "     df_full[query_coverage_col] = pd.to_numeric(df_full[query_coverage_col], errors='coerce')\n",
    "     df_full[subject_coverage_col] = pd.to_numeric(df_full[subject_coverage_col], errors='coerce')\n",
    "     \n",
    "     # Filter df_full (which now has coverage) to create df_high_quality_hits\n",
    "     df_high_quality_hits = df_full[\n",
    "        (df_full[group_col] == 'Asgard') & \n",
    "        (df_full[hit_flag_col] == True) &\n",
    "        (df_full[query_coverage_col].notna()) & # Ensure coverage was calculated\n",
    "        (df_full[subject_coverage_col].notna()) &\n",
    "        (df_full[query_coverage_col] >= min_query_coverage_filter) &\n",
    "        (df_full[subject_coverage_col] >= min_subject_coverage_filter) &\n",
    "        (df_full[euk_hit_pident_col] >= min_pident_filter)\n",
    "    ].copy() # IMPORTANT: Use .copy() to avoid SettingWithCopyWarning later\n",
    "     logging.info(f\"Defined 'df_high_quality_hits' with {len(df_high_quality_hits)} rows based on:\")\n",
    "     logging.info(f\"  Query Coverage >= {min_query_coverage_filter*100:.0f}%\")\n",
    "     logging.info(f\"  Subject Coverage >= {min_subject_coverage_filter*100:.0f}%\")\n",
    "     logging.info(f\"  Percent Identity >= {min_pident_filter}%\")\n",
    "else:\n",
    "     logging.error(f\"Cannot define 'df_high_quality_hits', missing required columns: {[c for c in required_filter_cols if c not in df_full.columns]}\")\n",
    "     # Define as empty df to allow subsequent cells to check .empty\n",
    "     df_high_quality_hits = pd.DataFrame(columns=df_full.columns if 'df_full' in locals() else []) \n",
    "\n",
    "\n",
    "# --- Step 2.5: Analyze Coverage Distributions (Optional Plotting) ---\n",
    "if not df_high_quality_hits.empty:\n",
    "    logging.info(f\"\\n--- Analyzing Alignment Coverage Distributions for High-Quality Hits---\")\n",
    "    # (Plots are commented out for brevity, uncomment if needed)\n",
    "    logging.info(f\"\\nDistribution of '{query_coverage_col}':\")\n",
    "    print(df_high_quality_hits[query_coverage_col].describe())\n",
    "    # fig_qcov_hist = px.histogram(...) fig_qcov_hist.show()\n",
    "    logging.info(f\"\\nDistribution of '{subject_coverage_col}':\")\n",
    "    print(df_high_quality_hits[subject_coverage_col].describe())\n",
    "    # fig_scov_hist = px.histogram(...) fig_scov_hist.show()\n",
    "else:\n",
    "    logging.warning(\"No high-quality hits defined, skipping coverage distribution analysis.\")\n",
    "\n",
    "# --- Step 2.6: Save the updated df_full with coverage ---\n",
    "# This DataFrame now contains the original v2.0 info PLUS the coverage columns\n",
    "logging.info(f\"\\nSaving updated database with coverage info to: {output_db_with_coverage_path}\")\n",
    "try:\n",
    "    # Ensure output directory exists - Use Path object methods\n",
    "    output_db_with_coverage_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    # Save the df_full dataframe which contains ALL proteins, now updated with coverage\n",
    "    df_full.to_csv(output_db_with_coverage_path, index=False)\n",
    "    logging.info(f\"Successfully saved updated database (df_full) to '{output_db_with_coverage_path}'.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to save updated database with coverage: {e}\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n--- Cell 2 (Add Coverage & Define High-Quality Hits) Complete ---\")\n",
    "print(\"The DataFrame 'df_full' has been updated with alignment details and coverage.\")\n",
    "print(\"The DataFrame 'df_high_quality_hits' is now defined based on filtering criteria.\")\n",
    "print(f\"The fully updated database (df_full) is saved as '{output_db_with_coverage_path}'.\")\n",
    "print(\"Subsequent cells can now use 'df_high_quality_hits'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41e7b4d-f660-4049-b3df-d6953511024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Phase 2.3 - Examine Taxonomic Distribution of Eukaryotic Hits\n",
    "\n",
    "# --- Standard Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    " \n",
    "# --- Plotly Imports ---\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "print(\"\\n\\n--- Cell 3: Phase 2.3 - Examine Taxonomic Distribution of Eukaryotic Hits ---\")\n",
    "\n",
    "# --- Assumptions ---\n",
    "# This cell assumes Cell 1 and Cell 2 have been run successfully.\n",
    "# It relies on the following DataFrames being available and populated:\n",
    "#   - df_full: The main DataFrame loaded in Cell 1.\n",
    "#   - df_asgard_hits: Asgard proteins with any hit (from Cell 1/2).\n",
    "#   - df_high_quality_hits: Asgard proteins filtered for hit quality in Cell 2.\n",
    "# It also relies on column name variables (e.g., euk_hit_organism_col, esp_col),\n",
    "# color palettes (e.g., arcadia_primary_manual, arcadia_secondary_manual),\n",
    "# and the output directory variables (e.g., output_summary_dir_phase1) defined in Cell 1.\n",
    "\n",
    "# --- Check if necessary DataFrames exist ---\n",
    "if 'df_high_quality_hits' not in locals():\n",
    "    print(\"ERROR: DataFrame 'df_high_quality_hits' not found. Please run Cell 2 first.\")\n",
    "    if 'df_asgard_hits' in locals() and not df_asgard_hits.empty:\n",
    "        print(\"WARNING: Using 'df_asgard_hits' (all hits) for taxonomic analysis as 'df_high_quality_hits' is missing.\")\n",
    "        df_to_analyze_taxa = df_asgard_hits\n",
    "    else:\n",
    "        print(\"ERROR: Neither 'df_high_quality_hits' nor 'df_asgard_hits' are available. Skipping taxonomic analysis.\")\n",
    "        df_to_analyze_taxa = pd.DataFrame() \n",
    "elif df_high_quality_hits.empty:\n",
    "     print(\"WARNING: DataFrame 'df_high_quality_hits' is empty (likely due to filtering criteria in Cell 2 or no initial hits).\")\n",
    "     print(\"         Attempting to use 'df_asgard_hits' if available, otherwise skipping taxonomic analysis.\")\n",
    "     if 'df_asgard_hits' in locals() and not df_asgard_hits.empty:\n",
    "        df_to_analyze_taxa = df_asgard_hits\n",
    "        print(\"         Using 'df_asgard_hits' (all hits passing e-value) for taxonomic analysis.\")\n",
    "     else:\n",
    "        df_to_analyze_taxa = pd.DataFrame() \n",
    "else:\n",
    "    df_to_analyze_taxa = df_high_quality_hits\n",
    "    print(\"Using 'df_high_quality_hits' (filtered by coverage/pident) for taxonomic analysis.\")\n",
    "\n",
    "# --- Perform Taxonomic Analysis ---\n",
    "if not df_to_analyze_taxa.empty and euk_hit_organism_col in df_to_analyze_taxa.columns:\n",
    "    print(f\"\\nAnalyzing distribution of '{euk_hit_organism_col}' for {len(df_to_analyze_taxa)} Asgard hits...\")\n",
    "\n",
    "    # Clean organism names (remove potential leading/trailing spaces)\n",
    "    df_to_analyze_taxa[euk_hit_organism_col] = df_to_analyze_taxa[euk_hit_organism_col].astype(str).str.strip()\n",
    "    \n",
    "    # Replace empty strings or common placeholders with 'Unknown'\n",
    "    df_to_analyze_taxa[euk_hit_organism_col] = df_to_analyze_taxa[euk_hit_organism_col].replace(['', 'nan', 'None', 'Unknown'], 'Unknown Organism')\n",
    "\n",
    "    # 1. Overall Top Hit Organisms\n",
    "    top_n_orgs = 30 # How many top organisms to show/color\n",
    "    organism_counts = df_to_analyze_taxa[euk_hit_organism_col].value_counts()\n",
    "    \n",
    "    print(f\"\\nTop {top_n_orgs} Eukaryotic Hit Organisms (from selected hits):\")\n",
    "    try:\n",
    "        print(organism_counts.head(top_n_orgs).to_markdown())\n",
    "    except ImportError:\n",
    "        print(organism_counts.head(top_n_orgs))\n",
    "\n",
    "    # --- Create Color Map for Top Eukaryotic Organisms ---\n",
    "    print(f\"\\nCreating color map for top {top_n_orgs} eukaryotic organisms...\")\n",
    "    euk_organism_color_map = {}\n",
    "    top_org_list = organism_counts.head(top_n_orgs).index.tolist()\n",
    "    # Combine primary and secondary palettes for more colors\n",
    "    euk_palette = arcadia_primary_manual + arcadia_secondary_manual \n",
    "    \n",
    "    for i, org_name in enumerate(top_org_list):\n",
    "        if org_name == 'Unknown Organism':\n",
    "             # Assign a specific neutral color to 'Unknown Organism' if it's in the top list\n",
    "             euk_organism_color_map[org_name] = arcadia_colors_manual.get('gray', '#bdbdbd')\n",
    "        else:\n",
    "            # Cycle through the combined palette\n",
    "            euk_organism_color_map[org_name] = euk_palette[i % len(euk_palette)]\n",
    "            \n",
    "    # Assign a default color for organisms not in the top N (optional, Plotly handles this)\n",
    "    # default_other_color = arcadia_colors_manual.get('chateau', '#BAB0A8') \n",
    "    print(f\"Color map created for {len(euk_organism_color_map)} organisms.\")\n",
    "\n",
    "    # Plot Overall Distribution using the color map\n",
    "    df_org_plot = organism_counts.head(top_n_orgs).reset_index()\n",
    "    df_org_plot.columns = [euk_hit_organism_col, 'Count'] \n",
    "\n",
    "    fig_org_overall = px.bar(\n",
    "        df_org_plot,\n",
    "        x=euk_hit_organism_col,\n",
    "        y='Count',\n",
    "        color=euk_hit_organism_col, # Color bars by organism\n",
    "        color_discrete_map=euk_organism_color_map, # Apply the created color map\n",
    "        title=f'Top {top_n_orgs} Eukaryotic Hit Organisms for Selected Asgard Hits',\n",
    "        labels={euk_hit_organism_col: 'Eukaryotic Organism (Best Hit)', 'Count': 'Number of Asgard Proteins Hit'}\n",
    "    )\n",
    "    fig_org_overall.update_xaxes(tickangle=45, categoryorder='total descending') \n",
    "    fig_org_overall.update_layout(showlegend=False) # Hide legend if too many items\n",
    "    fig_org_overall.show()\n",
    "\n",
    "    # 2. Top Hit Organisms for ESPs\n",
    "    if esp_col in df_to_analyze_taxa.columns:\n",
    "        df_esps_hits_subset = df_to_analyze_taxa[df_to_analyze_taxa[esp_col] == True]\n",
    "        if not df_esps_hits_subset.empty:\n",
    "            print(f\"\\nAnalyzing distribution of '{euk_hit_organism_col}' for {len(df_esps_hits_subset)} Asgard ESP hits...\")\n",
    "            organism_counts_esps = df_esps_hits_subset[euk_hit_organism_col].value_counts()\n",
    "            \n",
    "            print(f\"\\nTop {top_n_orgs} Eukaryotic Hit Organisms (Selected ESP Hits):\")\n",
    "            try:\n",
    "                print(organism_counts_esps.head(top_n_orgs).to_markdown())\n",
    "            except ImportError:\n",
    "                print(organism_counts_esps.head(top_n_orgs))\n",
    "\n",
    "            # Plot ESP Distribution using the SAME color map for consistency\n",
    "            df_org_plot_esps = organism_counts_esps.head(top_n_orgs).reset_index()\n",
    "            df_org_plot_esps.columns = [euk_hit_organism_col, 'Count']\n",
    "\n",
    "            fig_org_esps = px.bar(\n",
    "                df_org_plot_esps,\n",
    "                x=euk_hit_organism_col,\n",
    "                y='Count',\n",
    "                color=euk_hit_organism_col, # Color bars by organism\n",
    "                color_discrete_map=euk_organism_color_map, # Apply the created color map\n",
    "                title=f'Top {top_n_orgs} Eukaryotic Hit Organisms for Selected Asgard ESP Hits',\n",
    "                labels={euk_hit_organism_col: 'Eukaryotic Organism (Best Hit)', 'Count': 'Number of Asgard ESPs Hit'}\n",
    "            )\n",
    "            fig_org_esps.update_xaxes(tickangle=45, categoryorder='total descending')\n",
    "            fig_org_esps.update_layout(showlegend=False) # Hide legend if too many items\n",
    "            fig_org_esps.show()\n",
    "\n",
    "            # Save ESP organism counts\n",
    "            esp_org_counts_filename = os.path.join(output_summary_dir_phase1, \"esp_euk_hit_organism_counts.csv\")\n",
    "            try:\n",
    "                # Save the full list, not just top N\n",
    "                organism_counts_esps.reset_index().rename(columns={'index':euk_hit_organism_col, euk_hit_organism_col:'Count'}).to_csv(esp_org_counts_filename, index=False)\n",
    "                print(f\"Saved ESP hit organism counts to '{esp_org_counts_filename}'\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving ESP hit organism counts: {e}\")\n",
    "        else:\n",
    "            print(\"\\nNo hits found for Asgard ESPs in the selected subset to analyze taxonomically.\")\n",
    "    else:\n",
    "        print(f\"\\nSkipping ESP-specific taxonomic analysis (column '{esp_col}' not found in the selected hits).\")\n",
    "\n",
    "    # Save Overall organism counts\n",
    "    overall_org_counts_filename = os.path.join(output_summary_dir_phase1, \"overall_euk_hit_organism_counts.csv\")\n",
    "    try:\n",
    "        # Save the full list\n",
    "        organism_counts.reset_index().rename(columns={'index':euk_hit_organism_col, euk_hit_organism_col:'Count'}).to_csv(overall_org_counts_filename, index=False)\n",
    "        print(f\"Saved overall hit organism counts to '{overall_org_counts_filename}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving overall hit organism counts: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo Asgard hits available in the selected DataFrame ('df_to_analyze_taxa') to analyze eukaryotic organism distribution.\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n--- Cell 3 (Taxonomic Distribution Analysis) Complete ---\")\n",
    "print(\"Next steps from the plan would involve:\")\n",
    "print(\"  - Phase 2.1: Comparing Domain Architectures (requires Euk hit domain info).\")\n",
    "print(\"  - Phase 2.2: Analyzing Functional Annotations (requires fetching Euk hit functions).\")\n",
    "print(\"  - Phase 2.4: Cross-referencing with external orthology data.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8e51a3-fb52-4c3f-9fa6-63738c6052ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Statistical Profiling of Asgard Proteins with High-Quality Eukaryotic Hits\n",
    "\n",
    "# --- Standard Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "# --- Plotly Imports ---\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "print(\"\\n\\n--- Cell 4: Profiling Asgard Proteins with High-Quality Eukaryotic Hits ---\")\n",
    "\n",
    "# --- Assumptions ---\n",
    "# This cell assumes Cell 1, 2, and 3 have been run successfully.\n",
    "# It relies on the following DataFrames being available and populated:\n",
    "#   - df_full: The main DataFrame loaded in Cell 1.\n",
    "#   - df_high_quality_hits: Asgard proteins filtered for hit quality in Cell 2.\n",
    "#   - df_asgard_all: Baseline Asgard proteins from df_full (created in this cell if needed)\n",
    "# It also relies on column name variables (e.g., group_col, structurally_dark_col, \n",
    "# broad_func_cat_col, esp_col, num_domains_col) and color maps (arcadia_colors_manual) defined in Cell 1.\n",
    "\n",
    "# --- Check if necessary DataFrames exist ---\n",
    "if 'df_high_quality_hits' not in locals():\n",
    "    print(\"ERROR: DataFrame 'df_high_quality_hits' not found. Please run Cell 2 first.\")\n",
    "    # Stop or create empty df to prevent errors\n",
    "    df_high_quality_hits = pd.DataFrame() \n",
    "elif df_high_quality_hits.empty:\n",
    "     print(\"WARNING: DataFrame 'df_high_quality_hits' is empty. Profiling results will be empty.\")\n",
    "     # Continue with empty df for now, but analysis will show 0 counts\n",
    "\n",
    "if 'df_full' not in locals():\n",
    "     print(\"ERROR: DataFrame 'df_full' not found. Cannot calculate baseline Asgard profiles. Please run Cell 1.\")\n",
    "     # Stop or create empty df\n",
    "     df_asgard_all = pd.DataFrame()\n",
    "else:\n",
    "     # Get the baseline Asgard DataFrame if not already done\n",
    "     if 'df_asgard_all' not in locals() or df_asgard_all.empty:\n",
    "          print(\"Creating baseline df_asgard_all from df_full.\")\n",
    "          df_asgard_all = df_full[df_full[group_col] == 'Asgard'].copy()\n",
    "     \n",
    "     if df_asgard_all.empty:\n",
    "         print(\"WARNING: No Asgard proteins found in the main DataFrame ('df_full'). Baseline comparison not possible.\")\n",
    "\n",
    "\n",
    "# --- Helper Function for Comparative Profiling (Corrected) ---\n",
    "def plot_comparison_bar(df_subset, df_baseline, column_name, title_suffix, color_map=None, category_orders=None):\n",
    "    \"\"\"Generates a grouped bar chart comparing percentages in a subset vs baseline.\"\"\"\n",
    "    # Ensure color_map uses the correct keys if provided (it's not used in px.bar here, but good practice)\n",
    "    # This function now ignores the passed color_map for the px.bar call itself,\n",
    "    # as coloring is fixed to the 'Group' column.\n",
    "    \n",
    "    if column_name not in df_subset.columns or column_name not in df_baseline.columns:\n",
    "        print(f\"  Skipping plot for '{column_name}': Column not found in one or both DataFrames.\")\n",
    "        return\n",
    "\n",
    "    # Calculate percentages for the subset (high-quality hits)\n",
    "    subset_counts = df_subset[column_name].value_counts(normalize=True, dropna=False).mul(100)\n",
    "    subset_counts.name = 'High-Quality Hits (%)'\n",
    "\n",
    "    # Calculate percentages for the baseline (all Asgard)\n",
    "    baseline_counts = df_baseline[column_name].value_counts(normalize=True, dropna=False).mul(100)\n",
    "    baseline_counts.name = 'All Asgard (%)'\n",
    "\n",
    "    # Combine the percentages\n",
    "    df_compare = pd.concat([subset_counts, baseline_counts], axis=1).fillna(0).reset_index()\n",
    "    # Rename index column safely\n",
    "    df_compare.rename(columns={df_compare.columns[0]: column_name}, inplace=True)\n",
    "    \n",
    "    # Melt for plotting with plotly express\n",
    "    df_melted = df_compare.melt(id_vars=column_name, var_name='Group', value_name='Percentage')\n",
    "\n",
    "    # Sort categories if order provided, otherwise sort by baseline percentage\n",
    "    xaxis_categoryorder = None\n",
    "    xaxis_categoryarray = None\n",
    "    if category_orders and column_name in category_orders:\n",
    "         cat_order = category_orders[column_name]\n",
    "         # Ensure all values in df_melted are in the category order list\n",
    "         current_cats = df_melted[column_name].unique()\n",
    "         ordered_cats = [c for c in cat_order if c in current_cats]\n",
    "         # Add any missing categories from data to the end\n",
    "         missing_cats = [c for c in current_cats if c not in ordered_cats]\n",
    "         final_cat_order = ordered_cats + missing_cats\n",
    "         \n",
    "         df_melted[column_name] = pd.Categorical(df_melted[column_name], categories=final_cat_order, ordered=True)\n",
    "         df_melted = df_melted.sort_values(column_name) # Sort based on the defined category order\n",
    "         xaxis_categoryorder = 'array' \n",
    "         xaxis_categoryarray = final_cat_order # Use the final computed order\n",
    "    else:\n",
    "        # Default sort: Order categories by the baseline percentage descending for better viz\n",
    "        default_order = baseline_counts.sort_values(ascending=False).index.tolist()\n",
    "        df_melted[column_name] = pd.Categorical(df_melted[column_name], categories=default_order, ordered=True)\n",
    "        df_melted = df_melted.sort_values(column_name)\n",
    "        xaxis_categoryorder = 'array' # Use the order defined by the categorical type\n",
    "        xaxis_categoryarray = default_order\n",
    "\n",
    "\n",
    "    print(f\"\\nComparison for '{column_name}':\")\n",
    "    try:\n",
    "        print(df_compare.round(1).to_markdown(index=False))\n",
    "    except ImportError:\n",
    "        print(df_compare.round(1))\n",
    "\n",
    "    # Create the plot (Corrected: Removed second color_discrete_map)\n",
    "    fig = px.bar(\n",
    "        df_melted,\n",
    "        x=column_name,\n",
    "        y='Percentage',\n",
    "        color='Group', # Colors bars based on 'Group' column\n",
    "        barmode='group',\n",
    "        title=f'{column_name} Distribution: High-Quality Hits vs. All Asgard {title_suffix}',\n",
    "        labels={'Percentage': '% of Proteins', column_name: column_name.replace('_', ' ').title()}, # Nicer axis label\n",
    "        color_discrete_map={'High-Quality Hits (%)': arcadia_colors_manual.get('amber', '#F28360'), \n",
    "                            'All Asgard (%)': arcadia_colors_manual.get('aegean', '#5088C5')}\n",
    "        # Removed the problematic line: **({f'color_discrete_map': color_map} ... )\n",
    "    )\n",
    "    \n",
    "    # Apply category order to x-axis if needed (overrides default plotly sorting)\n",
    "    if xaxis_categoryorder == 'array':\n",
    "         fig.update_xaxes(categoryorder=xaxis_categoryorder, categoryarray=xaxis_categoryarray)\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# --- Perform Profiling ---\n",
    "\n",
    "if not df_high_quality_hits.empty and not df_asgard_all.empty:\n",
    "    print(f\"\\n--- Profiling {len(df_high_quality_hits)} Asgard proteins with high-quality eukaryotic hits ---\")\n",
    "\n",
    "    # 1. Structural Darkness Profile\n",
    "    if structurally_dark_col in df_high_quality_hits.columns:\n",
    "        plot_comparison_bar(\n",
    "            df_subset=df_high_quality_hits,\n",
    "            df_baseline=df_asgard_all,\n",
    "            column_name=structurally_dark_col,\n",
    "            title_suffix=\"(True = Dark)\"\n",
    "            # No color_map needed here as x-axis is boolean\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Skipping Structural Darkness profile: Column '{structurally_dark_col}' not found.\")\n",
    "\n",
    "    # 2. Broad Functional Category Profile\n",
    "    if broad_func_cat_col in df_high_quality_hits.columns:\n",
    "        # Get category order based on baseline abundance\n",
    "        baseline_cat_order = df_asgard_all[broad_func_cat_col].value_counts().index.tolist()\n",
    "        plot_comparison_bar(\n",
    "            df_subset=df_high_quality_hits,\n",
    "            df_baseline=df_asgard_all,\n",
    "            column_name=broad_func_cat_col,\n",
    "            title_suffix=\"\",\n",
    "            # color_map=broad_category_color_map, # Not used by px.bar in this setup\n",
    "            category_orders = {broad_func_cat_col: baseline_cat_order} # Pass the order for sorting x-axis\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Skipping Broad Functional Category profile: Column '{broad_func_cat_col}' not found.\")\n",
    "\n",
    "    # 3. ESP Profile\n",
    "    if esp_col in df_high_quality_hits.columns:\n",
    "        plot_comparison_bar(\n",
    "            df_subset=df_high_quality_hits,\n",
    "            df_baseline=df_asgard_all,\n",
    "            column_name=esp_col,\n",
    "            title_suffix=\"(True = ESP)\"\n",
    "            # No color_map needed here\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Skipping ESP profile: Column '{esp_col}' not found.\")\n",
    "\n",
    "    # 4. Domain-less Profile\n",
    "    if num_domains_col in df_high_quality_hits.columns:\n",
    "        # Create a temporary boolean column: True if domain-less (NaN), False otherwise\n",
    "        domainless_col_temp = 'Is_DomainLess'\n",
    "        # Ensure the temporary column is added before calling the plot function\n",
    "        if domainless_col_temp not in df_high_quality_hits.columns:\n",
    "             df_high_quality_hits[domainless_col_temp] = df_high_quality_hits[num_domains_col].isna()\n",
    "        if domainless_col_temp not in df_asgard_all.columns:\n",
    "             df_asgard_all[domainless_col_temp] = df_asgard_all[num_domains_col].isna()\n",
    "        \n",
    "        plot_comparison_bar(\n",
    "            df_subset=df_high_quality_hits,\n",
    "            df_baseline=df_asgard_all,\n",
    "            column_name=domainless_col_temp,\n",
    "            title_suffix=\"(True = Domain-less)\"\n",
    "            # No color_map needed here\n",
    "        )\n",
    "        # Clean up temporary column if desired (optional)\n",
    "        # if domainless_col_temp in df_high_quality_hits.columns:\n",
    "        #     df_high_quality_hits.drop(columns=[domainless_col_temp], inplace=True)\n",
    "        # if domainless_col_temp in df_asgard_all.columns:\n",
    "        #     df_asgard_all.drop(columns=[domainless_col_temp], inplace=True)\n",
    "    else:\n",
    "        print(f\"Skipping Domain-less profile: Column '{num_domains_col}' not found.\")\n",
    "\n",
    "else:\n",
    "     print(\"\\nSkipping profiling analysis because either 'df_high_quality_hits' or 'df_asgard_all' is empty or unavailable.\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n--- Cell 4 (Statistical Profiling) Complete ---\")\n",
    "print(\"This cell compared the profiles of Asgard proteins with high-quality eukaryotic hits\")\n",
    "print(\"against the overall Asgard proteome for key characteristics.\")\n",
    "print(\"Next steps involve deeper functional and domain comparisons (Phase 2.1, 2.2, 2.4).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed1128d-9f0c-446a-b77a-d9809dfd1a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Extract Unique Eukaryotic Hit IDs for External API Script\n",
    "\n",
    "# --- Standard Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(\"\\n\\n--- Cell 5: Extract Eukaryotic Hit IDs for External API Script ---\")\n",
    "\n",
    "# --- Assumptions ---\n",
    "# This cell assumes Cell 1 and Cell 2 have been run successfully.\n",
    "# It relies on 'df_high_quality_hits' being available and populated from Cell 2.\n",
    "# It also relies on column name variables (e.g., euk_hit_sseqid_col) and \n",
    "# output directory variables (e.g., output_summary_dir_phase1) defined in Cell 1.\n",
    "\n",
    "# --- Check if necessary DataFrame exists ---\n",
    "if 'df_high_quality_hits' not in locals() or df_high_quality_hits.empty:\n",
    "    print(\"ERROR: DataFrame 'df_high_quality_hits' not found or is empty.\")\n",
    "    print(\"       Cannot extract IDs. Please ensure Cell 2 ran successfully and generated results.\")\n",
    "    ids_to_save = [] # Ensure variable exists but is empty\n",
    "else:\n",
    "    print(f\"Using 'df_high_quality_hits' (shape: {df_high_quality_hits.shape}) to extract IDs.\")\n",
    "    \n",
    "    # --- Extract Unique Eukaryotic SSEQIDs ---\n",
    "    if euk_hit_sseqid_col in df_high_quality_hits.columns:\n",
    "        unique_euk_sseqids = df_high_quality_hits[euk_hit_sseqid_col].dropna().unique()\n",
    "        ids_to_save = unique_euk_sseqids.tolist()\n",
    "        print(f\"Found {len(ids_to_save)} unique eukaryotic SSEQIDs to save for mapping.\")\n",
    "    else:\n",
    "        print(f\"ERROR: Column '{euk_hit_sseqid_col}' not found in df_high_quality_hits.\")\n",
    "        ids_to_save = []\n",
    "\n",
    "# --- Save IDs to File ---\n",
    "if ids_to_save:\n",
    "    output_id_list_filename = os.path.join(output_summary_dir_phase1, \"eukaryotic_hit_sseqids_to_map.txt\")\n",
    "    try:\n",
    "        with open(output_id_list_filename, 'w') as f_out:\n",
    "            for prot_id in ids_to_save:\n",
    "                f_out.write(f\"{prot_id}\\n\")\n",
    "        print(f\"\\nSuccessfully saved {len(ids_to_save)} unique eukaryotic hit IDs to:\")\n",
    "        print(f\"  '{output_id_list_filename}'\")\n",
    "        print(\"\\nNext Steps:\")\n",
    "        print(f\"1. Modify your 'uniprot_api_lookup.py' script:\")\n",
    "        print(f\"   - Change the input loading section to read IDs from '{output_id_list_filename}'.\")\n",
    "        print(f\"   - Change the 'FROM_DB' variable to 'P_RefSeq_AC' (if IDs are XP_/NP_).\")\n",
    "        print(f\"   - Adjust 'OUTPUT_MAPPING_FILE' path if desired.\")\n",
    "        print(f\"2. Run the modified script from your terminal.\")\n",
    "        print(f\"3. Use the resulting mapping file for the next step: fetching annotations.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR: Could not save the ID list to file. Error: {e}\")\n",
    "else:\n",
    "    print(\"\\nNo unique eukaryotic hit IDs found or extracted to save.\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n--- Cell 5 (Extract Euk Hit IDs) Complete ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f960a0a8-4534-424e-b699-86234f9af3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Summary Statistics Visualization for Eukaryotic Hits in Asgard Subsets\n",
    "\n",
    "# --- Standard Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "# --- Plotly Imports ---\n",
    "# Assuming plotly express and graph_objects were imported as px and go in Cell 1\n",
    "# import plotly.express as px \n",
    "# import plotly.graph_objects as go \n",
    "\n",
    "print(\"\\n\\n--- Cell 6: Summary Statistics Visualization ---\")\n",
    "\n",
    "# --- Assumptions ---\n",
    "# This cell assumes Cell 1 has been run successfully.\n",
    "# It relies on 'df_full' being available and populated with columns:\n",
    "#   group_col, hit_flag_col, esp_col, structurally_dark_col, \n",
    "#   num_domains_col, broad_func_cat_col\n",
    "# It also relies on color palettes (e.g., arcadia_primary_manual) defined in Cell 1.\n",
    "\n",
    "# --- Check if necessary DataFrame exists ---\n",
    "if 'df_full' not in locals() or df_full.empty:\n",
    "    print(\"ERROR: DataFrame 'df_full' not found or is empty. Please run Cell 1 first.\")\n",
    "    # Stop or create empty df to prevent errors\n",
    "    df_full = pd.DataFrame() \n",
    "    df_asgard_all = pd.DataFrame() # Ensure this is also empty\n",
    "else:\n",
    "    # Ensure df_asgard_all exists from Cell 1/4 or create it\n",
    "    if 'df_asgard_all' not in locals() or df_asgard_all.empty:\n",
    "        print(\"Creating baseline df_asgard_all from df_full for summary.\")\n",
    "        if group_col in df_full.columns:\n",
    "            df_asgard_all = df_full[df_full[group_col] == 'Asgard'].copy()\n",
    "        else:\n",
    "            print(f\"ERROR: Cannot create df_asgard_all, missing '{group_col}'.\")\n",
    "            df_asgard_all = pd.DataFrame() # Ensure it's defined but empty\n",
    "            \n",
    "    if df_asgard_all.empty:\n",
    "         print(\"WARNING: No Asgard proteins found in 'df_full'. Summary will be empty.\")\n",
    "\n",
    "\n",
    "# --- Define and Analyze Subsets ---\n",
    "summary_data = []\n",
    "\n",
    "if not df_asgard_all.empty:\n",
    "    print(f\"Analyzing {len(df_asgard_all)} total Asgard proteins...\")\n",
    "    \n",
    "    # Define subsets based on columns\n",
    "    subsets_to_analyze = {\n",
    "        \"Overall Asgard\": df_asgard_all,\n",
    "        \"Asgard ESPs\": df_asgard_all[df_asgard_all[esp_col] == True] if esp_col in df_asgard_all.columns else pd.DataFrame(),\n",
    "        \"Structurally Dark\": df_asgard_all[df_asgard_all[structurally_dark_col] == True] if structurally_dark_col in df_asgard_all.columns else pd.DataFrame(),\n",
    "        \"Domain-less\": df_asgard_all[df_asgard_all[num_domains_col].isna()] if num_domains_col in df_asgard_all.columns else pd.DataFrame(),\n",
    "        \"Unknown/Unclassified\": df_asgard_all[df_asgard_all[broad_func_cat_col] == 'Unknown/Unclassified'] if broad_func_cat_col in df_asgard_all.columns else pd.DataFrame()\n",
    "    }\n",
    "\n",
    "    # Calculate stats for each subset\n",
    "    for name, df_subset in subsets_to_analyze.items():\n",
    "        print(f\"\\nProcessing subset: {name}\")\n",
    "        \n",
    "        # Check if subset DataFrame is valid and has the hit flag column\n",
    "        if df_subset.empty or hit_flag_col not in df_subset.columns:\n",
    "            if df_subset.empty:\n",
    "                 print(f\"  Subset '{name}' is empty (possibly due to missing definition column or no matching data). Skipping.\")\n",
    "            else:\n",
    "                 print(f\"  Skipping subset '{name}': Missing '{hit_flag_col}' column.\")\n",
    "            total_subset = 0\n",
    "            hits_in_subset = 0\n",
    "            percent_hits = 0.0\n",
    "        else:\n",
    "            total_subset = len(df_subset)\n",
    "            hits_in_subset = df_subset[hit_flag_col].sum()\n",
    "            percent_hits = (hits_in_subset / total_subset * 100) if total_subset > 0 else 0.0\n",
    "            print(f\"  Total Proteins: {total_subset}\")\n",
    "            print(f\"  With Euk Hit: {hits_in_subset}\")\n",
    "            print(f\"  Percentage: {percent_hits:.1f}%\")\n",
    "            \n",
    "        summary_data.append({\n",
    "            'Category': name,\n",
    "            'Total Proteins': total_subset,\n",
    "            'Proteins with Euk Hit': hits_in_subset,\n",
    "            'Percentage with Euk Hit (%)': percent_hits\n",
    "        })\n",
    "\n",
    "else:\n",
    "    print(\"Skipping summary calculation as no Asgard proteins were found.\")\n",
    "\n",
    "# --- Create and Display Summary DataFrame ---\n",
    "if summary_data:\n",
    "    df_summary = pd.DataFrame(summary_data)\n",
    "    \n",
    "    print(\"\\n\\n--- Summary Table: Eukaryotic Hits in Asgard Subsets ---\")\n",
    "    try:\n",
    "        # Format the table nicely\n",
    "        df_display_summary = df_summary.copy()\n",
    "        df_display_summary['Total Proteins'] = df_display_summary['Total Proteins'].map('{:,.0f}'.format)\n",
    "        df_display_summary['Proteins with Euk Hit'] = df_display_summary['Proteins with Euk Hit'].map('{:,.0f}'.format)\n",
    "        df_display_summary['Percentage with Euk Hit (%)'] = df_display_summary['Percentage with Euk Hit (%)'].map('{:.1f}%'.format)\n",
    "        print(df_display_summary.to_markdown(index=False))\n",
    "    except ImportError:\n",
    "        print(df_summary) # Fallback if tabulate not installed\n",
    "\n",
    "    # --- Generate Plot ---\n",
    "    print(\"\\n--- Generating Plot ---\")\n",
    "    \n",
    "    # Define a color map for the categories using Arcadia palettes\n",
    "    # Ensure arcadia_primary_manual is defined (should be from Cell 1)\n",
    "    if 'arcadia_primary_manual' not in locals():\n",
    "        print(\"Warning: Arcadia color palette not found. Using default Plotly colors.\")\n",
    "        category_color_map = None\n",
    "    else:\n",
    "         # Assign colors, maybe cycle through primary then secondary\n",
    "         plot_palette = arcadia_primary_manual + arcadia_secondary_manual\n",
    "         category_color_map = {\n",
    "             category: plot_palette[i % len(plot_palette)] \n",
    "             for i, category in enumerate(df_summary['Category'])\n",
    "         }\n",
    "\n",
    "    fig_summary = px.bar(\n",
    "        df_summary,\n",
    "        x='Category',\n",
    "        y='Percentage with Euk Hit (%)',\n",
    "        color='Category', # Color bars by category\n",
    "        color_discrete_map=category_color_map, # Apply the custom color map\n",
    "        title='Percentage of Asgard Protein Subsets with Eukaryotic DIAMOND Hits',\n",
    "        labels={'Percentage with Euk Hit (%)': '% Proteins with Eukaryotic Hit'},\n",
    "        text='Percentage with Euk Hit (%)' # Add percentage text on bars\n",
    "    )\n",
    "    \n",
    "    fig_summary.update_traces(texttemplate='%{text:.1f}%', textposition='outside') # Format text\n",
    "    fig_summary.update_layout(\n",
    "        xaxis_title=None, # Remove x-axis title if categories are clear\n",
    "        yaxis_title='% Proteins with Eukaryotic Hit',\n",
    "        yaxis_range=[0, df_summary['Percentage with Euk Hit (%)'].max() * 1.15], # Set y-axis range slightly above max\n",
    "        showlegend=False, # Hide legend as colors match x-axis labels\n",
    "        uniformtext_minsize=8, \n",
    "        uniformtext_mode='hide'\n",
    "    )\n",
    "    fig_summary.update_xaxes(categoryorder='array', categoryarray=df_summary['Category']) # Keep original order\n",
    "    fig_summary.show()\n",
    "    \n",
    "    # Save plot\n",
    "    plot_filename = os.path.join(output_plot_dir_phase1, \"asgard_subsets_euk_hit_percentage.html\")\n",
    "    try:\n",
    "        fig_summary.write_html(plot_filename)\n",
    "        print(f\"Saved summary plot to '{plot_filename}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving summary plot: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo summary data generated to display or plot.\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n--- Cell 6 (Summary Visualization) Complete ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f01a1e0-3d80-48d5-9f2b-301274b3e415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Comparison of High-Quality Eukaryotic Hits: ESPs vs. Non-ESPs\n",
    "\n",
    "# --- Standard Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "# --- Plotly Imports ---\n",
    "# Assuming plotly express and graph_objects were imported as px and go in Cell 1\n",
    "# import plotly.express as px \n",
    "# import plotly.graph_objects as go \n",
    "\n",
    "print(\"\\n\\n--- Cell 7: ESP vs. Non-ESP High-Quality Hit Comparison ---\")\n",
    "\n",
    "# --- Assumptions ---\n",
    "# This cell assumes Cell 1 and Cell 2 have been run successfully.\n",
    "# It relies on the following DataFrames being available and populated:\n",
    "#   - df_full: The main DataFrame loaded in Cell 1.\n",
    "#   - df_high_quality_hits: Asgard proteins filtered for hit quality in Cell 2.\n",
    "# It also relies on column name variables (e.g., group_col, esp_col, euk_hit_pident_col, etc.) \n",
    "# and color maps (arcadia_colors_manual, broad_category_color_map, etc.) defined in Cell 1.\n",
    "\n",
    "# --- Check if necessary DataFrames exist ---\n",
    "if 'df_high_quality_hits' not in locals() or df_high_quality_hits.empty:\n",
    "    print(\"ERROR: DataFrame 'df_high_quality_hits' not found or is empty.\")\n",
    "    print(\"       Cannot perform ESP comparison. Please ensure Cell 2 ran successfully and generated results.\")\n",
    "    # Create empty df to prevent downstream errors\n",
    "    df_high_quality_hits = pd.DataFrame() \n",
    "\n",
    "if 'df_full' not in locals() or df_full.empty:\n",
    "     print(\"ERROR: DataFrame 'df_full' not found. Needed for context if issues arise.\")\n",
    "     # Allow proceeding but some context might be missing if df_high_quality_hits is also empty\n",
    "\n",
    "# --- ESP Definition ---\n",
    "print(\"\\n--- ESP Definition Used ---\")\n",
    "print(f\"ESPs are defined based on the '{esp_col}' column in the DataFrame.\")\n",
    "print(\"This column was populated in Cell 1 of the 'integrate_diamond_hits' notebook\")\n",
    "print(f\"by checking if an Asgard protein's '{orthogroup_col}' was present in the list\")\n",
    "print(f\"loaded from 'all_esp_orthogroup_list_v4.txt'.\")\n",
    "\n",
    "# --- Subset Data into ESPs and Non-ESPs ---\n",
    "df_hq_esps = pd.DataFrame()\n",
    "df_hq_non_esps = pd.DataFrame()\n",
    "\n",
    "if not df_high_quality_hits.empty and esp_col in df_high_quality_hits.columns:\n",
    "    df_hq_esps = df_high_quality_hits[df_high_quality_hits[esp_col] == True].copy()\n",
    "    df_hq_non_esps = df_high_quality_hits[df_high_quality_hits[esp_col] == False].copy()\n",
    "    print(f\"\\nSubsetting high-quality hits:\")\n",
    "    print(f\"  Found {len(df_hq_esps)} ESPs with high-quality eukaryotic hits.\")\n",
    "    print(f\"  Found {len(df_hq_non_esps)} Non-ESPs with high-quality eukaryotic hits.\")\n",
    "elif not df_high_quality_hits.empty:\n",
    "     print(f\"ERROR: ESP column '{esp_col}' not found in df_high_quality_hits. Cannot subset.\")\n",
    "else:\n",
    "     print(\"Skipping subsetting as df_high_quality_hits is empty.\")\n",
    "\n",
    "\n",
    "# --- Perform Comparative Analyses ---\n",
    "\n",
    "# Helper function for comparative histograms/box plots\n",
    "def plot_comparison_dist(df1, df2, column, df1_name, df2_name, title, log_scale=False):\n",
    "    \"\"\"Plots overlapping histograms and box plots for a column from two dataframes.\"\"\"\n",
    "    if column not in df1.columns or column not in df2.columns:\n",
    "        print(f\"  Skipping distribution plot for '{column}': Column missing in one or both dataframes.\")\n",
    "        return\n",
    "        \n",
    "    if df1[column].isna().all() or df2[column].isna().all():\n",
    "        print(f\"  Skipping distribution plot for '{column}': All values are NaN in one or both dataframes.\")\n",
    "        return\n",
    "\n",
    "    fig = go.Figure()\n",
    "    # Histogram for df1\n",
    "    fig.add_trace(go.Histogram(x=df1[column], name=df1_name, opacity=0.75, \n",
    "                               marker_color=arcadia_colors_manual.get('amber', '#F28360')))\n",
    "    # Histogram for df2\n",
    "    fig.add_trace(go.Histogram(x=df2[column], name=df2_name, opacity=0.75,\n",
    "                               marker_color=arcadia_colors_manual.get('aegean', '#5088C5')))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        barmode='overlay',\n",
    "        title_text=f'Distribution Comparison: {title}',\n",
    "        xaxis_title_text=column.replace('_', ' ').title(),\n",
    "        yaxis_title_text='Count'\n",
    "    )\n",
    "    if log_scale:\n",
    "        # Handle potential non-positive values for log scale if needed\n",
    "        # This simple version assumes positive values or uses Plotly's default handling\n",
    "        fig.update_xaxes(type=\"log\") \n",
    "        fig.update_layout(xaxis_title_text=f\"Log({column.replace('_', ' ').title()})\")\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # Box plots\n",
    "    df_combined = pd.concat([\n",
    "        df1[[column]].assign(Group=df1_name),\n",
    "        df2[[column]].assign(Group=df2_name)\n",
    "    ], ignore_index=True)\n",
    "\n",
    "    fig_box = px.box(\n",
    "        df_combined.dropna(subset=[column]), \n",
    "        x='Group', \n",
    "        y=column, \n",
    "        color='Group',\n",
    "        title=f'Box Plot Comparison: {title}',\n",
    "        labels={column: column.replace('_', ' ').title()},\n",
    "        color_discrete_map={df1_name: arcadia_colors_manual.get('amber', '#F28360'), \n",
    "                            df2_name: arcadia_colors_manual.get('aegean', '#5088C5')}\n",
    "    )\n",
    "    if log_scale:\n",
    "         fig_box.update_yaxes(type=\"log\")\n",
    "         fig_box.update_layout(yaxis_title_text=f\"Log({column.replace('_', ' ').title()})\")\n",
    "         \n",
    "    fig_box.show()\n",
    "\n",
    "\n",
    "if not df_hq_esps.empty and not df_hq_non_esps.empty:\n",
    "    print(\"\\n--- Comparing ESP vs Non-ESP High-Quality Hits ---\")\n",
    "\n",
    "    # 1. Hit Quality Comparison\n",
    "    print(\"\\n--- 1. Hit Quality (PIDENT, E-value) ---\")\n",
    "    plot_comparison_dist(df_hq_esps, df_hq_non_esps, euk_hit_pident_col, 'ESPs', 'Non-ESPs', 'Euk Hit Percent Identity')\n",
    "    # For E-value, use the log10 version if calculated in Cell 1, otherwise calculate here\n",
    "    if 'log10_evalue' not in df_hq_esps.columns and euk_hit_evalue_col in df_hq_esps.columns:\n",
    "         df_hq_esps['log10_evalue'] = np.log10(df_hq_esps[euk_hit_evalue_col] + 1e-200)\n",
    "    if 'log10_evalue' not in df_hq_non_esps.columns and euk_hit_evalue_col in df_hq_non_esps.columns:\n",
    "         df_hq_non_esps['log10_evalue'] = np.log10(df_hq_non_esps[euk_hit_evalue_col] + 1e-200)\n",
    "         \n",
    "    if 'log10_evalue' in df_hq_esps.columns and 'log10_evalue' in df_hq_non_esps.columns:\n",
    "        plot_comparison_dist(df_hq_esps, df_hq_non_esps, 'log10_evalue', 'ESPs', 'Non-ESPs', 'Log10(Euk Hit E-value)')\n",
    "    else:\n",
    "        print(f\"  Skipping E-value comparison: Column '{euk_hit_evalue_col}' or 'log10_evalue' missing.\")\n",
    "\n",
    "    # 2. Alignment Coverage Comparison\n",
    "    print(\"\\n--- 2. Alignment Coverage (Query, Subject) ---\")\n",
    "    plot_comparison_dist(df_hq_esps, df_hq_non_esps, query_coverage_col, 'ESPs', 'Non-ESPs', 'Query (Asgard) Coverage')\n",
    "    plot_comparison_dist(df_hq_esps, df_hq_non_esps, subject_coverage_col, 'ESPs', 'Non-ESPs', 'Subject (Eukaryote) Coverage')\n",
    "\n",
    "    # 3. Taxonomic Distribution Comparison\n",
    "    print(\"\\n--- 3. Taxonomic Distribution of Hits ---\")\n",
    "    top_n_orgs_comp = 20 # Show fewer for comparison clarity\n",
    "    \n",
    "    org_counts_esps = df_hq_esps[euk_hit_organism_col].value_counts().head(top_n_orgs_comp)\n",
    "    org_counts_non_esps = df_hq_non_esps[euk_hit_organism_col].value_counts().head(top_n_orgs_comp)\n",
    "\n",
    "    df_org_comp = pd.DataFrame({\n",
    "        'ESP Hits': org_counts_esps,\n",
    "        'Non-ESP Hits': org_counts_non_esps\n",
    "    }).fillna(0).astype(int).reset_index()\n",
    "    df_org_comp.rename(columns={'index': euk_hit_organism_col}, inplace=True)\n",
    "    \n",
    "    df_org_melted = df_org_comp.melt(id_vars=euk_hit_organism_col, var_name='Group', value_name='Count')\n",
    "\n",
    "    print(f\"\\nTop {top_n_orgs_comp} Eukaryotic Hit Organisms (ESP vs Non-ESP):\")\n",
    "    # Display combined counts for top overall organisms\n",
    "    combined_top_orgs = df_org_comp.sort_values(['ESP Hits', 'Non-ESP Hits'], ascending=False).head(top_n_orgs_comp)\n",
    "    try:\n",
    "        print(combined_top_orgs.to_markdown(index=False))\n",
    "    except ImportError:\n",
    "        print(combined_top_orgs)\n",
    "\n",
    "    fig_org_comp = px.bar(\n",
    "        df_org_melted,\n",
    "        x=euk_hit_organism_col,\n",
    "        y='Count',\n",
    "        color='Group',\n",
    "        barmode='group',\n",
    "        title=f'Top {top_n_orgs_comp} Eukaryotic Hit Organisms: ESPs vs. Non-ESPs',\n",
    "        labels={'Count': 'Number of Asgard Proteins Hit', euk_hit_organism_col: 'Eukaryotic Organism'},\n",
    "        category_orders={euk_hit_organism_col: combined_top_orgs[euk_hit_organism_col].tolist()}, # Order by combined rank\n",
    "        color_discrete_map={'ESP Hits': arcadia_colors_manual.get('amber', '#F28360'), \n",
    "                            'Non-ESP Hits': arcadia_colors_manual.get('aegean', '#5088C5')}\n",
    "    )\n",
    "    fig_org_comp.update_xaxes(tickangle=45)\n",
    "    fig_org_comp.show()\n",
    "\n",
    "    # 4. Functional Category Comparison (within ESPs and within Non-ESPs)\n",
    "    print(\"\\n--- 4. Broad Functional Category Distribution ---\")\n",
    "    if broad_func_cat_col in df_hq_esps.columns and broad_func_cat_col in df_hq_non_esps.columns:\n",
    "        esp_cat_counts = df_hq_esps[broad_func_cat_col].value_counts(normalize=True).mul(100)\n",
    "        non_esp_cat_counts = df_hq_non_esps[broad_func_cat_col].value_counts(normalize=True).mul(100)\n",
    "\n",
    "        df_cat_comp = pd.DataFrame({'ESPs (%)': esp_cat_counts, 'Non-ESPs (%)': non_esp_cat_counts}).fillna(0)\n",
    "        # Order by ESP percentage for focus\n",
    "        df_cat_comp = df_cat_comp.sort_values('ESPs (%)', ascending=False)\n",
    "        \n",
    "        print(\"\\nBroad Functional Category Distribution within Hit Groups:\")\n",
    "        try:\n",
    "             print(df_cat_comp.round(1).to_markdown())\n",
    "        except ImportError:\n",
    "             print(df_cat_comp.round(1))\n",
    "             \n",
    "        # Plotting - maybe side-by-side? Or separate plots? Let's do separate for clarity.\n",
    "        fig_cat_esp = px.bar(\n",
    "            esp_cat_counts.reset_index(), x=broad_func_cat_col, y='proportion',\n",
    "            title='Functional Categories of ESPs with High-Quality Euk Hits',\n",
    "            labels={'proportion': '% of ESP Hits', broad_func_cat_col: 'Category'},\n",
    "            color=broad_func_cat_col, color_discrete_map=broad_category_color_map\n",
    "        )\n",
    "        fig_cat_esp.update_layout(showlegend=False)\n",
    "        fig_cat_esp.update_xaxes(categoryorder='total descending')\n",
    "        fig_cat_esp.show()\n",
    "\n",
    "        fig_cat_non_esp = px.bar(\n",
    "            non_esp_cat_counts.reset_index(), x=broad_func_cat_col, y='proportion',\n",
    "            title='Functional Categories of Non-ESPs with High-Quality Euk Hits',\n",
    "            labels={'proportion': '% of Non-ESP Hits', broad_func_cat_col: 'Category'},\n",
    "            color=broad_func_cat_col, color_discrete_map=broad_category_color_map\n",
    "        )\n",
    "        fig_cat_non_esp.update_layout(showlegend=False)\n",
    "        fig_cat_non_esp.update_xaxes(categoryorder='total descending')\n",
    "        fig_cat_non_esp.show()\n",
    "        \n",
    "    else:\n",
    "        print(f\"Skipping Functional Category comparison: Column '{broad_func_cat_col}' missing.\")\n",
    "\n",
    "    # 5. Structural Darkness Comparison\n",
    "    print(\"\\n--- 5. Structural Darkness Comparison ---\")\n",
    "    if structurally_dark_col in df_hq_esps.columns and structurally_dark_col in df_hq_non_esps.columns:\n",
    "        plot_comparison_bar(df_hq_esps, df_hq_non_esps, structurally_dark_col, 'ESP vs Non-ESP Hits (True=Dark)')\n",
    "    else:\n",
    "        print(f\"Skipping Structural Darkness comparison: Column '{structurally_dark_col}' missing.\")\n",
    "        \n",
    "    # 6. Domain-less Comparison\n",
    "    print(\"\\n--- 6. Domain-less Comparison ---\")\n",
    "    if num_domains_col in df_hq_esps.columns and num_domains_col in df_hq_non_esps.columns:\n",
    "        domainless_col_temp = 'Is_DomainLess'\n",
    "        df_hq_esps[domainless_col_temp] = df_hq_esps[num_domains_col].isna()\n",
    "        df_hq_non_esps[domainless_col_temp] = df_hq_non_esps[num_domains_col].isna()\n",
    "        plot_comparison_bar(df_hq_esps, df_hq_non_esps, domainless_col_temp, 'ESP vs Non-ESP Hits (True=Domain-less)')\n",
    "    else:\n",
    "        print(f\"Skipping Domain-less comparison: Column '{num_domains_col}' missing.\")\n",
    "\n",
    "elif df_high_quality_hits.empty:\n",
    "    print(\"Skipping ESP vs Non-ESP comparison as 'df_high_quality_hits' is empty.\")\n",
    "else:\n",
    "    print(\"Skipping ESP vs Non-ESP comparison due to missing ESP column or empty subsets.\")\n",
    "\n",
    "print(\"\\n\\n--- Cell 7 (ESP vs Non-ESP Comparison) Complete ---\")\n",
    "print(\"This cell compared key characteristics between Asgard ESPs and Non-ESPs\")\n",
    "print(\"that have high-quality eukaryotic hits.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0069fd5-9cec-4013-89d2-7b0c40cbe114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Profiling Domains and Functions of Non-ESPs with High-Quality Eukaryotic Hits\n",
    "\n",
    "# --- Standard Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from collections import Counter\n",
    "import re # For parsing IPRs\n",
    "\n",
    "# --- Plotly Imports ---\n",
    "# Assuming plotly express and graph_objects were imported as px and go in Cell 1\n",
    "# import plotly.express as px \n",
    "# import plotly.graph_objects as go \n",
    "\n",
    "print(\"\\n\\n--- Cell 8: Profiling Non-ESPs with High-Quality Eukaryotic Hits ---\")\n",
    "\n",
    "# --- Assumptions ---\n",
    "# This cell assumes Cell 1, 2, and subsequent cells defining df_high_quality_hits have been run.\n",
    "# It relies on the DataFrame 'df_high_quality_hits'.\n",
    "# It also relies on column names (esp_col, domain_arch_col, ipr_col, broad_func_cat_col) \n",
    "# and helper functions/variables (ipr_lookup, translate_architecture, output_summary_dir_phase1, etc.) defined in Cell 1.\n",
    "\n",
    "# --- Check if necessary DataFrames/Variables exist ---\n",
    "if 'df_high_quality_hits' not in locals() or df_high_quality_hits.empty:\n",
    "    print(\"ERROR: DataFrame 'df_high_quality_hits' not found or is empty.\")\n",
    "    print(\"       Cannot perform Non-ESP analysis. Please ensure Cell 2 ran successfully.\")\n",
    "    df_hq_non_esps = pd.DataFrame() # Create empty df to prevent errors\n",
    "else:\n",
    "    # Subset for Non-ESPs with high-quality hits\n",
    "    if esp_col in df_high_quality_hits.columns:\n",
    "        df_hq_non_esps = df_high_quality_hits[df_high_quality_hits[esp_col] == False].copy()\n",
    "        print(f\"Analyzing {len(df_hq_non_esps)} Non-ESP Asgard proteins with high-quality eukaryotic hits.\")\n",
    "        if df_hq_non_esps.empty:\n",
    "            print(\"No Non-ESP proteins found within the high-quality hits subset.\")\n",
    "    else:\n",
    "        print(f\"ERROR: ESP column '{esp_col}' not found in df_high_quality_hits. Cannot isolate Non-ESPs.\")\n",
    "        df_hq_non_esps = pd.DataFrame()\n",
    "\n",
    "# --- Perform Profiling on Non-ESP Subset ---\n",
    "\n",
    "if not df_hq_non_esps.empty:\n",
    "\n",
    "    # 1. Analyze Domain Architectures for Non-ESPs with Hits\n",
    "    print(f\"\\n--- 1. Domain Architectures (Top 20) for Non-ESPs with High-Quality Euk Hits ---\")\n",
    "    if domain_arch_col in df_hq_non_esps.columns:\n",
    "        # Fill NA with a placeholder for value_counts if needed\n",
    "        non_esp_arch_counts = df_hq_non_esps[domain_arch_col].fillna('No_Domain').value_counts()\n",
    "        \n",
    "        df_non_esp_arch_plot = non_esp_arch_counts.head(20).reset_index()\n",
    "        df_non_esp_arch_plot.columns = ['Architecture', 'Count']\n",
    "        \n",
    "        # Translate architectures for readability in the plot hover/table\n",
    "        if 'translate_architecture' in locals() and 'ipr_lookup' in locals():\n",
    "             df_non_esp_arch_plot['Translated Architecture'] = df_non_esp_arch_plot['Architecture'].apply(translate_architecture)\n",
    "             hover_col = 'Translated Architecture'\n",
    "        else:\n",
    "             print(\"Warning: 'translate_architecture' function or 'ipr_lookup' not found. Using raw architectures.\")\n",
    "             hover_col = 'Architecture'\n",
    "        \n",
    "        print(\"\\nTop 20 Domain Architectures:\")\n",
    "        try:\n",
    "             print(df_non_esp_arch_plot.to_markdown(index=False))\n",
    "        except ImportError:\n",
    "             print(df_non_esp_arch_plot)\n",
    "             \n",
    "        # Save summary\n",
    "        arch_filename = os.path.join(output_summary_dir_phase1, \"non_esp_hq_hits_top_architectures.csv\")\n",
    "        try:\n",
    "             non_esp_arch_counts.reset_index().rename(columns={'index':'Architecture', domain_arch_col:'Count'}).to_csv(arch_filename, index=False)\n",
    "             print(f\"Saved architecture counts to '{arch_filename}'\")\n",
    "        except Exception as e:\n",
    "             print(f\"Error saving architecture counts: {e}\")\n",
    "\n",
    "        # Plot\n",
    "        fig_arch = px.bar(\n",
    "            df_non_esp_arch_plot,\n",
    "            x='Architecture',\n",
    "            y='Count',\n",
    "            title='Top 20 Domain Architectures for Non-ESP Asgard Proteins with High-Quality Euk Hits',\n",
    "            hover_data=[hover_col],\n",
    "            labels={'Architecture': 'Domain Architecture (Raw)', 'Count': 'Number of Proteins'}\n",
    "        )\n",
    "        fig_arch.update_xaxes(tickangle=45, categoryorder='total descending', \n",
    "                              # Truncate long x-axis labels if needed\n",
    "                              ticktext = [truncate_string(s, 50) for s in df_non_esp_arch_plot['Architecture']],\n",
    "                              tickvals = df_non_esp_arch_plot['Architecture'])\n",
    "        fig_arch.show()\n",
    "    else:\n",
    "        print(f\"Skipping Domain Architecture analysis: Column '{domain_arch_col}' not found.\")\n",
    "        \n",
    "    # 2. Analyze Individual IPR Signatures for Non-ESPs with Hits\n",
    "    print(f\"\\n--- 2. Individual IPR Signatures (Top 30) for Non-ESPs with High-Quality Euk Hits ---\")\n",
    "    if ipr_col in df_hq_non_esps.columns and 'ipr_lookup' in locals():\n",
    "        all_iprs_non_esp_list = []\n",
    "        for ipr_string_raw in df_hq_non_esps[ipr_col].dropna():\n",
    "            processed_ipr_string = str(ipr_string_raw).replace('|', ';')\n",
    "            individual_iprs = processed_ipr_string.split(';')\n",
    "            for ipr_item in individual_iprs:\n",
    "                stripped_ipr = ipr_item.strip()\n",
    "                if stripped_ipr: \n",
    "                    all_iprs_non_esp_list.append(stripped_ipr)\n",
    "        \n",
    "        ipr_counts_non_esp = Counter(all_iprs_non_esp_list)\n",
    "        \n",
    "        if ipr_counts_non_esp:\n",
    "            top_n_iprs = 30\n",
    "            df_ipr_counts = pd.DataFrame(ipr_counts_non_esp.most_common(top_n_iprs), columns=['IPR_ID', 'Count'])\n",
    "            \n",
    "            # Add descriptions\n",
    "            df_ipr_counts['Description'] = df_ipr_counts['IPR_ID'].apply(lambda x: ipr_lookup.get(x, {}).get('Name', 'N/A'))\n",
    "            df_ipr_counts['Type'] = df_ipr_counts['IPR_ID'].apply(lambda x: ipr_lookup.get(x, {}).get('Type', 'N/A'))\n",
    "            \n",
    "            # Create a combined label for plotting\n",
    "            df_ipr_counts['Label'] = df_ipr_counts['Description'].apply(lambda x: truncate_string(x, 60)) + \" (\" + df_ipr_counts['IPR_ID'] + \")\"\n",
    "\n",
    "            print(f\"\\nTop {top_n_iprs} IPR Signatures:\")\n",
    "            try:\n",
    "                 print(df_ipr_counts[['IPR_ID', 'Count', 'Description']].to_markdown(index=False))\n",
    "            except ImportError:\n",
    "                 print(df_ipr_counts[['IPR_ID', 'Count', 'Description']])\n",
    "\n",
    "            # Save summary\n",
    "            ipr_filename = os.path.join(output_summary_dir_phase1, \"non_esp_hq_hits_top_iprs.csv\")\n",
    "            try:\n",
    "                 # Save full list sorted by count\n",
    "                 full_ipr_df = pd.DataFrame(ipr_counts_non_esp.items(), columns=['IPR_ID', 'Count']).sort_values('Count', ascending=False)\n",
    "                 full_ipr_df['Description'] = full_ipr_df['IPR_ID'].apply(lambda x: ipr_lookup.get(x, {}).get('Name', 'N/A'))\n",
    "                 full_ipr_df['Type'] = full_ipr_df['IPR_ID'].apply(lambda x: ipr_lookup.get(x, {}).get('Type', 'N/A'))\n",
    "                 full_ipr_df[['IPR_ID', 'Count', 'Description', 'Type']].to_csv(ipr_filename, index=False)\n",
    "                 print(f\"Saved IPR counts to '{ipr_filename}'\")\n",
    "            except Exception as e:\n",
    "                 print(f\"Error saving IPR counts: {e}\")\n",
    "\n",
    "            # Plot\n",
    "            fig_ipr = px.bar(\n",
    "                df_ipr_counts,\n",
    "                x='Label',\n",
    "                y='Count',\n",
    "                title=f'Top {top_n_iprs} IPR Signatures for Non-ESP Asgard Proteins with High-Quality Euk Hits',\n",
    "                hover_data=['IPR_ID', 'Description', 'Type'],\n",
    "                labels={'Label': 'IPR Signature', 'Count': 'Number of Proteins'}\n",
    "            )\n",
    "            fig_ipr.update_xaxes(tickangle=45, categoryorder='total descending')\n",
    "            fig_ipr.show()\n",
    "            \n",
    "        else:\n",
    "            print(\"No IPR signatures found to analyze in this subset.\")\n",
    "            \n",
    "    elif ipr_col not in df_hq_non_esps.columns:\n",
    "        print(f\"Skipping IPR analysis: Column '{ipr_col}' not found.\")\n",
    "    else: # ipr_lookup missing\n",
    "        print(f\"Skipping IPR analysis: 'ipr_lookup' dictionary not found (load from Cell 1).\")\n",
    "\n",
    "    # 3. Analyze Broad Functional Categories for Non-ESPs with Hits\n",
    "    print(f\"\\n--- 3. Broad Functional Categories for Non-ESPs with High-Quality Euk Hits ---\")\n",
    "    if broad_func_cat_col in df_hq_non_esps.columns:\n",
    "        non_esp_cat_counts = df_hq_non_esps[broad_func_cat_col].value_counts()\n",
    "        df_non_esp_cat_plot = non_esp_cat_counts.reset_index()\n",
    "        df_non_esp_cat_plot.columns = ['Category', 'Count']\n",
    "\n",
    "        print(\"\\nFunctional Category Distribution:\")\n",
    "        try:\n",
    "            print(non_esp_cat_counts.to_markdown())\n",
    "        except ImportError:\n",
    "            print(non_esp_cat_counts)\n",
    "            \n",
    "        # Save summary\n",
    "        cat_filename = os.path.join(output_summary_dir_phase1, \"non_esp_hq_hits_category_counts.csv\")\n",
    "        try:\n",
    "             non_esp_cat_counts.reset_index().rename(columns={'index':'Category', broad_func_cat_col:'Count'}).to_csv(cat_filename, index=False)\n",
    "             print(f\"Saved category counts to '{cat_filename}'\")\n",
    "        except Exception as e:\n",
    "             print(f\"Error saving category counts: {e}\")\n",
    "\n",
    "        # Plot\n",
    "        fig_cat_non_esp_only = px.bar(\n",
    "            df_non_esp_cat_plot,\n",
    "            x='Category',\n",
    "            y='Count',\n",
    "            title='Functional Categories of Non-ESPs with High-Quality Euk Hits',\n",
    "            labels={'Count': 'Number of Proteins', 'Category': 'Broad Functional Category'},\n",
    "            color='Category',\n",
    "            color_discrete_map=broad_category_color_map # Use map from Cell 1\n",
    "        )\n",
    "        fig_cat_non_esp_only.update_layout(showlegend=False)\n",
    "        fig_cat_non_esp_only.update_xaxes(categoryorder='total descending')\n",
    "        fig_cat_non_esp_only.show()\n",
    "    else:\n",
    "         print(f\"Skipping Functional Category analysis: Column '{broad_func_cat_col}' not found.\")\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"Skipping detailed Non-ESP profiling as the 'df_hq_non_esps' DataFrame is empty.\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n--- Cell 8 (Non-ESP Hit Profiling) Complete ---\")\n",
    "print(\"This cell analyzed the domain architectures, IPR signatures, and functional categories\")\n",
    "print(\"for the subset of Non-ESP Asgard proteins that have high-quality eukaryotic hits.\")\n",
    "print(\"Compare these results (e.g., top IPRs/Architectures) to typical ESP functions\")\n",
    "print(\"to investigate the hypothesis that some might be functionally related.\")\n",
    "print(\"The next crucial step is obtaining annotations for the eukaryotic hits themselves.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baacb50b-4efa-462b-b1e9-1a9a27e50770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Domain Architecture Comparison (ESP vs. Non-ESP High-Quality Hits)\n",
    "\n",
    "# --- Standard Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from collections import Counter\n",
    "import re # For parsing IPRs\n",
    "\n",
    "# --- Plotly Imports ---\n",
    "# Assuming plotly express and graph_objects were imported as px and go in Cell 1\n",
    "# import plotly.express as px \n",
    "# import plotly.graph_objects as go \n",
    "\n",
    "print(\"\\n\\n--- Cell 10: Domain Architecture Comparison (ESP vs Non-ESP High-Quality Hits) ---\")\n",
    "\n",
    "# --- Assumptions ---\n",
    "# This cell assumes Cell 1, 2, and 7 have been run successfully.\n",
    "# It relies on the following DataFrames being available and populated:\n",
    "#   - df_hq_esps: ESPs with high-quality hits (from Cell 7).\n",
    "#   - df_hq_non_esps: Non-ESPs with high-quality hits (from Cell 7).\n",
    "# It also relies on column names (domain_arch_col), helper functions (translate_architecture),\n",
    "# variables (ipr_lookup, arcadia_colors_manual), and output directories (output_summary_dir_phase1) defined in Cell 1.\n",
    "\n",
    "# --- Configuration ---\n",
    "top_n_arch_display = 30 # How many top architectures to show in tables/plots\n",
    "# Placeholder list of IPR IDs often associated with ESP functions \n",
    "# (Customize this list based on your project's definition and literature)\n",
    "esp_like_ipr_keywords = [\n",
    "    \"IPR001680\", # WD40 repeat\n",
    "    \"IPR002110\", # TPR repeat\n",
    "    \"IPR002110\", # Ankyrin repeat # Note: Duplicate IPR002110, likely meant IPR002110 (TPR) and IPR002068 (Ankyrin) or similar - check list\n",
    "    \"IPR001806\", # Ras family\n",
    "    \"IPR000159\", # RhoGAP domain\n",
    "    \"IPR001895\", # RhoGEF domain\n",
    "    \"IPR000719\", # Protein kinase domain\n",
    "    \"IPR001245\", # Protein kinase, catalytic domain\n",
    "    \"IPR011009\", # Protein kinase-like domain\n",
    "    \"IPR000387\", # Protein tyrosine kinase\n",
    "    \"IPR000980\", # Protein phosphatase 2C\n",
    "    \"IPR020633\", # Dual specificity phosphatase, catalytic domain\n",
    "    \"IPR001628\", # Protein tyrosine phosphatase\n",
    "    \"IPR000626\", # Ubiquitin-conjugating enzyme E2\n",
    "    \"IPR000569\", # Ubiquitin carboxyl-terminal hydrolase\n",
    "    \"IPR001342\", # Ubiquitin domain\n",
    "    \"IPR019744\", # Ubiquitin-like domain superfamily\n",
    "    \"IPR011080\", # ESCRT-I complex Vps28 C-terminal domain\n",
    "    \"IPR017946\", # ESCRT-II Vps25\n",
    "    \"IPR017944\", # ESCRT-III/Vps20/Vps32\n",
    "    \"IPR001478\", # Clathrin, heavy chain\n",
    "    \"IPR016024\", # Clathrin adaptor complex, medium subunit family\n",
    "    \"IPR008948\", # SNARE domain\n",
    "    \"IPR001007\", # Dynein heavy chain region\n",
    "    \"IPR001613\", # Kinesin motor domain\n",
    "    \"IPR004000\", # Actin\n",
    "    \"IPR001603\", # Tubulin\n",
    "    # Add more relevant IPR IDs based on your ESP definitions...\n",
    "]\n",
    "\n",
    "# --- Check if necessary DataFrames/Variables exist ---\n",
    "if 'df_hq_esps' not in locals() or 'df_hq_non_esps' not in locals():\n",
    "    print(\"ERROR: DataFrames 'df_hq_esps' and/or 'df_hq_non_esps' not found.\")\n",
    "    print(\"       Cannot perform comparison. Please ensure Cell 7 ran successfully.\")\n",
    "    if 'df_hq_esps' not in locals(): df_hq_esps = pd.DataFrame()\n",
    "    if 'df_hq_non_esps' not in locals(): df_hq_non_esps = pd.DataFrame()\n",
    "    \n",
    "if 'domain_arch_col' not in locals():\n",
    "     print(\"ERROR: Variable 'domain_arch_col' not defined. Please ensure Cell 1 ran.\")\n",
    "     domain_arch_col = 'Domain_Architecture' # Assign dummy - analysis will likely fail later\n",
    "\n",
    "if 'translate_architecture' not in locals() or 'ipr_lookup' not in locals():\n",
    "     print(\"WARNING: Helper function 'translate_architecture' or 'ipr_lookup' not found.\")\n",
    "     print(\"         Plots and tables will show raw IPR IDs only.\")\n",
    "     if 'translate_architecture' not in locals():\n",
    "         def translate_architecture(arch_string, lookup_dict=None): return arch_string\n",
    "     if 'ipr_lookup' not in locals():\n",
    "         ipr_lookup = {}\n",
    "         \n",
    "if 'arcadia_colors_manual' not in locals():\n",
    "     print(\"WARNING: Arcadia color palette 'arcadia_colors_manual' not found. Using default colors.\")\n",
    "     arcadia_colors_manual = {} # Define empty to avoid errors\n",
    "\n",
    "# --- Analyze and Compare Domain Architectures ---\n",
    "if (not df_hq_esps.empty or not df_hq_non_esps.empty):\n",
    "    # Ensure domain_arch_col exists before proceeding\n",
    "    if domain_arch_col not in df_hq_esps.columns or domain_arch_col not in df_hq_non_esps.columns:\n",
    "         print(f\"ERROR: Column '{domain_arch_col}' missing from ESP or Non-ESP DataFrame. Cannot proceed with architecture analysis.\")\n",
    "    else:\n",
    "        # 1. Calculate Architecture Counts for each group\n",
    "        esp_arch_counts = df_hq_esps[domain_arch_col].fillna('No_Domain').value_counts()\n",
    "        non_esp_arch_counts = df_hq_non_esps[domain_arch_col].fillna('No_Domain').value_counts()\n",
    "\n",
    "        # 2. Combine Counts into a Comparison DataFrame\n",
    "        df_arch_compare = pd.DataFrame({\n",
    "            'ESP_Count': esp_arch_counts,\n",
    "            'Non_ESP_Count': non_esp_arch_counts\n",
    "        }).fillna(0).astype(int)\n",
    "        \n",
    "        # Add a total count column for sorting\n",
    "        df_arch_compare['Total_Count'] = df_arch_compare['ESP_Count'] + df_arch_compare['Non_ESP_Count']\n",
    "        df_arch_compare = df_arch_compare.sort_values('Total_Count', ascending=False)\n",
    "        df_arch_compare.index.name = 'Architecture' # Name the index\n",
    "        \n",
    "        # Add translated architecture\n",
    "        df_arch_compare['Translated Architecture'] = df_arch_compare.index.map(lambda x: translate_architecture(x, lookup_dict=ipr_lookup))\n",
    "\n",
    "        print(f\"\\n--- Top {top_n_arch_display} Domain Architectures Overall (ESPs + Non-ESPs with High-Quality Euk Hits) ---\")\n",
    "        try:\n",
    "             # Select columns for display\n",
    "             display_cols_overall = ['ESP_Count', 'Non_ESP_Count', 'Total_Count', 'Translated Architecture']\n",
    "             print(df_arch_compare[display_cols_overall].head(top_n_arch_display).to_markdown())\n",
    "        except ImportError:\n",
    "             print(df_arch_compare[display_cols_overall].head(top_n_arch_display))\n",
    "             \n",
    "        # Save combined summary\n",
    "        combined_arch_filename = os.path.join(output_summary_dir_phase1, \"esp_vs_non_esp_hq_hits_architectures.csv\")\n",
    "        try:\n",
    "             df_arch_compare.reset_index().to_csv(combined_arch_filename, index=False)\n",
    "             print(f\"\\nSaved combined architecture counts to '{combined_arch_filename}'\")\n",
    "        except Exception as e:\n",
    "             print(f\"Error saving combined architecture counts: {e}\")\n",
    "\n",
    "        # 3. Identify Architectures with ESP-like Domains in Non-ESPs\n",
    "        print(f\"\\n--- Identifying Non-ESP Architectures with Potential ESP-like Domains ---\")\n",
    "        \n",
    "        non_esp_arch_with_esp_like_domains = []\n",
    "        \n",
    "        # Iterate through architectures found in Non-ESPs\n",
    "        for architecture, counts in df_arch_compare[df_arch_compare['Non_ESP_Count'] > 0].iterrows():\n",
    "            if architecture == 'No_Domain': continue # Skip the 'No_Domain' placeholder\n",
    "            \n",
    "            # Get IPR IDs in the current architecture\n",
    "            current_iprs = set(str(architecture).replace('|', ';').split(';')) - {''}\n",
    "            \n",
    "            # Check for overlap with ESP-like keywords\n",
    "            found_esp_like = False\n",
    "            for esp_ipr in esp_like_ipr_keywords:\n",
    "                if esp_ipr in current_iprs:\n",
    "                    found_esp_like = True\n",
    "                    break # Found one, no need to check further for this architecture\n",
    "                    \n",
    "            if found_esp_like:\n",
    "                non_esp_arch_with_esp_like_domains.append({\n",
    "                    'Architecture': architecture,\n",
    "                    'ESP_Count': counts['ESP_Count'],\n",
    "                    'Non_ESP_Count': counts['Non_ESP_Count'],\n",
    "                    'Total_Count': counts['Total_Count'],\n",
    "                    'Translated Architecture': counts['Translated Architecture']\n",
    "                })\n",
    "\n",
    "        if non_esp_arch_with_esp_like_domains:\n",
    "            df_non_esp_esp_like = pd.DataFrame(non_esp_arch_with_esp_like_domains).sort_values('Non_ESP_Count', ascending=False)\n",
    "            print(f\"Found {len(df_non_esp_esp_like)} architectures in Non-ESPs (with Euk hits) containing potential ESP-like domains:\")\n",
    "            try:\n",
    "                print(df_non_esp_esp_like.to_markdown(index=False))\n",
    "            except ImportError:\n",
    "                print(df_non_esp_esp_like)\n",
    "                \n",
    "            # Save this list\n",
    "            esp_like_filename = os.path.join(output_summary_dir_phase1, \"non_esp_hq_hits_with_esp_like_domains.csv\")\n",
    "            try:\n",
    "                 df_non_esp_esp_like.to_csv(esp_like_filename, index=False)\n",
    "                 print(f\"\\nSaved list of Non-ESP architectures with ESP-like domains to '{esp_like_filename}'\")\n",
    "            except Exception as e:\n",
    "                 print(f\"Error saving ESP-like domain list: {e}\")\n",
    "                 \n",
    "        else:\n",
    "            print(\"No architectures containing the specified ESP-like IPRs were found among the Non-ESPs with high-quality hits.\")\n",
    "            \n",
    "        # 4. Visualize Comparison (Focus on architectures present in both)\n",
    "        print(f\"\\n--- Visualizing Top Architectures Present in Both ESPs and Non-ESPs ---\")\n",
    "        \n",
    "        # Filter for architectures present in both groups\n",
    "        df_arch_common = df_arch_compare[(df_arch_compare['ESP_Count'] > 0) & (df_arch_compare['Non_ESP_Count'] > 0)].copy()\n",
    "        df_arch_common = df_arch_common.sort_values('Total_Count', ascending=False) # Keep all common ones for now\n",
    "        \n",
    "        if not df_arch_common.empty:\n",
    "            # Melt for plotting\n",
    "            df_common_melt = df_arch_common[['ESP_Count', 'Non_ESP_Count']].reset_index().melt(\n",
    "                id_vars='Architecture', \n",
    "                var_name='Group', \n",
    "                value_name='Count'\n",
    "            )\n",
    "            # Add translated architecture for hover\n",
    "            df_common_melt = df_common_melt.merge(\n",
    "                df_arch_compare[['Translated Architecture']].reset_index(), \n",
    "                on='Architecture', \n",
    "                how='left'\n",
    "            )\n",
    "\n",
    "            # --- Plot 1: Including \"No_Domain\" ---\n",
    "            df_plot_data_with_nodomain = df_common_melt[df_common_melt['Architecture'].isin(df_arch_common.head(top_n_arch_display).index)]\n",
    "            \n",
    "            fig_arch_common_with_nodomain = px.bar(\n",
    "                df_plot_data_with_nodomain,\n",
    "                x='Architecture',\n",
    "                y='Count',\n",
    "                color='Group',\n",
    "                barmode='group',\n",
    "                title=f'Top {top_n_arch_display} Common Architectures (Including No_Domain)',\n",
    "                hover_data=['Translated Architecture'],\n",
    "                labels={'Architecture': 'Domain Architecture (Raw)', 'Count': 'Number of Proteins'},\n",
    "                color_discrete_map={'ESP_Count': arcadia_colors_manual.get('amber', '#F28360'), \n",
    "                                    'Non_ESP_Count': arcadia_colors_manual.get('aegean', '#5088C5')}\n",
    "            )\n",
    "            fig_arch_common_with_nodomain.update_xaxes(\n",
    "                tickangle=45, \n",
    "                categoryorder='array', # Use the order from the sorted df_arch_common\n",
    "                categoryarray=df_arch_common.head(top_n_arch_display).index.tolist(), \n",
    "                ticktext = [truncate_string(s, 50) for s in df_arch_common.head(top_n_arch_display).index], \n",
    "                tickvals = df_arch_common.head(top_n_arch_display).index\n",
    "            )\n",
    "            fig_arch_common_with_nodomain.show()\n",
    "\n",
    "            # --- Plot 2: Excluding \"No_Domain\" ---\n",
    "            df_arch_common_no_nodomain = df_arch_common[df_arch_common.index != 'No_Domain'].head(top_n_arch_display)\n",
    "            \n",
    "            if not df_arch_common_no_nodomain.empty:\n",
    "                df_plot_data_no_nodomain = df_common_melt[df_common_melt['Architecture'].isin(df_arch_common_no_nodomain.index)]\n",
    "\n",
    "                fig_arch_common_no_nodomain = px.bar(\n",
    "                    df_plot_data_no_nodomain,\n",
    "                    x='Architecture',\n",
    "                    y='Count',\n",
    "                    color='Group',\n",
    "                    barmode='group',\n",
    "                    title=f'Top {top_n_arch_display} Common Architectures (Excluding No_Domain)',\n",
    "                    hover_data=['Translated Architecture'],\n",
    "                    labels={'Architecture': 'Domain Architecture (Raw)', 'Count': 'Number of Proteins'},\n",
    "                    color_discrete_map={'ESP_Count': arcadia_colors_manual.get('amber', '#F28360'), \n",
    "                                        'Non_ESP_Count': arcadia_colors_manual.get('aegean', '#5088C5')}\n",
    "                )\n",
    "                fig_arch_common_no_nodomain.update_xaxes(\n",
    "                    tickangle=45, \n",
    "                    categoryorder='array', # Use the order from the sorted df_arch_common_no_nodomain\n",
    "                    categoryarray=df_arch_common_no_nodomain.index.tolist(),\n",
    "                    ticktext = [truncate_string(s, 50) for s in df_arch_common_no_nodomain.index], \n",
    "                    tickvals = df_arch_common_no_nodomain.index\n",
    "                )\n",
    "                fig_arch_common_no_nodomain.show()\n",
    "            else:\n",
    "                print(\"\\nNo common architectures found after excluding 'No_Domain'.\")\n",
    "\n",
    "        else:\n",
    "            print(\"No common architectures found between ESPs and Non-ESPs in the high-quality hits subset.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping architecture comparison: Input DataFrames empty or required columns missing.\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n--- Cell 10 (Domain Architecture Comparison) Complete ---\")\n",
    "print(\"This cell compared domain architectures between ESPs and Non-ESPs with high-quality eukaryotic hits.\")\n",
    "print(\"Review the tables and plots, especially the list of Non-ESP architectures containing ESP-like domains.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5625a26a-aa08-49db-a879-a806a0dccc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: IPR Overlap Analysis (ESP vs. Non-ESP High-Quality Hits)\n",
    "\n",
    "# --- Standard Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from collections import Counter\n",
    "import re # For parsing IPRs\n",
    "\n",
    "# --- Plotly Imports ---\n",
    "# Assuming plotly express and graph_objects were imported as px and go in Cell 1\n",
    "# import plotly.express as px \n",
    "# import plotly.graph_objects as go \n",
    "\n",
    "print(\"\\n\\n--- Cell 11: IPR Overlap Analysis (ESP vs Non-ESP High-Quality Hits) ---\")\n",
    "\n",
    "# --- Assumptions ---\n",
    "# This cell assumes Cell 1, 2, and 7 have been run successfully.\n",
    "# It relies on the following DataFrames being available and populated:\n",
    "#   - df_hq_esps: ESPs with high-quality hits (from Cell 7).\n",
    "#   - df_hq_non_esps: Non-ESPs with high-quality hits (from Cell 7).\n",
    "# It also relies on column names (ipr_col), helper functions (translate_architecture - though not strictly needed here),\n",
    "# variables (ipr_lookup), and output directories (output_summary_dir_phase1) defined in Cell 1.\n",
    "\n",
    "# --- Configuration ---\n",
    "top_n_ipr_compare = 25 # How many top shared/unique IPRs to show\n",
    "\n",
    "# --- Check if necessary DataFrames/Variables exist ---\n",
    "if 'df_hq_esps' not in locals() or 'df_hq_non_esps' not in locals():\n",
    "    print(\"ERROR: DataFrames 'df_hq_esps' and/or 'df_hq_non_esps' not found.\")\n",
    "    print(\"       Cannot perform comparison. Please ensure Cell 7 ran successfully.\")\n",
    "    if 'df_hq_esps' not in locals(): df_hq_esps = pd.DataFrame()\n",
    "    if 'df_hq_non_esps' not in locals(): df_hq_non_esps = pd.DataFrame()\n",
    "\n",
    "if 'ipr_col' not in locals():\n",
    "     print(\"ERROR: Variable 'ipr_col' not defined. Please ensure Cell 1 ran.\")\n",
    "     # Exit or assign dummy value if you want to test structure\n",
    "     # sys.exit()\n",
    "     ipr_col = 'IPR_Signatures' # Assign dummy - analysis will likely fail later\n",
    "     \n",
    "if 'ipr_lookup' not in locals():\n",
    "     print(\"WARNING: 'ipr_lookup' dictionary not found. Descriptions will not be available.\")\n",
    "     ipr_lookup = {}\n",
    "\n",
    "# --- Helper function to get IPR counts from a DataFrame ---\n",
    "def get_ipr_counts(df, ipr_column_name):\n",
    "    \"\"\"Parses IPR strings and returns a Counter object of IPR frequencies.\"\"\"\n",
    "    if ipr_column_name not in df.columns:\n",
    "        print(f\"Warning: IPR column '{ipr_column_name}' not found in DataFrame. Returning empty Counter.\")\n",
    "        return Counter()\n",
    "        \n",
    "    all_iprs_list = []\n",
    "    for ipr_string_raw in df[ipr_column_name].dropna():\n",
    "        processed_ipr_string = str(ipr_string_raw).replace('|', ';')\n",
    "        individual_iprs = processed_ipr_string.split(';')\n",
    "        for ipr_item in individual_iprs:\n",
    "            stripped_ipr = ipr_item.strip()\n",
    "            if stripped_ipr and stripped_ipr.startswith(\"IPR\"): # Basic check for IPR format\n",
    "                all_iprs_list.append(stripped_ipr)\n",
    "            # else: # Optional: log skipped non-IPR items\n",
    "            #    if stripped_ipr: logging.debug(f\"Skipping non-IPR item: {stripped_ipr}\")\n",
    "                \n",
    "    return Counter(all_iprs_list)\n",
    "\n",
    "# --- Calculate IPR Counts for Both Groups ---\n",
    "ipr_counts_esp = Counter()\n",
    "ipr_counts_non_esp = Counter()\n",
    "\n",
    "if not df_hq_esps.empty:\n",
    "    print(\"Calculating IPR counts for ESPs with high-quality hits...\")\n",
    "    ipr_counts_esp = get_ipr_counts(df_hq_esps, ipr_col)\n",
    "    print(f\"  Found {len(ipr_counts_esp)} unique IPR IDs in ESPs.\")\n",
    "else:\n",
    "    print(\"ESP subset is empty, cannot calculate IPR counts.\")\n",
    "\n",
    "if not df_hq_non_esps.empty:\n",
    "    print(\"Calculating IPR counts for Non-ESPs with high-quality hits...\")\n",
    "    ipr_counts_non_esp = get_ipr_counts(df_hq_non_esps, ipr_col)\n",
    "    print(f\"  Found {len(ipr_counts_non_esp)} unique IPR IDs in Non-ESPs.\")\n",
    "else:\n",
    "    print(\"Non-ESP subset is empty, cannot calculate IPR counts.\")\n",
    "\n",
    "# --- Analyze Overlap ---\n",
    "if ipr_counts_esp and ipr_counts_non_esp:\n",
    "    print(\"\\n--- Analyzing IPR Overlap ---\")\n",
    "    \n",
    "    # Convert Counters to DataFrames for easier merging\n",
    "    df_ipr_esp = pd.DataFrame(ipr_counts_esp.items(), columns=['IPR_ID', 'ESP_Count']).set_index('IPR_ID')\n",
    "    df_ipr_non_esp = pd.DataFrame(ipr_counts_non_esp.items(), columns=['IPR_ID', 'Non_ESP_Count']).set_index('IPR_ID')\n",
    "    \n",
    "    # Outer merge to get all IPRs and their counts in each group\n",
    "    df_ipr_compare = pd.merge(df_ipr_esp, df_ipr_non_esp, left_index=True, right_index=True, how='outer').fillna(0).astype(int)\n",
    "    df_ipr_compare['Total_Count'] = df_ipr_compare['ESP_Count'] + df_ipr_compare['Non_ESP_Count']\n",
    "    \n",
    "    # Add descriptions\n",
    "    df_ipr_compare['Description'] = df_ipr_compare.index.map(lambda x: ipr_lookup.get(x, {}).get('Name', 'N/A'))\n",
    "    df_ipr_compare['Type'] = df_ipr_compare.index.map(lambda x: ipr_lookup.get(x, {}).get('Type', 'N/A'))\n",
    "    \n",
    "    df_ipr_compare = df_ipr_compare.sort_values('Total_Count', ascending=False)\n",
    "    \n",
    "    # 1. Common IPRs\n",
    "    df_ipr_common = df_ipr_compare[(df_ipr_compare['ESP_Count'] > 0) & (df_ipr_compare['Non_ESP_Count'] > 0)].copy()\n",
    "    print(f\"\\nFound {len(df_ipr_common)} IPR signatures common to both ESP and Non-ESP hits.\")\n",
    "    \n",
    "    if not df_ipr_common.empty:\n",
    "        print(f\"\\nTop {top_n_ipr_compare} Common IPR Signatures (Sorted by Total Count):\")\n",
    "        display_cols_common = ['ESP_Count', 'Non_ESP_Count', 'Total_Count', 'Description', 'Type']\n",
    "        try:\n",
    "            print(df_ipr_common[display_cols_common].head(top_n_ipr_compare).to_markdown())\n",
    "        except ImportError:\n",
    "            print(df_ipr_common[display_cols_common].head(top_n_ipr_compare))\n",
    "            \n",
    "        # Visualize top common IPRs\n",
    "        df_common_plot_data = df_ipr_common.head(top_n_ipr_compare).reset_index()\n",
    "        df_common_plot_data['Label'] = df_common_plot_data['Description'].apply(lambda x: truncate_string(x, 40)) + \" (\" + df_common_plot_data['IPR_ID'] + \")\"\n",
    "        \n",
    "        df_common_melt = df_common_plot_data.melt(\n",
    "            id_vars=['IPR_ID', 'Label', 'Description', 'Type'], \n",
    "            value_vars=['ESP_Count', 'Non_ESP_Count'],\n",
    "            var_name='Group', value_name='Count'\n",
    "        )\n",
    "        \n",
    "        fig_ipr_common = px.bar(\n",
    "            df_common_melt,\n",
    "            x='Label',\n",
    "            y='Count',\n",
    "            color='Group',\n",
    "            barmode='group',\n",
    "            title=f'Top {top_n_ipr_compare} Common IPR Frequencies (ESP vs Non-ESP Hits)',\n",
    "            hover_data=['IPR_ID', 'Description', 'Type'],\n",
    "            labels={'Label': 'IPR Signature', 'Count': 'Number of Proteins'},\n",
    "            color_discrete_map={'ESP_Count': arcadia_colors_manual.get('amber', '#F28360'), \n",
    "                                'Non_ESP_Count': arcadia_colors_manual.get('aegean', '#5088C5')}\n",
    "        )\n",
    "        fig_ipr_common.update_xaxes(tickangle=45, categoryorder='total descending') # Order by total count implicitly via melt order\n",
    "        fig_ipr_common.show()\n",
    "        \n",
    "    # 2. Unique IPRs\n",
    "    df_ipr_esp_unique = df_ipr_compare[(df_ipr_compare['ESP_Count'] > 0) & (df_ipr_compare['Non_ESP_Count'] == 0)].copy()\n",
    "    df_ipr_non_esp_unique = df_ipr_compare[(df_ipr_compare['ESP_Count'] == 0) & (df_ipr_compare['Non_ESP_Count'] > 0)].copy()\n",
    "\n",
    "    print(f\"\\nFound {len(df_ipr_esp_unique)} IPR signatures unique to ESP hits.\")\n",
    "    if not df_ipr_esp_unique.empty:\n",
    "        print(f\"Top {top_n_ipr_compare} Unique IPRs in ESP Hits:\")\n",
    "        display_cols_unique = ['ESP_Count', 'Description', 'Type']\n",
    "        try:\n",
    "            print(df_ipr_esp_unique[display_cols_unique].head(top_n_ipr_compare).to_markdown())\n",
    "        except ImportError:\n",
    "            print(df_ipr_esp_unique[display_cols_unique].head(top_n_ipr_compare))\n",
    "\n",
    "    print(f\"\\nFound {len(df_ipr_non_esp_unique)} IPR signatures unique to Non-ESP hits.\")\n",
    "    if not df_ipr_non_esp_unique.empty:\n",
    "        print(f\"Top {top_n_ipr_compare} Unique IPRs in Non-ESP Hits:\")\n",
    "        display_cols_unique_non = ['Non_ESP_Count', 'Description', 'Type']\n",
    "        try:\n",
    "            print(df_ipr_non_esp_unique[display_cols_unique_non].head(top_n_ipr_compare).to_markdown())\n",
    "        except ImportError:\n",
    "            print(df_ipr_non_esp_unique[display_cols_unique_non].head(top_n_ipr_compare))\n",
    "            \n",
    "    # Save unique lists\n",
    "    unique_esp_filename = os.path.join(output_summary_dir_phase1, \"unique_iprs_in_esp_hq_hits.csv\")\n",
    "    unique_non_esp_filename = os.path.join(output_summary_dir_phase1, \"unique_iprs_in_non_esp_hq_hits.csv\")\n",
    "    try:\n",
    "        df_ipr_esp_unique.reset_index().to_csv(unique_esp_filename, index=False)\n",
    "        df_ipr_non_esp_unique.reset_index().to_csv(unique_non_esp_filename, index=False)\n",
    "        print(f\"\\nSaved unique IPR lists to '{unique_esp_filename}' and '{unique_non_esp_filename}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving unique IPR lists: {e}\")\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping IPR overlap analysis: Could not calculate IPR counts for both ESP and Non-ESP groups.\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n--- Cell 11 (IPR Overlap Analysis) Complete ---\")\n",
    "print(\"This cell analyzed the overlap and differences in IPR signatures between ESPs and Non-ESPs with high-quality eukaryotic hits.\")\n",
    "print(\"Examine the common and unique IPR lists for functional insights.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a1e16c-99d4-4b4a-9645-aa214dfa4c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Integrate Eukaryotic Hit Names with Asgard Domain/IPR Analysis\n",
    "\n",
    "# --- Standard Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from collections import Counter\n",
    "import re \n",
    "\n",
    "# --- Plotly Imports ---\n",
    "# Assuming plotly express and graph_objects were imported as px and go in Cell 1\n",
    "# import plotly.express as px \n",
    "# import plotly.graph_objects as go \n",
    "\n",
    "print(\"\\n\\n--- Cell 12: Integrate Eukaryotic Hit Names with Asgard Domain/IPR Analysis ---\")\n",
    "\n",
    "# --- Assumptions ---\n",
    "# This cell assumes Cell 1 and Cell 2 have been run successfully.\n",
    "# It relies on the DataFrame 'df_full' being loaded from the LATEST database CSV \n",
    "# (v1.9 with Euk protein names) in Cell 1 (or re-loaded here).\n",
    "# It also assumes 'df_high_quality_hits' was defined in Cell 2 based on quality/coverage filters.\n",
    "# It relies on column names, helper functions (translate_architecture), ipr_lookup, \n",
    "# color maps, and output directories defined in Cell 1.\n",
    "\n",
    "# --- Configuration ---\n",
    "# Input DB path (ensure this is the LATEST version with Euk names)\n",
    "db_path_v1_9 = 'proteome_database_combined_v2.0.csv' \n",
    "\n",
    "# Filtering criteria for high-quality hits (should match Cell 2 for consistency)\n",
    "# Re-define here or ensure they are available from Cell 2's execution state\n",
    "min_query_coverage_filter = 0.70 \n",
    "min_subject_coverage_filter = 0.50\n",
    "min_pident_filter = 30.0 \n",
    "\n",
    "# Keywords to search for in Eukaryotic Hit Protein Names (Case-Insensitive)\n",
    "# Customize this list based on functions of interest (ESP-like, etc.)\n",
    "interesting_euk_keywords = [\n",
    "    'kinase', 'phosphatase', 'gtpase', 'ras', 'rho', 'rab', 'ran', 'arl', # Signaling/GTPases\n",
    "    'ubiquitin', 'nedd8', 'sumo', 'ubl', 'e1', 'e2', 'e3', 'ligase', 'dubb', # UBL system\n",
    "    'actin', 'tubulin', 'cytoskeleton', 'kinesin', 'dynein', 'myosin', # Cytoskeleton/Motors\n",
    "    'snare', 'escrt', 'vps', 'sec', 'adaptin', 'clathrin', 'copi', 'copii', # Trafficking\n",
    "    'nuclear pore', 'importin', 'exportin', 'karyopherin', # Nuclear transport\n",
    "    'wd40', 'tpr', 'ankyrin', 'armadillo', 'heat repeat', # Repeat domains common in Euks\n",
    "    'cadherin', 'integrin', 'catenin', # Adhesion (less expected but possible)\n",
    "    'signal recognition particle', 'translocon', # Secretion machinery\n",
    "    'receptor', 'transporter', 'channel' # Membrane proteins\n",
    "]\n",
    "# Create a regex pattern for searching (case-insensitive OR match)\n",
    "keyword_pattern = r'\\b(?:' + '|'.join(re.escape(kw) for kw in interesting_euk_keywords) + r')\\b'\n",
    "print(f\"Filtering Euk Hit Names using pattern: {keyword_pattern[:100]}...\") # Print start of pattern\n",
    "\n",
    "top_n_display = 20 # For tables/plots in this cell\n",
    "\n",
    "# --- Check/Load Data and Re-filter for High Quality Hits ---\n",
    "# It's safer to reload the v1.9 data here and re-apply filters\n",
    "# to ensure we have the Euk_Hit_Protein_Name column.\n",
    "print(f\"\\nLoading latest database: {db_path_v1_9}\")\n",
    "if 'df_full' not in locals() or euk_hit_protein_name_col not in df_full.columns:\n",
    "    try:\n",
    "        df_full = pd.read_csv(db_path_v1_9, low_memory=False)\n",
    "        # Ensure ProteinID is string type right after loading\n",
    "        if protein_id_col in df_full.columns:\n",
    "             df_full[protein_id_col] = df_full[protein_id_col].astype(str)\n",
    "        print(f\"Successfully loaded database v1.9. Shape: {df_full.shape}\")\n",
    "        # Re-calculate coverage if necessary (if Cell 2 wasn't just run with v1.9)\n",
    "        # Basic check - recalculate if coverage columns are missing or mostly NaN\n",
    "        needs_coverage_recalc = (\n",
    "            query_coverage_col not in df_full.columns or \n",
    "            subject_coverage_col not in df_full.columns or\n",
    "            df_full[query_coverage_col].isna().mean() > 0.95 or # If >95% NaN, likely not calculated on this df\n",
    "            df_full[subject_coverage_col].isna().mean() > 0.95\n",
    "        )\n",
    "        if needs_coverage_recalc:\n",
    "            print(\"Coverage columns missing or incomplete. Re-running coverage calculation steps from Cell 2...\")\n",
    "            # --- Add code snippet from Cell 2's coverage calculation here if needed ---\n",
    "            # This involves re-parsing DIAMOND, merging details, and calculating coverage.\n",
    "            # For brevity, assuming coverage columns ARE present in the loaded v1.9 file for now.\n",
    "            # If they are NOT, you MUST copy the relevant code block from Cell 2 here.\n",
    "            print(\"WARNING: Coverage recalculation logic not included here for brevity. Assuming columns exist in CSV.\")\n",
    "            pass \n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Database file '{db_path_v1_9}' not found.\")\n",
    "        sys.exit() # Stop if we can't load the required data\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading database '{db_path_v1_9}': {e}\")\n",
    "        sys.exit()\n",
    "\n",
    "# Re-apply high-quality filter\n",
    "required_filter_cols = [group_col, hit_flag_col, query_coverage_col, subject_coverage_col, euk_hit_pident_col]\n",
    "if all(col in df_full.columns for col in required_filter_cols):\n",
    "     df_high_quality_hits = df_full[\n",
    "        (df_full[group_col] == 'Asgard') & \n",
    "        (df_full[hit_flag_col] == True) &\n",
    "        (df_full[query_coverage_col] >= min_query_coverage_filter) &\n",
    "        (df_full[subject_coverage_col] >= min_subject_coverage_filter) &\n",
    "        (df_full[euk_hit_pident_col] >= min_pident_filter)\n",
    "    ].copy()\n",
    "     print(f\"\\nFiltered {len(df_high_quality_hits)} high-quality Asgard hits based on coverage/pident.\")\n",
    "else:\n",
    "     print(f\"ERROR: Cannot filter for high-quality hits, missing required columns: {[c for c in required_filter_cols if c not in df_full.columns]}\")\n",
    "     df_high_quality_hits = pd.DataFrame() # Ensure it's defined\n",
    "\n",
    "# --- Check if necessary DataFrames/Columns exist ---\n",
    "if df_high_quality_hits.empty:\n",
    "    print(\"ERROR: DataFrame 'df_high_quality_hits' is empty after filtering. Cannot proceed.\")\n",
    "elif euk_hit_protein_name_col not in df_high_quality_hits.columns:\n",
    "    print(f\"ERROR: Column '{euk_hit_protein_name_col}' not found in df_high_quality_hits.\")\n",
    "elif esp_col not in df_high_quality_hits.columns:\n",
    "     print(f\"ERROR: Column '{esp_col}' not found in df_high_quality_hits.\")\n",
    "else:\n",
    "    # --- Filter for Hits with Interesting Eukaryotic Protein Names ---\n",
    "    print(f\"\\n--- Filtering for hits where Euk Protein Name contains keywords ---\")\n",
    "    # Ensure the column is string and handle NA\n",
    "    df_high_quality_hits[euk_hit_protein_name_col] = df_high_quality_hits[euk_hit_protein_name_col].fillna('').astype(str)\n",
    "    \n",
    "    # Apply the regex filter (case-insensitive)\n",
    "    keyword_hits_mask = df_high_quality_hits[euk_hit_protein_name_col].str.contains(keyword_pattern, case=False, regex=True, na=False)\n",
    "    df_keyword_hits = df_high_quality_hits[keyword_hits_mask].copy()\n",
    "    \n",
    "    print(f\"Found {len(df_keyword_hits)} high-quality hits where Euk Protein Name matches keywords.\")\n",
    "\n",
    "    if not df_keyword_hits.empty:\n",
    "        # --- Analyze this interesting subset ---\n",
    "        \n",
    "        # Separate ESPs and Non-ESPs within this keyword-filtered subset\n",
    "        df_keyword_esps = df_keyword_hits[df_keyword_hits[esp_col] == True].copy()\n",
    "        df_keyword_non_esps = df_keyword_hits[df_keyword_hits[esp_col] == False].copy()\n",
    "        print(f\"  Keyword hits breakdown: {len(df_keyword_esps)} ESPs, {len(df_keyword_non_esps)} Non-ESPs.\")\n",
    "\n",
    "        # 1. Top Eukaryotic Protein Names in this subset (ESP vs Non-ESP)\n",
    "        print(\"\\n--- 1. Top Euk Protein Names in Keyword-Filtered Subset ---\")\n",
    "        if not df_keyword_esps.empty:\n",
    "             df_keyword_esps['Cleaned_Euk_Prot_Name'] = df_keyword_esps[euk_hit_protein_name_col].apply(clean_protein_name)\n",
    "             esp_keyword_name_counts = df_keyword_esps['Cleaned_Euk_Prot_Name'].value_counts()\n",
    "             print(f\"\\nTop {top_n_display} Cleaned Euk Names (Keyword Hits - ESPs):\")\n",
    "             print(esp_keyword_name_counts.head(top_n_display).to_markdown())\n",
    "        if not df_keyword_non_esps.empty:\n",
    "             df_keyword_non_esps['Cleaned_Euk_Prot_Name'] = df_keyword_non_esps[euk_hit_protein_name_col].apply(clean_protein_name)\n",
    "             non_esp_keyword_name_counts = df_keyword_non_esps['Cleaned_Euk_Prot_Name'].value_counts()\n",
    "             print(f\"\\nTop {top_n_display} Cleaned Euk Names (Keyword Hits - Non-ESPs):\")\n",
    "             print(non_esp_keyword_name_counts.head(top_n_display).to_markdown())\n",
    "\n",
    "        # 2. Domain Architectures of Non-ESPs hitting interesting Euk proteins\n",
    "        print(\"\\n--- 2. Domain Architectures of Non-ESPs hitting interesting Euk proteins ---\")\n",
    "        if not df_keyword_non_esps.empty and domain_arch_col in df_keyword_non_esps.columns:\n",
    "            keyword_non_esp_arch_counts = df_keyword_non_esps[domain_arch_col].fillna('No_Domain').value_counts()\n",
    "            df_keyword_non_esp_arch_plot = keyword_non_esp_arch_counts.head(top_n_display).reset_index()\n",
    "            df_keyword_non_esp_arch_plot.columns = ['Architecture', 'Count']\n",
    "            df_keyword_non_esp_arch_plot['Translated Architecture'] = df_keyword_non_esp_arch_plot['Architecture'].apply(translate_architecture, lookup_dict=ipr_lookup)\n",
    "            \n",
    "            print(f\"\\nTop {top_n_display} Architectures (Keyword Hits - Non-ESPs):\")\n",
    "            print(df_keyword_non_esp_arch_plot.to_markdown(index=False))\n",
    "\n",
    "            # Plot\n",
    "            fig_arch_kw_non_esp = px.bar(\n",
    "                df_keyword_non_esp_arch_plot, x='Architecture', y='Count',\n",
    "                title=f'Top {top_n_display} Architectures of Non-ESPs hitting Euk Proteins with Keywords',\n",
    "                hover_data=['Translated Architecture'],\n",
    "                labels={'Architecture': 'Domain Architecture (Raw)', 'Count': 'Number of Non-ESP Proteins'}\n",
    "            )\n",
    "            fig_arch_kw_non_esp.update_xaxes(tickangle=45, categoryorder='total descending',\n",
    "                                          ticktext = [truncate_string(s, 50) for s in df_keyword_non_esp_arch_plot['Architecture']],\n",
    "                                          tickvals = df_keyword_non_esp_arch_plot['Architecture'])\n",
    "            fig_arch_kw_non_esp.show()\n",
    "        else:\n",
    "            print(\"No Non-ESPs found hitting Euk proteins with keywords, or domain column missing.\")\n",
    "\n",
    "        # 3. IPR Signatures of Non-ESPs hitting interesting Euk proteins\n",
    "        print(\"\\n--- 3. IPR Signatures of Non-ESPs hitting interesting Euk proteins ---\")\n",
    "        if not df_keyword_non_esps.empty and ipr_col in df_keyword_non_esps.columns and 'ipr_lookup' in locals():\n",
    "            ipr_counts_kw_non_esp = get_ipr_counts(df_keyword_non_esps, ipr_col) # Use helper from Cell 11\n",
    "            if ipr_counts_kw_non_esp:\n",
    "                df_ipr_kw_non_esp = pd.DataFrame(ipr_counts_kw_non_esp.most_common(top_n_display), columns=['IPR_ID', 'Count'])\n",
    "                df_ipr_kw_non_esp['Description'] = df_ipr_kw_non_esp['IPR_ID'].apply(lambda x: ipr_lookup.get(x, {}).get('Name', 'N/A'))\n",
    "                df_ipr_kw_non_esp['Label'] = df_ipr_kw_non_esp['Description'].apply(lambda x: truncate_string(x, 60)) + \" (\" + df_ipr_kw_non_esp['IPR_ID'] + \")\"\n",
    "\n",
    "                print(f\"\\nTop {top_n_display} IPRs (Keyword Hits - Non-ESPs):\")\n",
    "                print(df_ipr_kw_non_esp[['IPR_ID', 'Count', 'Description']].to_markdown(index=False))\n",
    "\n",
    "                # Plot\n",
    "                fig_ipr_kw_non_esp = px.bar(\n",
    "                    df_ipr_kw_non_esp, x='Label', y='Count',\n",
    "                    title=f'Top {top_n_display} IPRs of Non-ESPs hitting Euk Proteins with Keywords',\n",
    "                    hover_data=['IPR_ID', 'Description'],\n",
    "                    labels={'Label': 'IPR Signature', 'Count': 'Number of Non-ESP Proteins'}\n",
    "                )\n",
    "                fig_ipr_kw_non_esp.update_xaxes(categoryorder='total descending')\n",
    "                fig_ipr_kw_non_esp.show()\n",
    "            else:\n",
    "                print(\"No IPRs found for Non-ESPs hitting Euk proteins with keywords.\")\n",
    "        else:\n",
    "            print(f\"Skipping IPR analysis for keyword hits: Column '{ipr_col}' missing or ipr_lookup not defined.\")\n",
    "\n",
    "        # 4. Generate Summary Table of Potential \"Missed\" ESPs\n",
    "        print(\"\\n--- 4. Potential 'Missed' ESP Candidates (Non-ESPs hitting Euk proteins with keywords) ---\")\n",
    "        # Select relevant columns for the summary table\n",
    "        summary_cols = [\n",
    "            protein_id_col, orthogroup_col, broad_func_cat_col, domain_arch_col, ipr_col, \n",
    "            euk_hit_sseqid_col, euk_hit_organism_col, euk_hit_protein_name_col,\n",
    "            euk_hit_pident_col, euk_hit_evalue_col, query_coverage_col, subject_coverage_col\n",
    "        ]\n",
    "        # Ensure columns exist before selecting\n",
    "        summary_cols_present = [c for c in summary_cols if c in df_keyword_non_esps.columns]\n",
    "        df_missed_esp_candidates = df_keyword_non_esps[summary_cols_present].copy()\n",
    "        \n",
    "        # Optionally add translated architecture\n",
    "        if 'translate_architecture' in locals() and domain_arch_col in df_missed_esp_candidates.columns:\n",
    "             df_missed_esp_candidates['Translated Architecture'] = df_missed_esp_candidates[domain_arch_col].apply(translate_architecture, lookup_dict=ipr_lookup)\n",
    "             \n",
    "        print(f\"Found {len(df_missed_esp_candidates)} potential candidates.\")\n",
    "        print(\"Displaying first 15 candidates:\")\n",
    "        try:\n",
    "            print(df_missed_esp_candidates.head(15).to_markdown(index=False))\n",
    "        except ImportError:\n",
    "            print(df_missed_esp_candidates.head(15))\n",
    "            \n",
    "        # Save this candidate list\n",
    "        candidates_filename = os.path.join(output_summary_dir_phase1, \"potential_missed_esp_candidates.csv\")\n",
    "        try:\n",
    "             df_missed_esp_candidates.to_csv(candidates_filename, index=False)\n",
    "             print(f\"\\nSaved potential 'missed' ESP candidate list to '{candidates_filename}'\")\n",
    "        except Exception as e:\n",
    "             print(f\"Error saving candidate list: {e}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No high-quality hits found where Euk Protein Name matches keywords.\")\n",
    "\n",
    "print(\"\\n\\n--- Cell 12 (Integrate Euk Names & Asgard Domains) Complete ---\")\n",
    "print(\"This cell filtered for Asgard proteins hitting Eukaryotic proteins with potentially\")\n",
    "print(\"interesting functions (based on keywords) and analyzed their domain content.\")\n",
    "print(\"The 'potential_missed_esp_candidates.csv' file lists Non-ESPs in this category.\")\n",
    "print(\"Focusing on these candidates for deeper dives (RBH, MSA, Phylogeny) is a good strategy for publication.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152064e3-dd4c-426f-aac3-a965394350bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Analysis of Eukaryotic Hit Protein Names & Focus on \"No Domain\" Hits\n",
    "\n",
    "# --- Standard Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from collections import Counter\n",
    "import re \n",
    "import sys # Added for checking variable existence robustly\n",
    "\n",
    "# --- Plotly Imports ---\n",
    "# Assuming plotly express and graph_objects were imported as px and go in Cell 1\n",
    "import plotly.express as px \n",
    "import plotly.graph_objects as go \n",
    "\n",
    "print(\"\\n\\n--- Cell 13: Euk Hit Protein Name Analysis & 'No Domain' Focus ---\")\n",
    "\n",
    "# --- Assumptions ---\n",
    "# This cell assumes Cell 1 and the consolidated Cell 2 have been run successfully.\n",
    "# It relies on the DataFrame 'df_high_quality_hits' being available and populated.\n",
    "# It also relies on column names (euk_hit_protein_name_col, num_domains_col, domain_arch_col), \n",
    "# helper functions (clean_protein_name), and output directories defined in Cell 1.\n",
    "\n",
    "# --- Configuration ---\n",
    "top_n_names_display = 30 # How many top names to show in tables/plots\n",
    "\n",
    "# --- Check if necessary DataFrames/Variables exist ---\n",
    "required_dataframes = ['df_high_quality_hits']\n",
    "required_variables = [\n",
    "    'euk_hit_protein_name_col', 'num_domains_col', 'domain_arch_col', \n",
    "    'clean_protein_name', 'output_summary_dir_phase1' \n",
    "] # Removed function checks as they should be defined if Cell 1 ran\n",
    "\n",
    "missing_data = False\n",
    "for df_name in required_dataframes:\n",
    "    if df_name not in locals() or locals()[df_name].empty:\n",
    "        print(f\"ERROR: Required DataFrame '{df_name}' is missing or empty. Please run previous cells (especially Cell 2).\")\n",
    "        missing_data = True\n",
    "        # Define as empty to prevent downstream errors\n",
    "        if df_name == 'df_high_quality_hits': df_high_quality_hits = pd.DataFrame() \n",
    "        \n",
    "for var_name in required_variables:\n",
    "     if var_name not in locals():\n",
    "         print(f\"ERROR: Required variable or function '{var_name}' is missing. Please run previous cells (especially Cell 1).\")\n",
    "         missing_data = True\n",
    "         \n",
    "if missing_data:\n",
    "    print(\"Cannot proceed with Cell 13 due to missing data or variables.\")\n",
    "    # Optionally raise an error: raise NameError(\"Prerequisite data/variables missing.\")\n",
    "else:\n",
    "    # --- Analyze Top Eukaryotic Hit Protein Names (Overall High-Quality Hits) ---\n",
    "    print(f\"\\n--- Analyzing Eukaryotic Hit Protein Names for ALL {len(df_high_quality_hits)} High-Quality Hits ---\")\n",
    "    \n",
    "    # Apply cleaning function (defined in Cell 1 or previous cells)\n",
    "    if euk_hit_protein_name_col in df_high_quality_hits.columns:\n",
    "        df_high_quality_hits['Cleaned_Euk_Prot_Name'] = df_high_quality_hits[euk_hit_protein_name_col].apply(clean_protein_name)\n",
    "        overall_hit_name_counts = df_high_quality_hits['Cleaned_Euk_Prot_Name'].value_counts()\n",
    "        \n",
    "        print(f\"\\nTop {top_n_names_display} Cleaned Eukaryotic Protein Names (Overall High-Quality Hits):\")\n",
    "        # Filter out generic 'Uncharacterized' or 'Unknown' before displaying top N\n",
    "        overall_hit_name_counts_filtered = overall_hit_name_counts[~overall_hit_name_counts.index.isin(['Uncharacterized', 'Unknown/Not Found'])]\n",
    "        try:\n",
    "            print(overall_hit_name_counts_filtered.head(top_n_names_display).to_markdown())\n",
    "        except ImportError:\n",
    "            print(overall_hit_name_counts_filtered.head(top_n_names_display))\n",
    "            \n",
    "        # Save summary (Full list)\n",
    "        overall_names_filename = os.path.join(output_summary_dir_phase1, \"overall_hq_hits_top_euk_prot_names.csv\")\n",
    "        try:\n",
    "             overall_hit_name_counts.reset_index().rename(columns={'index':'Cleaned_Euk_Prot_Name', 'Cleaned_Euk_Prot_Name':'Count'}).to_csv(overall_names_filename, index=False)\n",
    "             print(f\"Saved overall hit protein name counts to '{overall_names_filename}'\")\n",
    "        except Exception as e:\n",
    "             print(f\"Error saving overall hit protein name counts: {e}\")\n",
    "\n",
    "        # Plot (Top N filtered names)\n",
    "        df_overall_names_plot = overall_hit_name_counts_filtered.head(top_n_names_display).reset_index()\n",
    "        if not df_overall_names_plot.empty: # Check if anything remains after filtering\n",
    "            df_overall_names_plot.columns = ['Cleaned Name', 'Count']\n",
    "            fig_names_overall = px.bar(\n",
    "                df_overall_names_plot,\n",
    "                x='Cleaned Name',\n",
    "                y='Count',\n",
    "                title=f'Top {top_n_names_display} Eukaryotic Hit Protein Names (Functionally Annotated) for Overall High-Quality Asgard Hits',\n",
    "                labels={'Cleaned Name': 'Cleaned Eukaryotic Protein Name', 'Count': 'Number of Asgard Hits'}\n",
    "            )\n",
    "            fig_names_overall.update_xaxes(tickangle=45, categoryorder='total descending')\n",
    "            fig_names_overall.show()\n",
    "        else:\n",
    "            print(\"No functionally annotated protein names found in the top overall high-quality hits after cleaning.\")\n",
    "    else:\n",
    "         print(f\"Cannot analyze Euk Hit Names: Column '{euk_hit_protein_name_col}' not found.\")\n",
    "\n",
    "\n",
    "    # --- Analyze Hits for \"No Domain\" Asgard Proteins ---\n",
    "    print(f\"\\n\\n--- Analyzing Eukaryotic Hits for 'No Domain' Asgard Proteins ---\")\n",
    "    \n",
    "    # Identify \"No Domain\" proteins within the high-quality hits\n",
    "    # Prefer checking Num_Domains is NaN, fallback to checking Domain_Architecture\n",
    "    if num_domains_col in df_high_quality_hits.columns:\n",
    "        no_domain_mask = df_high_quality_hits[num_domains_col].isna()\n",
    "        print(f\"Identifying 'No Domain' proteins based on '{num_domains_col}' being NaN.\")\n",
    "    elif domain_arch_col in df_high_quality_hits.columns:\n",
    "         no_domain_mask = df_high_quality_hits[domain_arch_col].fillna('No_Domain') == 'No_Domain'\n",
    "         print(f\"Identifying 'No Domain' proteins based on '{domain_arch_col}' being 'No_Domain' or NaN.\")\n",
    "    else:\n",
    "         print(\"Cannot identify 'No Domain' proteins, required columns missing.\")\n",
    "         no_domain_mask = pd.Series(False, index=df_high_quality_hits.index) # Create mask of False\n",
    "\n",
    "    df_no_domain_hits = df_high_quality_hits[no_domain_mask].copy()\n",
    "    \n",
    "    if not df_no_domain_hits.empty:\n",
    "        print(f\"Found {len(df_no_domain_hits)} 'No Domain' Asgard proteins within the high-quality hits subset.\")\n",
    "\n",
    "        # Analyze the Euk Hit Protein Names for this subset\n",
    "        # Use the already cleaned names from the parent df if available\n",
    "        if 'Cleaned_Euk_Prot_Name' in df_no_domain_hits.columns:\n",
    "             no_domain_hit_name_counts = df_no_domain_hits['Cleaned_Euk_Prot_Name'].value_counts()\n",
    "        elif euk_hit_protein_name_col in df_no_domain_hits.columns: # Apply cleaning if needed\n",
    "             df_no_domain_hits['Cleaned_Euk_Prot_Name'] = df_no_domain_hits[euk_hit_protein_name_col].apply(clean_protein_name)\n",
    "             no_domain_hit_name_counts = df_no_domain_hits['Cleaned_Euk_Prot_Name'].value_counts()\n",
    "        else:\n",
    "             print(\"Cannot get Euk Hit names for No Domain subset.\")\n",
    "             no_domain_hit_name_counts = pd.Series(dtype='int64') # Empty series\n",
    "\n",
    "        \n",
    "        if not no_domain_hit_name_counts.empty:\n",
    "            print(f\"\\nTop {top_n_names_display} Cleaned Eukaryotic Protein Names (Hits for 'No Domain' Asgard Proteins):\")\n",
    "            # Filter out generic 'Uncharacterized' or 'Unknown' \n",
    "            no_domain_hit_name_counts_filtered = no_domain_hit_name_counts[~no_domain_hit_name_counts.index.isin(['Uncharacterized', 'Unknown/Not Found'])]\n",
    "            try:\n",
    "                print(no_domain_hit_name_counts_filtered.head(top_n_names_display).to_markdown())\n",
    "            except ImportError:\n",
    "                print(no_domain_hit_name_counts_filtered.head(top_n_names_display))\n",
    "                \n",
    "            # Save summary (Full list)\n",
    "            no_domain_names_filename = os.path.join(output_summary_dir_phase1, \"no_domain_hq_hits_top_euk_prot_names.csv\")\n",
    "            try:\n",
    "                 no_domain_hit_name_counts.reset_index().rename(columns={'index':'Cleaned_Euk_Prot_Name', 'Cleaned_Euk_Prot_Name':'Count'}).to_csv(no_domain_names_filename, index=False)\n",
    "                 print(f\"Saved 'No Domain' hit protein name counts to '{no_domain_names_filename}'\")\n",
    "            except Exception as e:\n",
    "                 print(f\"Error saving 'No Domain' hit protein name counts: {e}\")\n",
    "\n",
    "            # Plot (Top N filtered names)\n",
    "            df_no_domain_names_plot = no_domain_hit_name_counts_filtered.head(top_n_names_display).reset_index()\n",
    "            if not df_no_domain_names_plot.empty: # Check if anything remains after filtering\n",
    "                df_no_domain_names_plot.columns = ['Cleaned Name', 'Count']\n",
    "                fig_names_no_domain = px.bar(\n",
    "                    df_no_domain_names_plot,\n",
    "                    x='Cleaned Name',\n",
    "                    y='Count',\n",
    "                    title=f\"Top {top_n_names_display} Eukaryotic Hit Protein Names (Functionally Annotated) for 'No Domain' Asgard Proteins\",\n",
    "                    labels={'Cleaned Name': 'Cleaned Eukaryotic Protein Name', 'Count': \"Number of 'No Domain' Asgard Hits\"}\n",
    "                )\n",
    "                fig_names_no_domain.update_xaxes(tickangle=45, categoryorder='total descending')\n",
    "                fig_names_no_domain.show()\n",
    "            else:\n",
    "                print(\"No functionally annotated protein names found in the top hits for 'No Domain' Asgard proteins after cleaning.\")\n",
    "        else:\n",
    "             print(\"Could not calculate Euk Hit name counts for 'No Domain' subset.\")\n",
    "\n",
    "    else:\n",
    "        print(\"No 'No Domain' Asgard proteins found within the high-quality hits subset.\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n--- Cell 13 (Euk Hit Name Analysis & 'No Domain' Focus) Complete ---\")\n",
    "print(\"This cell analyzed the Eukaryotic protein names associated with high-quality hits,\")\n",
    "print(\"both overall and specifically for Asgard proteins lacking annotated domains.\")\n",
    "print(\"Consider if the hits for 'No Domain' proteins provide functional clues.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7005127-589a-482f-ac0b-1d0ed6a52ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Prepare for Reciprocal Best Hit (RBH) Analysis - Select & Extract Euk Sequences (All HQ Hits)\n",
    "\n",
    "# --- Standard Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import re \n",
    "import sys \n",
    "import logging \n",
    "from pathlib import Path \n",
    "# Ensure Biopython is installed: pip install biopython\n",
    "try:\n",
    "    from Bio import SeqIO \n",
    "    from Bio.SeqRecord import SeqRecord\n",
    "    from Bio.Seq import Seq\n",
    "except ImportError:\n",
    "    print(\"ERROR: Biopython is required for this script. Please install it: pip install biopython\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"\\n\\n--- Cell 14: Prepare for Reciprocal Best Hit (RBH) Analysis (All High-Quality Hits) ---\")\n",
    "\n",
    "# --- Assumptions ---\n",
    "# This cell assumes Cell 1 and Cell 2 have been run successfully.\n",
    "# It relies on 'df_high_quality_hits' being available and populated (defined in Cell 2).\n",
    "# It also relies on column names (euk_hit_sseqid_col, protein_id_col) and \n",
    "# file paths (euk_fasta_path, output_summary_dir_phase1) defined in Cell 1.\n",
    "\n",
    "# --- Configuration ---\n",
    "# Output: FASTA file containing sequences of ALL unique eukaryotic hits from the HQ set\n",
    "output_euk_hits_fasta = Path(output_summary_dir_phase1) / \"all_hq_euk_hits_for_rbh.fasta\"\n",
    "# Output: TSV file mapping Asgard ID to its HQ Euk Hit ID (for easy comparison later)\n",
    "output_rbh_pairs_map = Path(output_summary_dir_phase1) / \"asgard_euk_hq_rbh_pairs_to_test.tsv\"\n",
    "\n",
    "# --- Check if necessary DataFrames/Variables exist ---\n",
    "required_dataframes = ['df_high_quality_hits']\n",
    "required_variables = [\n",
    "    'euk_fasta_path', 'euk_hit_sseqid_col', 'protein_id_col', \n",
    "    'output_summary_dir_phase1'\n",
    "]\n",
    "\n",
    "missing_data = False\n",
    "for df_name in required_dataframes:\n",
    "    if df_name not in locals() or locals()[df_name].empty:\n",
    "        print(f\"ERROR: Required DataFrame '{df_name}' is missing or empty. Please run previous cells (especially Cell 2).\")\n",
    "        missing_data = True\n",
    "        if df_name == 'df_high_quality_hits': df_high_quality_hits = pd.DataFrame() \n",
    "        \n",
    "for var_name in required_variables:\n",
    "     if var_name not in locals():\n",
    "         print(f\"ERROR: Required variable or function '{var_name}' is missing. Please run previous cells (especially Cell 1).\")\n",
    "         missing_data = True\n",
    "         \n",
    "if missing_data:\n",
    "    print(\"Cannot proceed with Cell 14 due to missing data or variables.\")\n",
    "    # Optionally raise an error: raise NameError(\"Prerequisite data/variables missing.\")\n",
    "else:\n",
    "    # --- Step 1: Select ALL Unique Target Eukaryotic Hit SSEQIDs ---\n",
    "    print(\"\\n--- Selecting ALL Unique Eukaryotic Hit IDs from High-Quality Set ---\")\n",
    "    \n",
    "    if euk_hit_sseqid_col not in df_high_quality_hits.columns:\n",
    "         print(f\"ERROR: Column '{euk_hit_sseqid_col}' not found. Cannot get Eukaryotic IDs.\")\n",
    "         euk_ids_to_extract = set()\n",
    "         df_rbh_targets = pd.DataFrame() # Ensure df is defined\n",
    "    else:\n",
    "         # Get unique Euk IDs directly from the high-quality hits df\n",
    "         euk_ids_to_extract = set(df_high_quality_hits[euk_hit_sseqid_col].dropna().astype(str))\n",
    "         print(f\"Found {len(euk_ids_to_extract)} unique Eukaryotic Hit SSEQIDs to extract from {len(df_high_quality_hits)} high-quality hits.\")\n",
    "         # Define the target pairs map (all high-quality hits)\n",
    "         df_rbh_targets = df_high_quality_hits[[protein_id_col, euk_hit_sseqid_col]].copy()\n",
    "\n",
    "\n",
    "    # --- Step 2: Extract Eukaryotic FASTA Sequences ---\n",
    "    if not euk_ids_to_extract:\n",
    "        print(\"\\nNo Eukaryotic IDs selected to extract sequences.\")\n",
    "    elif not euk_fasta_path.is_file():\n",
    "        print(f\"\\nERROR: Eukaryotic FASTA file '{euk_fasta_path}' not found. Cannot extract sequences.\")\n",
    "    else:\n",
    "        print(f\"\\n--- Extracting {len(euk_ids_to_extract)} Eukaryotic FASTA sequences ---\")\n",
    "        print(f\"Reading from: {euk_fasta_path}\")\n",
    "        print(f\"Writing to: {output_euk_hits_fasta}\")\n",
    "        \n",
    "        sequences_written = 0\n",
    "        sequences_found = set() # Keep track of IDs we actually found and wrote\n",
    "        \n",
    "        try:\n",
    "            with open(output_euk_hits_fasta, \"w\") as f_out:\n",
    "                fasta_sequences = SeqIO.parse(euk_fasta_path, \"fasta\")\n",
    "                for record in fasta_sequences:\n",
    "                    # Extract ID from record (handle potential variations)\n",
    "                    current_id = record.id \n",
    "                    # Also check the first part before pipe/space if needed\n",
    "                    first_part_id = record.description.split('|')[0].split(' ')[0].lstrip('>')\n",
    "                    \n",
    "                    # Check if either the direct ID or the first part matches our target list\n",
    "                    id_in_target = None\n",
    "                    if current_id in euk_ids_to_extract:\n",
    "                        id_in_target = current_id\n",
    "                    elif first_part_id in euk_ids_to_extract:\n",
    "                         id_in_target = first_part_id\n",
    "                         # Optional: Log if we had to use the first part ID\n",
    "                         # if current_id != first_part_id: \n",
    "                         #    logging.debug(f\"Using parsed ID '{first_part_id}' instead of SeqIO ID '{current_id}' for matching.\")\n",
    "\n",
    "                    if id_in_target:\n",
    "                        # Use the ID that was in the target list as the key/header\n",
    "                        if id_in_target not in sequences_found: # Avoid writing duplicates\n",
    "                             # Create a new SeqRecord with just the ID we need for the reverse search\n",
    "                             output_record = SeqRecord(record.seq, id=id_in_target, description=\"\") \n",
    "                             SeqIO.write(output_record, f_out, \"fasta\")\n",
    "                             sequences_written += 1\n",
    "                             sequences_found.add(id_in_target)\n",
    "                             \n",
    "                             if sequences_written % 10000 == 0: # Log less frequently for large numbers\n",
    "                                  logging.info(f\"  Written {sequences_written:,} sequences...\")\n",
    "                                 \n",
    "            print(f\"\\nFinished extraction. Wrote {sequences_written} sequences to '{output_euk_hits_fasta}'.\")\n",
    "            \n",
    "            # Check how many target IDs were not found\n",
    "            ids_not_found_set = euk_ids_to_extract - sequences_found\n",
    "            ids_not_found_count = len(ids_not_found_set)\n",
    "            if ids_not_found_count > 0:\n",
    "                 logging.warning(f\"{ids_not_found_count} target Eukaryotic SSEQIDs were not found in the FASTA file.\")\n",
    "                 # Optionally save the list of missing IDs\n",
    "                 missing_ids_file = Path(output_summary_dir_phase1) / \"rbh_euk_ids_not_found_in_fasta.txt\"\n",
    "                 with open(missing_ids_file, 'w') as f_miss:\n",
    "                     for missing_id in sorted(list(ids_not_found_set)):\n",
    "                         f_miss.write(f\"{missing_id}\\n\")\n",
    "                 logging.warning(f\"List of missing IDs saved to '{missing_ids_file}'\")\n",
    "                 \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR during FASTA extraction: {e}\")\n",
    "\n",
    "    # --- Step 3: Save the Asgard-Euk Pair Mapping ---\n",
    "    if not df_rbh_targets.empty:\n",
    "        print(f\"\\n--- Saving Asgard-Euk Pair Mapping for RBH ---\")\n",
    "        try:\n",
    "            # Select only the Asgard ID and its corresponding Euk Hit ID from the HQ set\n",
    "            df_pairs_to_save = df_rbh_targets[[protein_id_col, euk_hit_sseqid_col]].drop_duplicates().dropna()\n",
    "            df_pairs_to_save.to_csv(output_rbh_pairs_map, sep='\\t', index=False, header=True)\n",
    "            print(f\"Saved {len(df_pairs_to_save)} Asgard-Euk pairs to test to '{output_rbh_pairs_map}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR saving pairs map: {e}\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n--- Cell 14 (RBH Preparation - All HQ Hits) Complete ---\")\n",
    "print(\"Next Steps:\")\n",
    "print(\"1. Create a DIAMOND database from your Asgard proteome FASTA file (if not done already).\")\n",
    "print(f\"   Example command: diamond makedb --in <path_to_asgard.fasta> -d asgard_proteome_db\")\n",
    "print(f\"2. Run the reverse DIAMOND search using '{output_euk_hits_fasta}' as query against the Asgard DB.\")\n",
    "print(f\"   Example command: diamond blastp -d asgard_proteome_db.dmnd -q '{output_euk_hits_fasta}' -o reverse_diamond_hits.tsv --outfmt 6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore --sensitive -k 10 --threads <N>\")\n",
    "print(f\"3. Create a new cell (Cell 15) to parse 'reverse_diamond_hits.tsv' and '{output_rbh_pairs_map}' to identify reciprocal best hits.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b754d245-3a6c-457f-89a4-78f39f0d355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Identify Reciprocal Best Hits (RBH)\n",
    "\n",
    "# --- Standard Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import logging # Use logger defined in Cell 1\n",
    "from pathlib import Path \n",
    "import sys # For exit checks\n",
    "\n",
    "print(\"\\n\\n--- Cell 15: Identify Reciprocal Best Hits (RBH) ---\")\n",
    "\n",
    "# --- Assumptions ---\n",
    "# This cell assumes Cell 1 and Cell 2 have been run successfully.\n",
    "# It assumes the reverse DIAMOND search was run externally, producing 'reverse_diamond_hits.tsv'.\n",
    "# It relies on 'df_full' (with coverage & annotations, saved as v2.1 in Cell 2) being available.\n",
    "# It uses the mapping file 'asgard_euk_hq_rbh_pairs_to_test.tsv' generated in Cell 14.\n",
    "# It relies on column name variables defined in Cell 1.\n",
    "\n",
    "# --- Configuration ---\n",
    "# Input: Path to the database file saved at the end of Cell 2 (with coverage)\n",
    "db_with_coverage_path = output_db_with_coverage_path # Defined in Cell 1, points to v2.1\n",
    "\n",
    "# Input: Mapping file of Asgard -> Euk pairs tested (from Cell 14)\n",
    "forward_pairs_map_path = output_rbh_pairs_map # Defined in Cell 14\n",
    "\n",
    "# Input: Results from the reverse DIAMOND search (Euk query -> Asgard DB)\n",
    "reverse_diamond_results_path = Path(output_summary_dir_phase1) / \"reverse_diamond_hits.tsv\"\n",
    "\n",
    "# Output: Database updated with an 'Is_RBH' column\n",
    "output_db_with_rbh_path = Path(output_summary_dir_phase1) / 'proteome_database_combined_v2.2_rbh.csv'\n",
    "# Output: List of confirmed RBH pairs\n",
    "output_rbh_confirmed_pairs_path = Path(output_summary_dir_phase1) / \"asgard_euk_rbh_confirmed_pairs.tsv\"\n",
    "\n",
    "# E-value threshold for filtering reverse hits (can be same or different from forward)\n",
    "reverse_e_value_threshold = 1e-10 \n",
    "\n",
    "# DIAMOND output columns for reverse search (should match the --outfmt 6 used)\n",
    "reverse_diamond_col_names = [\n",
    "    'qseqid', 'sseqid', 'pident', 'length', 'mismatch', 'gapopen',\n",
    "    'qstart', 'qend', 'sstart', 'send', 'evalue', 'bitscore' \n",
    "    # Add qlen, slen if they were included in your reverse search output format\n",
    "]\n",
    "# Rename columns for clarity after parsing reverse hits\n",
    "reverse_col_rename = {\n",
    "    'qseqid': euk_hit_sseqid_col, # Query in reverse search is the Euk Hit SSEQID\n",
    "    'sseqid': 'Reverse_Hit_Asgard_ID', # Subject in reverse search is the Asgard ProteinID\n",
    "    'evalue': 'Reverse_Hit_Evalue',\n",
    "    'bitscore': 'Reverse_Hit_Bitscore'\n",
    "    # Add others if needed\n",
    "}\n",
    "rbh_flag_col = 'Is_RBH' # Name for the new boolean column\n",
    "\n",
    "# --- Check if necessary Files/DataFrames exist ---\n",
    "if 'df_full' not in locals() or df_full.empty:\n",
    "     # Attempt to load from the file saved in Cell 2\n",
    "     if output_db_with_coverage_path.is_file():\n",
    "          print(f\"Loading database with coverage from: {output_db_with_coverage_path}\")\n",
    "          try:\n",
    "              df_full = pd.read_csv(output_db_with_coverage_path, low_memory=False)\n",
    "              # Ensure ProteinID is string\n",
    "              if protein_id_col in df_full.columns:\n",
    "                   df_full[protein_id_col] = df_full[protein_id_col].astype(str)\n",
    "              print(f\"Loaded df_full. Shape: {df_full.shape}\")\n",
    "          except Exception as e:\n",
    "              print(f\"ERROR: Failed to load database from {output_db_with_coverage_path}. Error: {e}\")\n",
    "              df_full = pd.DataFrame() # Ensure it's defined as empty\n",
    "     else:\n",
    "          print(f\"ERROR: DataFrame 'df_full' not found in memory and file '{output_db_with_coverage_path}' not found. Please run Cell 2.\")\n",
    "          df_full = pd.DataFrame()\n",
    "\n",
    "if not forward_pairs_map_path.is_file():\n",
    "    print(f\"ERROR: Forward pairs map file not found: '{forward_pairs_map_path}'. Please run Cell 14.\")\n",
    "    # Cannot proceed without this map\n",
    "    sys.exit(1) \n",
    "\n",
    "if not reverse_diamond_results_path.is_file():\n",
    "    print(f\"ERROR: Reverse DIAMOND results file not found: '{reverse_diamond_results_path}'. Please ensure the external DIAMOND search completed.\")\n",
    "    # Cannot proceed without reverse hits\n",
    "    sys.exit(1) \n",
    "\n",
    "# --- Step 1: Load Forward Hit Pairs ---\n",
    "print(f\"\\n--- Loading Forward Asgard -> Euk Hit Pairs ---\")\n",
    "try:\n",
    "    df_forward_pairs = pd.read_csv(forward_pairs_map_path, sep='\\t')\n",
    "    # Ensure columns are string type for matching\n",
    "    df_forward_pairs[protein_id_col] = df_forward_pairs[protein_id_col].astype(str)\n",
    "    df_forward_pairs[euk_hit_sseqid_col] = df_forward_pairs[euk_hit_sseqid_col].astype(str)\n",
    "    # Keep only the first mapping if an Asgard protein hit multiple Euks in the HQ set (unlikely with best hit logic)\n",
    "    df_forward_pairs = df_forward_pairs.drop_duplicates(subset=[protein_id_col], keep='first')\n",
    "    print(f\"Loaded {len(df_forward_pairs)} unique Asgard -> Euk pairs to check for RBH.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading forward pairs map: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# --- Step 2: Parse Reverse DIAMOND Hits ---\n",
    "print(f\"\\n--- Parsing Reverse Euk -> Asgard DIAMOND Hits ---\")\n",
    "best_reverse_hits = {} # Euk_ID -> Best_Asgard_Hit_ID\n",
    "\n",
    "try:\n",
    "    # Read the raw reverse DIAMOND output\n",
    "    df_reverse_raw = pd.read_csv(reverse_diamond_results_path, sep='\\t', header=None, names=reverse_diamond_col_names)\n",
    "    logging.info(f\"  Read {len(df_reverse_raw)} total reverse DIAMOND alignments.\")\n",
    "\n",
    "    # Filter by e-value\n",
    "    df_reverse_filtered = df_reverse_raw[df_reverse_raw['evalue'] <= reverse_e_value_threshold].copy()\n",
    "    logging.info(f\"  Found {len(df_reverse_filtered)} reverse alignments passing e-value threshold <= {reverse_e_value_threshold}.\")\n",
    "\n",
    "    if not df_reverse_filtered.empty:\n",
    "        # Ensure IDs are strings\n",
    "        df_reverse_filtered['qseqid'] = df_reverse_filtered['qseqid'].astype(str)\n",
    "        df_reverse_filtered['sseqid'] = df_reverse_filtered['sseqid'].astype(str)\n",
    "        \n",
    "        # Sort by Euk query ID (qseqid), then by e-value/bitscore to get the best Asgard hit (sseqid)\n",
    "        df_reverse_filtered.sort_values(by=['qseqid', 'evalue', 'bitscore'], ascending=[True, True, False], inplace=True)\n",
    "        \n",
    "        # Keep only the best Asgard hit per Eukaryotic query\n",
    "        df_best_reverse_hits = df_reverse_filtered.drop_duplicates(subset=['qseqid'], keep='first')\n",
    "        logging.info(f\"  Identified {len(df_best_reverse_hits)} unique Eukaryotic queries with a best Asgard hit.\")\n",
    "\n",
    "        # Create the reverse mapping dictionary\n",
    "        best_reverse_hits = pd.Series(\n",
    "            df_best_reverse_hits['sseqid'].values, \n",
    "            index=df_best_reverse_hits['qseqid']\n",
    "        ).to_dict()\n",
    "        \n",
    "        # Diagnostic: Print a few reverse mappings\n",
    "        print(\"  Example Reverse Best Hits (Euk ID -> Best Asgard Hit ID):\")\n",
    "        count = 0\n",
    "        for k, v in best_reverse_hits.items():\n",
    "            print(f\"    '{k}' -> '{v}'\")\n",
    "            count += 1\n",
    "            if count >= 5: break\n",
    "            \n",
    "    else:\n",
    "        print(\"  No reverse DIAMOND hits passed the e-value threshold.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Reverse DIAMOND results file not found at '{reverse_diamond_results_path}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while parsing reverse DIAMOND results: {e}\")\n",
    "    # Continue without reverse hits if parsing fails, RBH will be False\n",
    "\n",
    "# --- Step 3: Identify RBH Pairs ---\n",
    "print(\"\\n--- Identifying Reciprocal Best Hit Pairs ---\")\n",
    "rbh_pairs_list = []\n",
    "rbh_count = 0\n",
    "\n",
    "if not df_forward_pairs.empty and best_reverse_hits:\n",
    "    for _, row in df_forward_pairs.iterrows():\n",
    "        asgard_id = row[protein_id_col]\n",
    "        euk_id = row[euk_hit_sseqid_col]\n",
    "        \n",
    "        # Check if the Euk ID has a best reverse hit recorded\n",
    "        best_reverse_asgard_hit = best_reverse_hits.get(euk_id)\n",
    "        \n",
    "        # Check if the best reverse hit matches the original Asgard ID\n",
    "        if best_reverse_asgard_hit == asgard_id:\n",
    "            rbh_pairs_list.append({protein_id_col: asgard_id, euk_hit_sseqid_col: euk_id})\n",
    "            rbh_count += 1\n",
    "            \n",
    "    print(f\"Identified {rbh_count} Reciprocal Best Hit (RBH) pairs.\")\n",
    "    \n",
    "    # Create DataFrame of confirmed RBHs\n",
    "    if rbh_pairs_list:\n",
    "        df_rbh_confirmed = pd.DataFrame(rbh_pairs_list)\n",
    "        # Save the confirmed RBH pairs\n",
    "        try:\n",
    "            df_rbh_confirmed.to_csv(output_rbh_confirmed_pairs_path, sep='\\t', index=False)\n",
    "            print(f\"Saved {len(df_rbh_confirmed)} confirmed RBH pairs to '{output_rbh_confirmed_pairs_path}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR saving confirmed RBH pairs: {e}\")\n",
    "    else:\n",
    "        df_rbh_confirmed = pd.DataFrame(columns=[protein_id_col, euk_hit_sseqid_col]) # Empty df\n",
    "        \n",
    "else:\n",
    "    print(\"Skipping RBH identification: Missing forward pairs or reverse hits.\")\n",
    "    df_rbh_confirmed = pd.DataFrame(columns=[protein_id_col, euk_hit_sseqid_col]) # Empty df\n",
    "\n",
    "# --- Step 4: Add RBH Flag to Main DataFrame ---\n",
    "if not df_full.empty:\n",
    "    print(f\"\\n--- Adding '{rbh_flag_col}' column to main DataFrame ---\")\n",
    "    \n",
    "    # Create a set of Asgard IDs that are part of an RBH pair for quick lookup\n",
    "    rbh_asgard_ids = set(df_rbh_confirmed[protein_id_col])\n",
    "    \n",
    "    # Add the boolean column\n",
    "    if protein_id_col in df_full.columns:\n",
    "        df_full[rbh_flag_col] = df_full[protein_id_col].isin(rbh_asgard_ids)\n",
    "        print(f\"Flagged {df_full[rbh_flag_col].sum()} proteins as RBH.\")\n",
    "    else:\n",
    "        print(f\"ERROR: Cannot add RBH flag, '{protein_id_col}' not in df_full.\")\n",
    "        # Add empty column if needed for consistency\n",
    "        if rbh_flag_col not in df_full.columns: df_full[rbh_flag_col] = False\n",
    "\n",
    "else:\n",
    "    print(\"Skipping addition of RBH flag as df_full is not available.\")\n",
    "\n",
    "\n",
    "# --- Step 5: Save Final Updated Database ---\n",
    "if not df_full.empty and rbh_flag_col in df_full.columns:\n",
    "    # This df_full now contains original v2.0 info + coverage + RBH flag\n",
    "    final_output_db_path = output_db_with_rbh_path # Use path defined earlier\n",
    "    print(f\"\\n--- Saving Final Updated Database (with RBH flag) ---\")\n",
    "    print(f\"Saving to: {final_output_db_path}\")\n",
    "    try:\n",
    "        # Ensure output directory exists\n",
    "        final_output_db_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df_full.to_csv(final_output_db_path, index=False)\n",
    "        logging.info(f\"Successfully saved final updated database to '{final_output_db_path}'.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save final updated database: {e}\")\n",
    "else:\n",
    "     print(\"\\nSkipping final save: df_full empty or RBH flag not added.\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n--- Cell 15 (RBH Analysis) Complete ---\")\n",
    "print(f\"Identified {rbh_count} RBH pairs.\")\n",
    "print(f\"The main DataFrame 'df_full' now includes the '{rbh_flag_col}' column.\")\n",
    "print(f\"The final database is saved as '{output_db_with_rbh_path}'.\")\n",
    "print(\"Next steps involve analyzing the RBH pairs (e.g., their functions, domains) and potentially performing MSA/Phylogeny on them.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31194098-3677-4c65-bd5c-5099839dc460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Prepare Input FASTA Files for Intra-Orthogroup MAFFT MSA (Size >= 5)\n",
    "\n",
    "# --- Standard Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import logging # Use logger defined in Cell 1\n",
    "from pathlib import Path \n",
    "import sys \n",
    "import shutil # For copying files\n",
    "# Ensure Biopython is installed: pip install biopython\n",
    "try:\n",
    "    from Bio import SeqIO \n",
    "    from Bio.SeqRecord import SeqRecord\n",
    "    from Bio.Seq import Seq\n",
    "except ImportError:\n",
    "    print(\"ERROR: Biopython is required for this script. Please install it: pip install biopython\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"\\n\\n--- Cell 16: Prepare Input FASTA Files for Intra-Orthogroup MAFFT MSA (Size >= 5) ---\")\n",
    "\n",
    "# --- Assumptions ---\n",
    "# This cell assumes Cell 1 (Setup) has been run.\n",
    "# It relies on the existence of directories containing per-OG FASTA files.\n",
    "# It relies on output directories defined in Cell 1.\n",
    "\n",
    "# --- Configuration ---\n",
    "# *** UPDATED Input Directory based on user screenshot ***\n",
    "og_fasta_input_dir = Path('diamond_run_all_ogs/all_ogs_standardized_fastas') \n",
    "\n",
    "# Output: Directory to store the selected multi-sequence FASTA files for MAFFT input\n",
    "# Ensure output_summary_dir_phase1 is defined in Cell 1\n",
    "if 'output_summary_dir_phase1' not in locals():\n",
    "     print(\"ERROR: output_summary_dir_phase1 not defined. Please run Cell 1.\")\n",
    "     # Define a default or raise error\n",
    "     output_summary_dir_phase1 = Path(\"./output_summary_data_hit_validation_phase1\") # Default if not set\n",
    "     \n",
    "mafft_input_intra_og_dir = Path(output_summary_dir_phase1) / \"mafft_input_intra_og\"\n",
    "\n",
    "# Minimum number of sequences required in an OG FASTA file to be included\n",
    "min_og_size_for_msa = 5 \n",
    "\n",
    "# --- Check if necessary Variables/Files exist ---\n",
    "required_variables = ['output_summary_dir_phase1']\n",
    "missing_data = False\n",
    "for var_name in required_variables:\n",
    "     if var_name not in locals():\n",
    "         print(f\"ERROR: Required variable '{var_name}' is missing. Please run Cell 1 first.\")\n",
    "         missing_data = True\n",
    "if not og_fasta_input_dir.is_dir():\n",
    "     print(f\"ERROR: Input OG FASTA directory not found: '{og_fasta_input_dir}'\")\n",
    "     missing_data = True\n",
    "     \n",
    "if missing_data:\n",
    "    print(\"Cannot proceed with Cell 16 due to missing directories or variables.\")\n",
    "    # sys.exit(1) # Or handle differently\n",
    "else:\n",
    "    # --- Step 1: Identify and Copy Target OG FASTA Files (Size >= 5) ---\n",
    "    print(f\"\\n--- Identifying and Copying OG FASTA files (Size >= {min_og_size_for_msa}) ---\")\n",
    "    mafft_input_intra_og_dir.mkdir(parents=True, exist_ok=True) # Create output dir\n",
    "    \n",
    "    copied_count = 0\n",
    "    skipped_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    # Process ALL FASTA files in the input directory\n",
    "    print(f\"\\nProcessing FASTA files from: {og_fasta_input_dir}\")\n",
    "    # Use a pattern that matches both .ASG.fasta and .GV.fasta\n",
    "    all_og_fasta_files = list(og_fasta_input_dir.glob('*.fasta')) \n",
    "    print(f\"Found {len(all_og_fasta_files)} total OG FASTA files.\")\n",
    "    \n",
    "    for fasta_file in all_og_fasta_files:\n",
    "        try:\n",
    "            # Count sequences in the file\n",
    "            seq_count = 0\n",
    "            with open(fasta_file, \"r\") as handle:\n",
    "                for record in SeqIO.parse(handle, \"fasta\"):\n",
    "                    seq_count += 1\n",
    "            \n",
    "            # Check size and copy if sufficient\n",
    "            if seq_count >= min_og_size_for_msa:\n",
    "                destination_file = mafft_input_intra_og_dir / fasta_file.name\n",
    "                shutil.copy2(fasta_file, destination_file) # copy2 preserves metadata\n",
    "                copied_count += 1\n",
    "                if copied_count % 500 == 0:\n",
    "                     logging.info(f\"  Copied {copied_count} qualifying OG FASTA files...\")\n",
    "            else:\n",
    "                skipped_count += 1\n",
    "                logging.debug(f\"Skipping {fasta_file.name}: Size ({seq_count}) < {min_og_size_for_msa}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing file {fasta_file.name}: {e}\")\n",
    "            error_count += 1\n",
    "\n",
    "    print(f\"\\nFinished selecting and copying OG FASTA files.\")\n",
    "    print(f\"  Total FASTA files copied (Size >= {min_og_size_for_msa}): {copied_count}\")\n",
    "    print(f\"  Total FASTA files skipped (Size < {min_og_size_for_msa}): {skipped_count}\")\n",
    "    print(f\"  Errors during processing: {error_count}\")\n",
    "    print(f\"  Input files for MAFFT are located in: '{mafft_input_intra_og_dir}'\")\n",
    "\n",
    "    # --- Step 2: Provide Instructions for MAFFT Script ---\n",
    "    print(\"\\n--- Instructions for Running MAFFT on Intra-OG files ---\")\n",
    "    print(\"You can now use your 'run_mafft_parallel.py' script on the generated OG files.\")\n",
    "    print(\"Example command (adjust paths and parameters as needed):\")\n",
    "    print(\"\\npython run_mafft_parallel.py \\\\\")\n",
    "    print(f\"    --input_dir {mafft_input_intra_og_dir} \\\\\")\n",
    "    print(f\"    --output_dir {mafft_input_intra_og_dir}_aligned \\\\\") \n",
    "    print(f\"    --log_dir {mafft_input_intra_og_dir}_logs \\\\\")      \n",
    "    print(f\"    --input_suffix .fasta \\\\\") # Assuming input files end with .fasta              \n",
    "    print(f\"    --output_suffix .aln \\\\\")                 \n",
    "    print(f\"    --mafft_exe mafft \\\\\")                   \n",
    "    print(f\"    --mafft_args '--auto --thread 1' \\\\\") # Use --auto or more specific MAFFT strategy\n",
    "    print(f\"    --num_cores 8\") # Adjust core count                         \n",
    "    print(\"\\nEnsure the output and log directories are created before running.\")\n",
    "    print(\"After running MAFFT, proceed to the TrimAl step.\")\n",
    "\n",
    "print(\"\\n\\n--- Cell 16 (Prepare Input for Intra-OG MAFFT MSA) Complete ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334dfea5-0872-48d3-8c9a-badc2ba94dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Calculate Intra-Orthogroup Average Pairwise Sequence Identity (APSI)\n",
    "\n",
    "# --- Standard Library Imports ---\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import logging # Use logger defined in Cell 1\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Ensure Biopython is installed and imported\n",
    "try:\n",
    "    from Bio import AlignIO\n",
    "    from Bio.Align import MultipleSeqAlignment\n",
    "except ImportError:\n",
    "    logger.error(\"ERROR: Biopython is required for this script. Please install it: pip install biopython\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"\\n\\n--- Cell 17: Calculate Intra-Orthogroup Average Pairwise Sequence Identity (APSI) ---\")\n",
    "\n",
    "# --- Assumptions ---\n",
    "# This cell assumes Cell 1 (Setup) has been run successfully to define variables like output_summary_dir_phase1.\n",
    "# It assumes the TrimAl step was run successfully, producing trimmed FASTA files.\n",
    "\n",
    "# --- Configuration ---\n",
    "# Input: Directory containing the trimmed alignment files (FASTA format)\n",
    "# This should be the output directory from your TrimAl script\n",
    "# Make sure this path matches where your trimmed files were saved!\n",
    "# Example: If your TrimAl script output to 'trimal_output_intra_og_gappyout'\n",
    "# trimmed_alignments_dir = Path('./trimal_output_intra_og_gappyout') # <-- Update this path\n",
    "# Assuming the trimmed files end with '.trimmed.fasta' as per the refined script suggestion\n",
    "trimmed_alignments_dir = Path('./intra_og_analysis/trimal_output_intra_og') # Assuming default output dir from refined script\n",
    "trimmed_file_suffix = '*_trimmed.fasta' # Corrected: Removed selection tags. This pattern matches files ending with '_trimmed.fasta'\n",
    "\n",
    "# Output: File to save the calculated APSI values\n",
    "# Ensure output_summary_dir_phase1 is defined and is a Path object\n",
    "if 'output_summary_dir_phase1' not in locals():\n",
    "     logger.error(\"ERROR: output_summary_dir_phase1 not defined. Please run Cell 1.\")\n",
    "     # Define a default or exit\n",
    "     output_summary_dir_phase1 = Path(\"./output_summary_data_hit_validation_phase1\") # Default if not set\n",
    "     # Ensure default directory exists\n",
    "     output_summary_dir_phase1.mkdir(parents=True, exist_ok=True)\n",
    "elif not isinstance(output_summary_dir_phase1, Path):\n",
    "    # Defensive check: if the variable exists but isn't a Path, try to convert it\n",
    "    logger.warning(f\"output_summary_dir_phase1 is not a Path object (it's {type(output_summary_dir_phase1)}). Attempting to convert.\")\n",
    "    try:\n",
    "        output_summary_dir_phase1 = Path(str(output_summary_dir_phase1))\n",
    "        output_summary_dir_phase1.mkdir(parents=True, exist_ok=True) # Ensure directory exists after conversion\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to convert output_summary_dir_phase1 to Path: {e}. Please ensure Cell 1 runs correctly.\")\n",
    "        sys.exit(1) # Exit if conversion fails\n",
    "\n",
    "# Now use the Path object for joining\n",
    "apsi_output_file = output_summary_dir_phase1 / \"intra_og_apsi_values.csv\"\n",
    "\n",
    "# --- Helper Function to Calculate Pairwise Identity ---\n",
    "def calculate_pairwise_identity(seq1, seq2):\n",
    "    \"\"\" Calculates identity between two sequences, ignoring positions with gaps in either sequence. \"\"\"\n",
    "    if len(seq1) != len(seq2):\n",
    "        logger.warning(\"Sequence lengths do not match for pairwise identity calculation.\")\n",
    "        return 0.0\n",
    "\n",
    "    identical_residues = 0\n",
    "    aligned_length = 0 # Count positions where neither sequence has a gap\n",
    "\n",
    "    for i in range(len(seq1)):\n",
    "        res1 = seq1[i]\n",
    "        res2 = seq2[i]\n",
    "\n",
    "        if res1 != '-' and res2 != '-':\n",
    "            aligned_length += 1\n",
    "            if res1 == res2:\n",
    "                identical_residues += 1\n",
    "\n",
    "    # Avoid division by zero if no aligned positions\n",
    "    return (identical_residues / aligned_length) if aligned_length > 0 else 0.0\n",
    "\n",
    "# --- Main APSI Calculation Logic ---\n",
    "if not trimmed_alignments_dir.is_dir():\n",
    "    logger.error(f\"Input trimmed alignment directory not found: '{trimmed_alignments_dir}'\")\n",
    "    print(\"Cannot calculate APSI. Please check the 'trimmed_alignments_dir' path.\")\n",
    "else:\n",
    "    print(f\"Reading trimmed alignments from: {trimmed_alignments_dir}\")\n",
    "    print(f\"Saving APSI results to: {apsi_output_file}\")\n",
    "\n",
    "    # Use glob with the corrected suffix pattern\n",
    "    all_trimmed_files = list(trimmed_alignments_dir.glob(f\"{trimmed_file_suffix}\"))\n",
    "\n",
    "    if not all_trimmed_files:\n",
    "        logger.warning(f\"No trimmed alignment files found matching pattern '{trimmed_file_suffix}' in '{trimmed_alignments_dir}'.\")\n",
    "        print(\"No alignments to calculate APSI for.\")\n",
    "    else:\n",
    "        print(f\"Found {len(all_trimmed_files)} trimmed alignment files.\")\n",
    "        apsi_results = []\n",
    "        processed_count = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for alignment_file in all_trimmed_files:\n",
    "            og_id = alignment_file.stem # Get filename without suffix\n",
    "            # Adjust og_id extraction based on actual filename format if needed\n",
    "            # For example, if files are OG0000000_trimmed.fasta, stem is 'OG0000000_trimmed'\n",
    "            # We want 'OG0000000'\n",
    "            if og_id.endswith('_trimmed'):\n",
    "                 og_id = og_id[:-len('_trimmed')]\n",
    "\n",
    "\n",
    "            try:\n",
    "                # Read the alignment\n",
    "                alignment = AlignIO.read(alignment_file, \"fasta\")\n",
    "                sequences = [str(record.seq) for record in alignment]\n",
    "                num_sequences = len(sequences)\n",
    "\n",
    "                if num_sequences < 2:\n",
    "                    # Should not happen if input was filtered for size >= 5, but handle defensively\n",
    "                    logger.warning(f\"Alignment for {og_id} has less than 2 sequences ({num_sequences}). Cannot calculate APSI.\")\n",
    "                    apsi_results.append({'Orthogroup': og_id, 'APSI': None, 'Num_Sequences': num_sequences})\n",
    "                    processed_count += 1\n",
    "                    continue\n",
    "\n",
    "                # Calculate pairwise identities for all pairs\n",
    "                pairwise_identities = []\n",
    "                for i in range(num_sequences):\n",
    "                    for j in range(i + 1, num_sequences):\n",
    "                        identity = calculate_pairwise_identity(sequences[i], sequences[j])\n",
    "                        pairwise_identities.append(identity)\n",
    "\n",
    "                # Calculate Average Pairwise Sequence Identity\n",
    "                average_identity = sum(pairwise_identities) / len(pairwise_identities) if pairwise_identities else 0.0\n",
    "\n",
    "                apsi_results.append({\n",
    "                    'Orthogroup': og_id,\n",
    "                    'APSI': average_identity,\n",
    "                    'Num_Sequences': num_sequences\n",
    "                })\n",
    "\n",
    "                processed_count += 1\n",
    "                if processed_count % 100 == 0:\n",
    "                    logger.info(f\"Processed {processed_count}/{len(all_trimmed_files)} alignments for APSI...\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing alignment file {alignment_file}: {e}\")\n",
    "                apsi_results.append({'Orthogroup': og_id, 'APSI': None, 'Num_Sequences': None, 'Error': str(e)})\n",
    "                processed_count += 1\n",
    "\n",
    "\n",
    "        # Save the results to a DataFrame and then to CSV\n",
    "        if apsi_results:\n",
    "            df_apsi = pd.DataFrame(apsi_results)\n",
    "            try:\n",
    "                df_apsi.to_csv(apsi_output_file, index=False)\n",
    "                print(f\"\\nSuccessfully calculated and saved APSI for {len(df_apsi)} orthogroups to '{apsi_output_file}'.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to save APSI results to '{apsi_output_file}': {e}\")\n",
    "        else:\n",
    "            print(\"\\nNo APSI results were generated.\")\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(f\"APSI calculation completed in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n--- Cell 17 (APSI Calculation) Complete ---\")\n",
    "print(\"The APSI values are saved and ready to be merged into your main database.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d19408-7168-4c5a-ade1-5144332c8c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: Identify Conserved Motifs in Trimmed Alignments\n",
    "\n",
    "# --- Standard Library Imports ---\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import logging # Use logger defined in Cell 1\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "# Ensure Biopython is installed and imported\n",
    "try:\n",
    "    from Bio import AlignIO\n",
    "    from Bio.Align import MultipleSeqAlignment\n",
    "except ImportError:\n",
    "    logger.error(\"ERROR: Biopython is required for this script. Please install it: pip install biopython\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"\\n\\n--- Cell 18: Identify Conserved Motifs in Trimmed Alignments ---\")\n",
    "\n",
    "# --- Assumptions ---\n",
    "# This cell assumes Cell 1 (Setup) has been run.\n",
    "# It assumes the TrimAl step was run successfully, producing trimmed FASTA files.\n",
    "# It relies on the output directory variable (e.g., output_summary_dir_phase1) defined in Cell 1.\n",
    "\n",
    "# --- Configuration ---\n",
    "# Input: Directory containing the trimmed alignment files (FASTA format)\n",
    "# This should be the output directory from your TrimAl script\n",
    "# Make sure this path matches where your trimmed files were saved!\n",
    "# Example: If your TrimAl script output to 'trimal_output_intra_og_gappyout'\n",
    "# trimmed_alignments_dir = Path('./trimal_output_intra_og_gappyout') # <-- Update this path\n",
    "# Assuming the trimmed files end with '.trimmed.fasta' as per the refined script suggestion\n",
    "trimmed_alignments_dir = Path('intra_og_analysis/trimal_output_intra_og') # Assuming default output dir from refined script\n",
    "trimmed_file_suffix = '_trimmed.fasta' # Assuming output suffix from refined script\n",
    "\n",
    "# Output: File to save the identified conserved motifs\n",
    "if 'output_summary_dir_phase1' not in locals():\n",
    "     logger.error(\"ERROR: output_summary_dir_phase1 not defined. Please run Cell 1.\")\n",
    "     # Define a default or exit\n",
    "     output_summary_dir_phase1 = Path(\"./output_summary_data_hit_validation_phase1\") # Default if not set\n",
    "     output_summary_dir_phase1.mkdir(parents=True, exist_ok=True) # Ensure it exists\n",
    "\n",
    "motifs_output_file = output_summary_dir_phase1 / \"intra_og_conserved_motifs.csv\"\n",
    "\n",
    "# Motif finding parameters\n",
    "min_motif_length = 4 # Minimum length of a conserved motif\n",
    "min_conservation_percentage = 90 # Minimum percentage of identical residues in a column for it to be considered conserved (e.g., 90 for 90%)\n",
    "\n",
    "# --- Helper Function to Check Column Conservation ---\n",
    "def is_column_conserved(column, min_conservation_percentage):\n",
    "    \"\"\" Checks if a column in an alignment is conserved above a threshold, ignoring gaps. \"\"\"\n",
    "    # Remove gaps from the column\n",
    "    nongap_residues = [res for res in column if res != '-']\n",
    "    num_nongap = len(nongap_residues)\n",
    "\n",
    "    if num_nongap == 0:\n",
    "        return False, None # Cannot determine conservation if only gaps\n",
    "\n",
    "    # Count frequency of each residue\n",
    "    residue_counts = Counter(nongap_residues)\n",
    "    most_common_residue, count = residue_counts.most_common(1)[0]\n",
    "\n",
    "    # Calculate conservation percentage\n",
    "    conservation = (count / num_nongap) * 100\n",
    "\n",
    "    return conservation >= min_conservation_percentage, most_common_residue\n",
    "\n",
    "# --- Main Motif Identification Logic ---\n",
    "if not trimmed_alignments_dir.is_dir():\n",
    "    logger.error(f\"Input trimmed alignment directory not found: '{trimmed_alignments_dir}'\")\n",
    "    print(\"Cannot identify conserved motifs. Please check the 'trimmed_alignments_dir' path.\")\n",
    "else:\n",
    "    print(f\"Reading trimmed alignments from: {trimmed_alignments_dir}\")\n",
    "    print(f\"Saving motif results to: {motifs_output_file}\")\n",
    "    print(f\"Motif parameters: Min Length = {min_motif_length}, Min Conservation = {min_conservation_percentage}%\")\n",
    "\n",
    "    all_trimmed_files = list(trimmed_alignments_dir.glob(f\"*{trimmed_file_suffix}\"))\n",
    "\n",
    "    if not all_trimmed_files:\n",
    "        logger.warning(f\"No trimmed alignment files found with suffix '{trimmed_file_suffix}' in '{trimmed_alignments_dir}'.\")\n",
    "        print(\"No alignments to identify motifs in.\")\n",
    "    else:\n",
    "        print(f\"Found {len(all_trimmed_files)} trimmed alignment files.\")\n",
    "        conserved_motifs_results = []\n",
    "        processed_count = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for alignment_file in all_trimmed_files:\n",
    "            og_id = alignment_file.stem # Get filename without suffix\n",
    "            if og_id.endswith('_trimmed'): # Remove the '_trimmed' part added by TrimAl script\n",
    "                 og_id = og_id[:-len('_trimmed')]\n",
    "\n",
    "            try:\n",
    "                # Read the alignment\n",
    "                alignment = AlignIO.read(alignment_file, \"fasta\")\n",
    "                alignment_length = alignment.get_alignment_length()\n",
    "                num_sequences = len(alignment)\n",
    "\n",
    "                if num_sequences == 0 or alignment_length == 0:\n",
    "                     logger.warning(f\"Alignment for {og_id} is empty or has zero length. Skipping motif search.\")\n",
    "                     processed_count += 1\n",
    "                     continue\n",
    "\n",
    "                # Iterate through columns to find conserved regions\n",
    "                current_motif = \"\"\n",
    "                motif_start_col = -1\n",
    "\n",
    "                for i in range(alignment_length):\n",
    "                    column = alignment[:, i] # Get the i-th column\n",
    "                    is_cons, conserved_residue = is_column_conserved(column, min_conservation_percentage)\n",
    "\n",
    "                    if is_cons:\n",
    "                        if motif_start_col == -1:\n",
    "                            motif_start_col = i # Start of a potential motif\n",
    "                        current_motif += conserved_residue # Append the conserved residue\n",
    "                    else:\n",
    "                        # End of a conserved stretch\n",
    "                        if motif_start_col != -1:\n",
    "                            # Check if the collected motif meets the minimum length\n",
    "                            if len(current_motif) >= min_motif_length:\n",
    "                                conserved_motifs_results.append({\n",
    "                                    'Orthogroup': og_id,\n",
    "                                    'Motif': current_motif,\n",
    "                                    'Start_Column': motif_start_col,\n",
    "                                    'End_Column': i - 1, # End column is the one *before* the break\n",
    "                                    'Alignment_Length': alignment_length,\n",
    "                                    'Num_Sequences': num_sequences\n",
    "                                })\n",
    "                            # Reset for the next potential motif\n",
    "                            current_motif = \"\"\n",
    "                            motif_start_col = -1\n",
    "\n",
    "                # After the loop, check if the last stretch is a motif\n",
    "                if motif_start_col != -1 and len(current_motif) >= min_motif_length:\n",
    "                     conserved_motifs_results.append({\n",
    "                        'Orthogroup': og_id,\n",
    "                        'Motif': current_motif,\n",
    "                        'Start_Column': motif_start_col,\n",
    "                        'End_Column': alignment_length - 1, # Last column\n",
    "                        'Alignment_Length': alignment_length,\n",
    "                        'Num_Sequences': num_sequences\n",
    "                    })\n",
    "\n",
    "\n",
    "                processed_count += 1\n",
    "                if processed_count % 100 == 0:\n",
    "                    logger.info(f\"Processed {processed_count}/{len(all_trimmed_files)} alignments for motifs...\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing alignment file {alignment_file} for motifs: {e}\")\n",
    "                # Optionally log the OG ID with the error\n",
    "                processed_count += 1\n",
    "\n",
    "\n",
    "        # Save the results to a DataFrame and then to CSV\n",
    "        if conserved_motifs_results:\n",
    "            df_motifs = pd.DataFrame(conserved_motifs_results)\n",
    "            try:\n",
    "                df_motifs.to_csv(motifs_output_file, index=False)\n",
    "                print(f\"\\nSuccessfully identified and saved conserved motifs for {len(set(df_motifs['Orthogroup']))} orthogroups to '{motifs_output_file}'.\")\n",
    "                print(f\"Total {len(df_motifs)} motifs identified across all orthogroups.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to save motif results to '{motifs_output_file}': {e}\")\n",
    "        else:\n",
    "            print(\"\\nNo conserved motifs were identified based on the specified parameters.\")\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(f\"Motif identification completed in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n--- Cell 18 (Conserved Motif Identification) Complete ---\")\n",
    "print(\"The identified conserved motifs are saved and ready for analysis and integration.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3760f36e-8a0e-4e8b-9dee-c81347915914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell Y: Check Output Directory Path and Existence\n",
    "\n",
    "print(\"\\n\\n--- Cell Y: Checking Output Directory ---\")\n",
    "\n",
    "# This assumes 'output_summary_dir_phase1' is defined in a previous cell (e.g., Cell 1)\n",
    "# and 'Path' is imported (also in Cell 1).\n",
    "\n",
    "if 'output_summary_dir_phase1' in locals() and 'Path' in locals():\n",
    "    output_dir_path = Path(output_summary_dir_phase1)\n",
    "\n",
    "    print(f\"Configured output directory path: {output_dir_path}\")\n",
    "\n",
    "    # Check if the directory exists\n",
    "    if output_dir_path.is_dir():\n",
    "        print(\"Directory exists.\")\n",
    "    else:\n",
    "        print(\"Directory DOES NOT exist. Attempting to create it...\")\n",
    "        try:\n",
    "            output_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "            print(\"Directory created successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Failed to create directory. Please check permissions and the path: {e}\")\n",
    "            output_dir_path = None # Set to None to indicate failure\n",
    "\n",
    "    # Check write permissions (basic check by trying to touch a dummy file)\n",
    "    if output_dir_path and output_dir_path.is_dir():\n",
    "        dummy_file = output_dir_path / \".test_write_permission\"\n",
    "        try:\n",
    "            dummy_file.touch()\n",
    "            dummy_file.unlink() # Clean up the dummy file\n",
    "            print(\"Write permissions seem OK.\")\n",
    "        except Exception as e:\n",
    "            print(f\"WARNING: Could not write to directory. Saving might fail. Error: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"ERROR: Required variables 'output_summary_dir_phase1' or 'Path' not found. Please run Cell 1.\")\n",
    "\n",
    "print(\"\\n--- Cell Y (Check Output Directory) Complete ---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5ba31a-598a-47dc-a3b1-d7021c352ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell X: Apply Revised Broad Functional Categorization and Replace Original Column\n",
    "\n",
    "print(\"\\n\\n--- Cell X: Applying Revised Broad Functional Categorization and Replacing Original Column ---\")\n",
    "\n",
    "# --- Assumptions ---\n",
    "# This cell assumes 'df_full' DataFrame is loaded and available.\n",
    "# It will replace the original 'Broad_Functional_Category' column with the revised one.\n",
    "# It relies on the 'broad_func_cat_col' variable defined in Cell 1.\n",
    "\n",
    "# --- Define the mapping from current to revised categories ---\n",
    "category_mapping = {\n",
    "    # Genome Information Processing\n",
    "    \"DNA Info Processing\": \"DNA Processing & Maintenance\",\n",
    "    \"DNA Replication, Repair, Recombination\": \"DNA Processing & Maintenance\",\n",
    "    \"RNA Info Processing\": \"RNA Processing & Transcription\",\n",
    "    \"Transcription and RNA Processing\": \"RNA Processing & Transcription\",\n",
    "\n",
    "    # Protein Synthesis & Modification\n",
    "    \"Translation\": \"Translation & Protein Synthesis\",\n",
    "    \"Translation and Protein Synthesis\": \"Translation & Protein Synthesis\",\n",
    "    # N-glycosylation is kept separate\n",
    "\n",
    "    # Trafficking & ESCRT\n",
    "    \"Membrane Trafficking/Vesicles\": \"Membrane Trafficking, Vesicles & ESCRT\",\n",
    "    \"ESCRT/Endosomal Sorting\": \"Membrane Trafficking, Vesicles & ESCRT\",\n",
    "\n",
    "    # General Features (Catch-all)\n",
    "    \"Homologous_superfamily\": \"General Protein Features\",\n",
    "    \"Binding_site\": \"General Protein Features\",\n",
    "    \"Active_site\": \"General Protein Features\",\n",
    "}\n",
    "\n",
    "# --- Apply the mapping and replace the original column ---\n",
    "print(f\"\\nApplying revised categorization and replacing the '{broad_func_cat_col}' column...\")\n",
    "\n",
    "if broad_func_cat_col in df_full.columns:\n",
    "    # Create a temporary new column with the revised categories\n",
    "    temp_revised_col_name = 'Temp_Revised_Category'\n",
    "    df_full[temp_revised_col_name] = df_full[broad_func_cat_col].apply(\n",
    "        lambda x: category_mapping.get(x, x) # Get the new category, or keep the old one if not in map\n",
    "    )\n",
    "    print(f\"Created temporary column '{temp_revised_col_name}'.\")\n",
    "\n",
    "    # Drop the original column\n",
    "    df_full = df_full.drop(columns=[broad_func_cat_col])\n",
    "    print(f\"Dropped original column '{broad_func_cat_col}'.\")\n",
    "\n",
    "    # Rename the temporary column to the original column name\n",
    "    df_full = df_full.rename(columns={temp_revised_col_name: broad_func_cat_col})\n",
    "    print(f\"Renamed '{temp_revised_col_name}' to '{broad_func_cat_col}'.\")\n",
    "\n",
    "    # --- Verification ---\n",
    "    print(f\"\\nVerification: Value counts for the updated '{broad_func_cat_col}' column:\")\n",
    "    # Ensure the column exists and display counts\n",
    "    if broad_func_cat_col in df_full.columns:\n",
    "        print(df_full[broad_func_cat_col].value_counts(dropna=False).to_markdown())\n",
    "    else:\n",
    "        print(f\"Error: Column '{broad_func_cat_col}' not found after renaming.\")\n",
    "\n",
    "\n",
    "    # Optional: Display a few rows to show the change\n",
    "    print(\"\\nSample rows showing the updated functional category:\")\n",
    "    sample_cols = [broad_func_cat_col] # Only display the one column now\n",
    "    if broad_func_cat_col in df_full.columns:\n",
    "        try:\n",
    "             print(df_full[sample_cols].head(10).to_markdown(index=False))\n",
    "        except ImportError:\n",
    "             print(df_full[sample_cols].head(10))\n",
    "    else:\n",
    "        print(\"Could not display sample rows: Required column not found.\")\n",
    "\n",
    "\n",
    "else:\n",
    "    print(f\"ERROR: Original broad functional category column '{broad_func_cat_col}' not found in df_full.\")\n",
    "    print(\"       Cannot apply the revised categorization.\")\n",
    "\n",
    "\n",
    "print(f\"\\n\\n--- Cell X (Apply Revised Categorization and Replace) Complete ---\")\n",
    "print(f\"The '{broad_func_cat_col}' column has been updated with the revised categories.\")\n",
    "\n",
    "# --- Optional: Save the updated DataFrame ---\n",
    "# Decide if you want to overwrite the v2.1 file or save as v2.2\n",
    "output_db_revised_cat_path = Path(output_summary_dir_phase1) / 'proteome_database_combined_v2.2_revised_cat.csv'\n",
    "\n",
    "print(f\"\\nAttempting to save updated database with revised categories to: {output_db_revised_cat_path}\") # <--- ADD THIS PRINT LINE\n",
    "\n",
    "try:\n",
    "    output_db_revised_cat_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_full.to_csv(output_db_revised_cat_path, index=False)\n",
    "    print(f\"Successfully saved updated database to '{output_db_revised_cat_path}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to save updated database: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59e7a32-a9d0-418c-91db-b8e97700867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20: Clean and Reorder DataFrame Columns\n",
    "\n",
    "print(\"\\n\\n--- Cell 20: Cleaning and Reordering DataFrame Columns ---\")\n",
    "\n",
    "# --- Assumptions ---\n",
    "# This cell assumes 'df_full' DataFrame is loaded and available\n",
    "# with the revised Broad_Functional_Category already applied and named 'Broad_Functional_Category'.\n",
    "\n",
    "# --- Define the desired order of columns ---\n",
    "# This list includes the columns you want to keep, in your specified order.\n",
    "# It excludes 'Specific_Functional_Category', 'Category_Trigger', and\n",
    "# the duplicate 'Broad_Functional_Category' that was at the end.\n",
    "desired_column_order = [\n",
    "    'ProteinID',\n",
    "    'Sequence',\n",
    "    'Length',\n",
    "    'Source_Dataset',\n",
    "    'Source_Genome_Assembly_Accession',\n",
    "    'Source_Protein_Annotation',\n",
    "    'NCBI_TaxID',\n",
    "    'Asgard_Phylum',\n",
    "    'Virus_Family',\n",
    "    'Virus_Name',\n",
    "    'Orthogroup',\n",
    "    'IPR_Signatures',\n",
    "    'IPR_GO_Terms',\n",
    "    'UniProtKB_AC',\n",
    "    'Num_Domains',\n",
    "    'Domain_Architecture',\n",
    "    'Type',\n",
    "    'Is_Hypothetical',\n",
    "    'Has_Known_Structure',\n",
    "    'Percent_Disorder',\n",
    "    'Signal_Peptide_USPNet',\n",
    "    'SP_Cleavage_Site_USPNet',\n",
    "    'Original_Seq_Length',\n",
    "    'Group',\n",
    "    'SeqSearch_PDB_Hit',\n",
    "    'SeqSearch_AFDB_Hit',\n",
    "    'Has_Reference_Structure',\n",
    "    'Predicted_Subcellular_Localization',\n",
    "    'Mature_Protein_Sequence',\n",
    "    'Mature_Seq_Length',\n",
    "    'SeqSearch_MGnify_Hit',\n",
    "    'SeqSearch_ESMA_Hit',\n",
    "    'Is_Structurally_Dark',\n",
    "    'Is_ESP',\n",
    "    'Has_Euk_DIAMOND_Hit',\n",
    "    'Euk_Hit_SSEQID',\n",
    "    'Euk_Hit_Organism',\n",
    "    'Euk_Hit_PIDENT',\n",
    "    'Euk_Hit_EVALUE',\n",
    "    'Euk_Hit_Protein_Name',\n",
    "    'Euk_Hit_Qstart',\n",
    "    'Euk_Hit_Qend',\n",
    "    'Euk_Hit_Sstart',\n",
    "    'Euk_Hit_Send',\n",
    "    'Euk_Hit_Slen_Diamond',\n",
    "    'Query_Coverage',\n",
    "    'Subject_Coverage',\n",
    "    'Is_RBH',\n",
    "    'Broad_Functional_Category' # This column now contains the revised categories\n",
    "    # Exclude the duplicate 'Broad_Functional_Category' here (if it existed after loading)\n",
    "]\n",
    "\n",
    "# --- Check for columns that are supposed to be in the desired list but are missing from df_full ---\n",
    "# This helps identify potential issues before reindexing\n",
    "current_cols = df_full.columns.tolist()\n",
    "missing_desired_cols = [col for col in desired_column_order if col not in current_cols]\n",
    "\n",
    "if missing_desired_cols:\n",
    "    print(f\"WARNING: The following columns are in the desired order list but not found in the DataFrame: {missing_desired_cols}\")\n",
    "    print(\"These columns will not be included in the cleaned DataFrame.\")\n",
    "    # Optionally, filter desired_column_order to only include present columns\n",
    "    # desired_column_order = [col for col in desired_column_order if col in current_cols]\n",
    "\n",
    "\n",
    "# --- Check for columns in df_full that are NOT in the desired list (these will be dropped) ---\n",
    "cols_to_drop = [col for col in current_cols if col not in desired_column_order]\n",
    "print(f\"Columns that will be dropped: {cols_to_drop}\")\n",
    "\n",
    "# --- Reindex the DataFrame to select and reorder columns ---\n",
    "try:\n",
    "    df_full = df_full[desired_column_order]\n",
    "    print(f\"\\nDataFrame cleaned and reordered successfully. New shape: {df_full.shape}\")\n",
    "    print(\"New columns order:\")\n",
    "    print(df_full.columns.tolist())\n",
    "\n",
    "except KeyError as e:\n",
    "    print(f\"ERROR: Failed to reindex DataFrame. A specified column was not found after the previous steps: {e}\")\n",
    "    print(\"Please check the column names in the 'desired_column_order' list.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during reindexing: {e}\")\n",
    "\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"\\nVerification: First 5 rows of the cleaned DataFrame:\")\n",
    "try:\n",
    "    print(df_full.head().to_markdown())\n",
    "except ImportError:\n",
    "    print(df_full.head())\n",
    "\n",
    "print(\"\\n\\n--- Cell 20 (Clean and Reorder Columns) Complete ---\")\n",
    "\n",
    "# --- Optional: Save the cleaned DataFrame ---\n",
    "# Choose a new filename to indicate this level of cleaning/revision\n",
    "output_db_cleaned_path = Path(output_summary_dir_phase1) / 'proteome_database_combined_v2.3_cleaned.csv'\n",
    "print(f\"\\nSaving cleaned database to: {output_db_cleaned_path}\")\n",
    "try:\n",
    "     output_db_cleaned_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "     df_full.to_csv(output_db_cleaned_path, index=False)\n",
    "     print(f\"Successfully saved cleaned database to '{output_db_cleaned_path}'.\")\n",
    "except Exception as e:\n",
    "     print(f\"Failed to save cleaned database: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04931461-78f3-40da-bf03-a4c79af3d178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup, Data Loading for Deaminase Exploration\n",
    "\n",
    "# --- Standard Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from collections import Counter\n",
    "from pathlib import Path \n",
    "\n",
    "# --- Plotly Imports ---\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "# --- Arcadia Style Imports (Attempt) ---\n",
    "try:\n",
    "    import arcadia_pycolor as apc\n",
    "    arcadia_primary_palette = apc.primary_standard_rgb_strings\n",
    "    arcadia_secondary_palette = apc.secondary_standard_rgb_strings\n",
    "    arcadia_neutrals_palette = apc.neutrals_standard_rgb_strings\n",
    "    arcadia_paper_color = apc.paper_rgb_string\n",
    "    arcadia_font_family = \"Arial, sans-serif\" \n",
    "    arcadia_font_color = apc.default_text_rgb_string\n",
    "    arcadia_axis_color = apc.default_text_rgb_string \n",
    "    arcadia_grid_color = apc.gray_medium_rgb_string \n",
    "    ARCADIA_STYLE_AVAILABLE = True\n",
    "    print(\"Successfully imported arcadia_pycolor. Arcadia style will be applied.\")\n",
    "except (ImportError, AttributeError) as e: \n",
    "    ARCADIA_STYLE_AVAILABLE = False\n",
    "    print(f\"Warning: arcadia_pycolor import or attribute access failed ({e}). Using manual Arcadia color palettes and Plotly defaults.\")\n",
    "    # Manual Fallback Arcadia Color Palettes\n",
    "    arcadia_colors_manual = {\n",
    "        \"aegean\": \"#5088C5\", \"amber\": \"#F28360\", \"seaweed\": \"#3B9886\", \"canary\": \"#F7B846\",\n",
    "        \"aster\": \"#7A77AB\", \"rose\": \"#F898AE\", \"vital\": \"#73B5E3\", \"tangerine\": \"#FFB984\",\n",
    "        \"oat\": \"#F5E4BE\", \"wish\": \"#BABEE0\", \"lime\": \"#97CD78\", \"dragon\": \"#C85152\",\n",
    "        \"sky\": \"#C6E7F4\", \"dress\": \"#F8C5C1\", \"taupe\": \"#DBD1C3\", \"denim\": \"#B6C8D4\",\n",
    "        \"sage\": \"#B5BEA4\", \"mars\": \"#DA9085\", \"marine\": \"#8A99AD\", \"shell\": \"#EDE0D6\",\n",
    "        \"white\": \"#FFFFFF\", \"gray\": \"#EBEDE8\", \"chateau\": \"#BAB0A8\", \"bark\": \"#8F8885\",\n",
    "        \"slate\": \"#43413F\", \"charcoal\": \"#484B50\", \"crow\": \"#292928\", \"black\": \"#09090A\",\n",
    "        \"forest\": \"#596F74\", \"parchment\": \"#FEF7F1\", \"zephyr\": \"#F4FBFF\",\n",
    "        \"lichen\": \"#F7FBEF\", \"dawn\": \"#F8F4F1\"\n",
    "    }\n",
    "    arcadia_primary_palette = [\n",
    "        arcadia_colors_manual[\"aegean\"], arcadia_colors_manual[\"amber\"], arcadia_colors_manual[\"seaweed\"],\n",
    "        arcadia_colors_manual[\"canary\"], arcadia_colors_manual[\"aster\"], arcadia_colors_manual[\"rose\"],\n",
    "        arcadia_colors_manual[\"vital\"], arcadia_colors_manual[\"tangerine\"], arcadia_colors_manual[\"oat\"],\n",
    "        arcadia_colors_manual[\"wish\"], arcadia_colors_manual[\"lime\"], arcadia_colors_manual[\"dragon\"]\n",
    "    ]\n",
    "    arcadia_secondary_palette = [\n",
    "        arcadia_colors_manual[\"sky\"], arcadia_colors_manual[\"dress\"], arcadia_colors_manual[\"taupe\"],\n",
    "        arcadia_colors_manual[\"denim\"], arcadia_colors_manual[\"sage\"], arcadia_colors_manual[\"mars\"],\n",
    "        arcadia_colors_manual[\"marine\"], arcadia_colors_manual[\"shell\"]\n",
    "    ]\n",
    "    arcadia_neutrals_palette = [\n",
    "        arcadia_colors_manual[\"gray\"], arcadia_colors_manual[\"chateau\"], arcadia_colors_manual[\"bark\"],\n",
    "        arcadia_colors_manual[\"slate\"], arcadia_colors_manual[\"charcoal\"], arcadia_colors_manual[\"forest\"],\n",
    "        arcadia_colors_manual[\"crow\"]\n",
    "    ]\n",
    "    arcadia_paper_color = arcadia_colors_manual[\"white\"]\n",
    "    arcadia_font_family = \"Arial, sans-serif\" \n",
    "    arcadia_font_color = arcadia_colors_manual[\"slate\"] \n",
    "    arcadia_axis_color = arcadia_colors_manual[\"slate\"] \n",
    "    arcadia_grid_color = arcadia_colors_manual[\"gray\"]   \n",
    "\n",
    "print(\"\\n--- Database Pub Analysis Notebook Setup ---\")\n",
    "\n",
    "# --- Configuration ---\n",
    "# !!! USER: Ensure this path points to your latest, most complete database CSV !!!\n",
    "DB_PATH = 'proteome_database_v2.5.csv' \n",
    "\n",
    "# Define output directories for this notebook's specific analyses\n",
    "# These might differ from the GV_Analysis notebook\n",
    "DEAMINASE_PLOT_OUTPUT_DIR = Path('output_plots_deaminase_analysis') \n",
    "DEAMINASE_SUMMARY_DATA_DIR = Path('output_summary_data_deaminase_analysis')\n",
    "\n",
    "DEAMINASE_PLOT_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DEAMINASE_SUMMARY_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Plot output directory for deaminase analysis: {DEAMINASE_PLOT_OUTPUT_DIR.resolve()}\")\n",
    "print(f\"Summary data directory for deaminase analysis: {DEAMINASE_SUMMARY_DATA_DIR.resolve()}\")\n",
    "\n",
    "INTERPRO_ENTRY_PATH = 'interpro_entry.txt' # Path to your InterPro names file\n",
    "\n",
    "# --- Define Common Column Names (ensure these match your CSV) ---\n",
    "protein_id_col = 'ProteinID'\n",
    "og_col = 'Orthogroup'\n",
    "group_col = 'Group' # 'Asgard' or 'GV'\n",
    "source_dataset_col = 'Source_Dataset' # Asgard / GV\n",
    "virus_family_col = 'Virus_Family' # For GVs\n",
    "virus_name_col = 'Virus_Name' \n",
    "asgard_phylum_col = 'Asgard_Phylum' # For Asgard\n",
    "broad_func_cat_col = 'Broad_Functional_Category'\n",
    "specific_func_cat_col = 'Specific_Functional_Category' # If you have this\n",
    "domain_arch_col = 'Domain_Architecture'\n",
    "num_domains_col = 'Num_Domains'\n",
    "ipr_col = 'IPR_Signatures'\n",
    "source_annot_col = 'Source_Protein_Annotation' \n",
    "dark_col = 'Is_Structurally_Dark'\n",
    "length_col = 'Original_Seq_Length' \n",
    "disorder_col = 'Percent_Disorder'\n",
    "localization_col = 'Predicted_Subcellular_Localization'\n",
    "sp_type_col = 'Signal_Peptide_USPNet'\n",
    "sequence_col = 'Sequence'\n",
    "is_esp_col = 'Is_ESP' # For Asgard proteins\n",
    "\n",
    "# Eukaryotic Hit Columns\n",
    "has_euk_hit_col = 'Has_Euk_DIAMOND_Hit'\n",
    "euk_hit_sseqid_col = 'Euk_Hit_SSEQID'\n",
    "euk_hit_organism_col = 'Euk_Hit_Organism' \n",
    "euk_hit_pident_col = 'Euk_Hit_PIDENT'\n",
    "euk_hit_evalue_col = 'Euk_Hit_EVALUE'\n",
    "euk_hit_protein_name_col = 'Euk_Hit_Protein_Name' \n",
    "euk_hit_qstart_col = 'Euk_Hit_Qstart'\n",
    "euk_hit_qend_col = 'Euk_Hit_Qend'\n",
    "euk_hit_sstart_col = 'Euk_Hit_Sstart'\n",
    "euk_hit_send_col = 'Euk_Hit_Send'\n",
    "euk_hit_slen_col = 'Euk_Hit_Slen_Diamond'\n",
    "query_coverage_col = 'Query_Coverage'\n",
    "subject_coverage_col = 'Subject_Coverage'\n",
    "\n",
    "# Intra-OG Diversity Columns (if merged into main DB, otherwise load separately)\n",
    "apsi_col = 'Intra_OG_APSI' # Example name, adjust if different\n",
    "shannon_entropy_col = 'Shannon_Entropy' # Example name\n",
    "observed_richness_col = 'Observed_Richness' # Example name\n",
    "\n",
    "# List of all columns you anticipate needing for general analyses in this notebook\n",
    "columns_to_load_main = list(set([ \n",
    "    protein_id_col, og_col, group_col, source_dataset_col, virus_family_col, virus_name_col, asgard_phylum_col,\n",
    "    broad_func_cat_col, specific_func_cat_col, domain_arch_col, num_domains_col, ipr_col, source_annot_col,\n",
    "    dark_col, length_col, disorder_col, localization_col, sp_type_col, sequence_col, is_esp_col,\n",
    "    has_euk_hit_col, euk_hit_sseqid_col, euk_hit_organism_col, euk_hit_pident_col,\n",
    "    euk_hit_evalue_col, euk_hit_protein_name_col, euk_hit_qstart_col, euk_hit_qend_col,\n",
    "    euk_hit_sstart_col, euk_hit_send_col, euk_hit_slen_col,\n",
    "    query_coverage_col, subject_coverage_col,\n",
    "    apsi_col, shannon_entropy_col, observed_richness_col # Add diversity metrics here\n",
    "]))\n",
    "print(f\"\\nWill attempt to load these columns for general analysis: {columns_to_load_main}\")\n",
    "\n",
    "# --- Load Main DataFrame (df_full) ---\n",
    "print(f\"\\n--- Loading Main Data from '{DB_PATH}' ---\")\n",
    "start_time = time.time()\n",
    "df_full = pd.DataFrame() \n",
    "\n",
    "try:\n",
    "    if not Path(DB_PATH).is_file():\n",
    "        print(f\"CRITICAL ERROR: Database file not found at '{DB_PATH}'. Cannot proceed.\")\n",
    "        raise FileNotFoundError(f\"Database file not found: {DB_PATH}\")\n",
    "\n",
    "    header_df = pd.read_csv(DB_PATH, nrows=0)\n",
    "    actual_cols_to_load = [col for col in columns_to_load_main if col in header_df.columns]\n",
    "    missing_cols_in_file = [col for col in columns_to_load_main if col not in header_df.columns]\n",
    "    if missing_cols_in_file:\n",
    "        print(f\"Warning: The following specified columns were NOT found in '{DB_PATH}': {missing_cols_in_file}\")\n",
    "    \n",
    "    if not actual_cols_to_load:\n",
    "        print(f\"CRITICAL ERROR: No columns to load from '{DB_PATH}' based on 'columns_to_load_main'. Check column names.\")\n",
    "        raise ValueError(\"No columns to load.\")\n",
    "\n",
    "    df_full = pd.read_csv(DB_PATH, usecols=actual_cols_to_load, low_memory=False)\n",
    "    print(f\"Loaded main database. Shape: {df_full.shape}\")\n",
    "\n",
    "    print(\"\\nPerforming initial data cleaning and type setting on loaded data...\")\n",
    "    # Ensure ProteinID is string\n",
    "    if protein_id_col in df_full.columns:\n",
    "        df_full[protein_id_col] = df_full[protein_id_col].astype(str)\n",
    "    else:\n",
    "        print(f\"CRITICAL WARNING: '{protein_id_col}' not found in loaded df_full. This will cause issues.\")\n",
    "\n",
    "    # Set categorical columns\n",
    "    cat_cols_main = [group_col, source_dataset_col, virus_family_col, asgard_phylum_col, \n",
    "                     broad_func_cat_col, specific_func_cat_col, localization_col, sp_type_col]\n",
    "    cat_fill_values_main = {\n",
    "        group_col: 'Unknown', source_dataset_col: 'Unknown', virus_family_col: 'Unknown', \n",
    "        asgard_phylum_col: 'Unknown', broad_func_cat_col: 'Unknown/Unclassified', \n",
    "        specific_func_cat_col: 'Unknown/Unclassified', localization_col: 'Unknown', sp_type_col: 'NO_SP'\n",
    "    }\n",
    "    for col in cat_cols_main:\n",
    "        if col in df_full.columns:\n",
    "            df_full[col] = df_full[col].fillna(cat_fill_values_main.get(col, 'Unknown')).astype('category')\n",
    "\n",
    "    # Set string columns\n",
    "    str_cols_main = [og_col, ipr_col, domain_arch_col, source_annot_col, virus_name_col, \n",
    "                     euk_hit_sseqid_col, euk_hit_organism_col, euk_hit_protein_name_col, sequence_col]\n",
    "    for col in str_cols_main:\n",
    "        if col in df_full.columns:\n",
    "            df_full[col] = df_full[col].fillna('').astype(str)\n",
    "    \n",
    "    # Set boolean columns\n",
    "    bool_cols_main = [dark_col, has_euk_hit_col, is_esp_col]\n",
    "    for col in bool_cols_main:\n",
    "        if col in df_full.columns:\n",
    "            df_full[col] = df_full[col].fillna(False).astype(bool)\n",
    "    \n",
    "    # Set numeric columns\n",
    "    num_cols_main = [\n",
    "        num_domains_col, length_col, disorder_col, euk_hit_pident_col, euk_hit_evalue_col,\n",
    "        euk_hit_qstart_col, euk_hit_qend_col, euk_hit_sstart_col, euk_hit_send_col,\n",
    "        euk_hit_slen_col, query_coverage_col, subject_coverage_col,\n",
    "        apsi_col, shannon_entropy_col, observed_richness_col # Diversity metrics\n",
    "    ]\n",
    "    for col in num_cols_main:\n",
    "         if col in df_full.columns:\n",
    "              df_full[col] = pd.to_numeric(df_full[col], errors='coerce')\n",
    "    \n",
    "    # Check if df_full is empty after loading/cleaning\n",
    "    if df_full.empty:\n",
    "        print(\"CRITICAL WARNING: df_full is empty after loading and initial processing. Check DB_PATH and column definitions.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Database file not found at '{DB_PATH}'. Please ensure the path is correct.\")\n",
    "    # df_full remains empty as initialized\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading or cleaning the database CSV: {e}\")\n",
    "    # df_full remains empty\n",
    "print(f\"Database loading and initial processing finished in {time.time() - start_time:.2f} seconds.\")\n",
    "print(f\"Shape of df_full: {df_full.shape}\")\n",
    "\n",
    "\n",
    "# --- Load InterPro Entry Data (for descriptions) ---\n",
    "print(f\"\\n--- Loading InterPro Entry Data ---\")\n",
    "ipr_lookup = {}\n",
    "start_time_ipr = time.time()\n",
    "if Path(INTERPRO_ENTRY_PATH).is_file():\n",
    "    try:\n",
    "        ipr_info_df = pd.read_csv(\n",
    "            INTERPRO_ENTRY_PATH, sep='\\t', usecols=[0, 1, 2],\n",
    "            names=['IPR_ID', 'Type', 'Name'], header=None, comment='#', on_bad_lines='warn'\n",
    "        )\n",
    "        ipr_info_df['IPR_ID'] = ipr_info_df['IPR_ID'].astype(str).str.strip()\n",
    "        ipr_lookup = ipr_info_df.set_index('IPR_ID')[['Type', 'Name']].to_dict('index')\n",
    "        print(f\"Loaded InterPro entry data for {len(ipr_lookup)} entries in {time.time() - start_time_ipr:.2f} seconds.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: An error occurred loading or processing '{INTERPRO_ENTRY_PATH}': {e}\")\n",
    "else:\n",
    "    print(f\"Warning: InterPro entry file not found at '{INTERPRO_ENTRY_PATH}'. IPR name translations will be unavailable.\")\n",
    "if not ipr_lookup: print(\"Warning: ipr_lookup is empty. Domain name translations will not be available.\")\n",
    "\n",
    "\n",
    "# --- Configure Plotly Defaults (Arcadia Style) ---\n",
    "print(\"\\n--- Configuring Plotly Defaults (Arcadia Style) ---\")\n",
    "plotly_layout_defaults = go.Layout(\n",
    "    font=dict(family=arcadia_font_family, size=12, color=arcadia_font_color),\n",
    "    title_font=dict(family=arcadia_font_family, size=18, color=arcadia_font_color),\n",
    "    paper_bgcolor=arcadia_paper_color,\n",
    "    plot_bgcolor=arcadia_paper_color, \n",
    "    xaxis=dict(\n",
    "        title_font=dict(size=15, family=arcadia_font_family, color=arcadia_axis_color), \n",
    "        tickfont=dict(size=12, family=arcadia_font_family, color=arcadia_axis_color),\n",
    "        showgrid=False, zeroline=False,\n",
    "        linecolor=arcadia_axis_color, linewidth=1.5, ticks='outside', ticklen=5, tickwidth=1.5, tickcolor=arcadia_axis_color\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title_font=dict(size=15, family=arcadia_font_family, color=arcadia_axis_color),\n",
    "        tickfont=dict(size=12, family=arcadia_font_family, color=arcadia_axis_color),\n",
    "        showgrid=False, zeroline=False,\n",
    "        linecolor=arcadia_axis_color, linewidth=1.5, ticks='outside', ticklen=5, tickwidth=1.5, tickcolor=arcadia_axis_color\n",
    "    ),\n",
    "    legend=dict(\n",
    "        font=dict(size=12, family=arcadia_font_family, color=arcadia_font_color),\n",
    "        bgcolor=arcadia_paper_color,\n",
    "        bordercolor=arcadia_grid_color, \n",
    "        borderwidth=1\n",
    "    )\n",
    ")\n",
    "pio.templates[\"arcadia_theme_db_analysis\"] = pio.templates[\"plotly_white\"] \n",
    "pio.templates[\"arcadia_theme_db_analysis\"].layout = plotly_layout_defaults\n",
    "pio.templates.default = \"arcadia_theme_db_analysis\" \n",
    "print(\"Plotly default template set to 'arcadia_theme_db_analysis'.\")\n",
    "\n",
    "# --- Define Helper Functions ---\n",
    "print(\"\\n--- Defining Helper Functions ---\")\n",
    "\n",
    "def translate_architecture(arch_string, lookup_dict=ipr_lookup): \n",
    "    if not isinstance(arch_string, str) or not arch_string or not lookup_dict: return arch_string\n",
    "    ipr_ids = arch_string.split(';')\n",
    "    translated_parts = []\n",
    "    for ipr_id in ipr_ids:\n",
    "        ipr_id_clean = ipr_id.strip()\n",
    "        if ipr_id_clean in lookup_dict:\n",
    "            name = lookup_dict[ipr_id_clean].get('Name', ipr_id_clean)\n",
    "            name = name[:40] + '...' if len(name) > 43 else name # Truncate long names\n",
    "            translated_parts.append(f\"{name} ({ipr_id_clean})\")\n",
    "        elif ipr_id_clean: # Handle cases where IPR ID might not be in lookup\n",
    "            translated_parts.append(ipr_id_clean)\n",
    "    full_translation = \"; \".join(translated_parts)\n",
    "    max_len = 150 \n",
    "    if len(full_translation) > max_len:\n",
    "         full_translation = full_translation[:max_len-3] + \"...\"\n",
    "    return full_translation\n",
    "\n",
    "def clean_protein_name(name_str):\n",
    "    if not isinstance(name_str, str):\n",
    "        return \"Unknown\" \n",
    "    name_str = re.sub(r'\\[.*?\\]', '', name_str) \n",
    "    name_str = re.sub(r'\\s*\\(Fragment\\)$', '', name_str, flags=re.IGNORECASE)\n",
    "    name_str = re.sub(r'^(PREDICTED|LOW QUALITY PROTEIN|UNCHARACTERIZED PROTEIN|HYPOTHETICAL PROTEIN):\\s*', '', name_str, flags=re.IGNORECASE)\n",
    "    name_str = re.sub(r',\\s*partial$', '', name_str, flags=re.IGNORECASE)\n",
    "    name_str = re.sub(r'\\s*protein$', '', name_str, flags=re.IGNORECASE) \n",
    "    name_str = re.sub(r'\\s*isoform X\\d*$', '', name_str, flags=re.IGNORECASE) \n",
    "    name_str = re.sub(r'\\s*isoform \\w+$', '', name_str, flags=re.IGNORECASE) \n",
    "    name_str = re.sub(r'type\\s+\\w+', '', name_str, flags=re.IGNORECASE).strip() \n",
    "    name_str = name_str.strip() \n",
    "    if not name_str or name_str.lower() in [\"hypothetical\", \"uncharacterized\", \"unknown\", \"predicted\"]: # Added \"predicted\"\n",
    "        return \"Unknown\"\n",
    "    return name_str[0].upper() + name_str[1:] if len(name_str) > 0 else \"Unknown\"\n",
    "\n",
    "# Color maps (can be expanded or moved to specific cells if only used there)\n",
    "group_colors = {\n",
    "    'Asgard': arcadia_primary_palette[0 % len(arcadia_primary_palette)],\n",
    "    'GV': arcadia_primary_palette[1 % len(arcadia_primary_palette)],\n",
    "    'Other': arcadia_neutrals_palette[0 % len(arcadia_neutrals_palette)]\n",
    "}\n",
    "\n",
    "broad_category_color_map = {}\n",
    "if 'Broad_Functional_Category' in df_full.columns and not df_full.empty:\n",
    "    all_broad_categories_cat = df_full['Broad_Functional_Category'].cat.categories.tolist()\n",
    "    category_assignments = { \n",
    "        'Cytoskeleton': arcadia_primary_palette[0], 'Membrane Trafficking/Vesicles': arcadia_primary_palette[1],\n",
    "        'ESCRT/Endosomal Sorting': arcadia_primary_palette[2], 'Ubiquitin System': arcadia_primary_palette[3],\n",
    "        'N-glycosylation': arcadia_primary_palette[4], 'Nuclear Transport/Pore': arcadia_primary_palette[5],\n",
    "        'DNA Info Processing': arcadia_primary_palette[6], 'DNA Processing & Maintenance': arcadia_primary_palette[6], \n",
    "        'RNA Info Processing': arcadia_primary_palette[7], 'RNA Processing & Transcription': arcadia_primary_palette[7], \n",
    "        'Translation': arcadia_primary_palette[8], 'Translation & Protein Synthesis': arcadia_primary_palette[8], \n",
    "        'Signal Transduction': arcadia_primary_palette[9], 'Metabolism': arcadia_primary_palette[10],\n",
    "        'Other Specific Annotation': arcadia_secondary_palette[0 % len(arcadia_secondary_palette)],\n",
    "        'Unknown/Unclassified': arcadia_neutrals_palette[0 % len(arcadia_neutrals_palette)],\n",
    "        'General Protein Features': arcadia_neutrals_palette[1 % len(arcadia_neutrals_palette)],\n",
    "        'Homologous_superfamily': arcadia_secondary_palette[1 % len(arcadia_secondary_palette)], \n",
    "        'Binding_site': arcadia_secondary_palette[2 % len(arcadia_secondary_palette)],\n",
    "        'Active_site': arcadia_secondary_palette[3 % len(arcadia_secondary_palette)],\n",
    "    }\n",
    "    fallback_palette_cat = arcadia_primary_palette + arcadia_secondary_palette + arcadia_neutrals_palette\n",
    "    fallback_idx_cat = 0\n",
    "    for category in all_broad_categories_cat:\n",
    "        if category in category_assignments: \n",
    "            broad_category_color_map[category] = category_assignments[category]\n",
    "        else: \n",
    "            broad_category_color_map[category] = fallback_palette_cat[fallback_idx_cat % len(fallback_palette_cat)]\n",
    "            fallback_idx_cat += 1\n",
    "print(f\"Broad functional category color map created for {len(broad_category_color_map)} categories (if df_full loaded).\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Database Pub Analysis Setup Complete ---\")\n",
    "# df_full is now loaded and cleaned.\n",
    "# Key variables, helper functions, and color maps are defined.\n",
    "\n",
    "# Add a final check to ensure df_full is not empty before subsequent cells try to use it.\n",
    "if 'df_full' in locals() and df_full.empty:\n",
    "    print(\"CRITICAL WARNING: df_full is defined but EMPTY after loading and processing. Please check your input CSV, the column definitions, and any filtering logic.\")\n",
    "elif 'df_full' not in locals():\n",
    "    print(\"CRITICAL ERROR: df_full was NOT defined in Cell 1. Subsequent cells will fail.\")\n",
    "else:\n",
    "    print(f\"df_full is ready for analysis. Shape: {df_full.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ecdaae-345b-4d08-b2f3-741fea043c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 22: Exploration of Putative Deaminases\n",
    "\n",
    "# This cell assumes Cell 1 (Setup for Deaminase Exploration) has been run.\n",
    "# Required variables from Cell 1:\n",
    "# - df_full (DataFrame with all annotations)\n",
    "# - DEAMINASE_PLOT_OUTPUT_DIR, DEAMINASE_SUMMARY_DATA_DIR (Path objects for output)\n",
    "# - protein_id_col, group_col, broad_func_cat_col, specific_func_cat_col, \n",
    "#   source_annot_col, ipr_col, euk_hit_protein_name_col, has_euk_hit_col,\n",
    "#   dark_col, is_esp_col\n",
    "# - ipr_lookup (dictionary for IPR descriptions)\n",
    "# - clean_protein_name (helper function)\n",
    "# - group_colors, broad_category_color_map (color maps)\n",
    "# - plotly_layout_defaults (implicitly used by pio.templates.default)\n",
    "\n",
    "print(\"--- Cell 22: Deaminase Exploration ---\")\n",
    "\n",
    "if 'df_full' not in locals() or df_full.empty:\n",
    "    raise NameError(\"DataFrame 'df_full' is not defined or empty. Please run Cell 1 (Setup) first.\")\n",
    "\n",
    "# --- Define Deaminase-Related Keywords and IPR IDs ---\n",
    "# Keywords are case-insensitive for searching text fields\n",
    "deaminase_keywords = [\n",
    "    'deaminase', 'cytidine', 'adenosine', 'cda', 'ada', 'adar', 'apobec', \n",
    "    'aid', 'ampo', # APOBEC/AMP deaminase related\n",
    "    'ctd', # cytidine triphosphate deaminase\n",
    "    'dcd', # dCMP deaminase\n",
    "    'monomethylarsonate reductase', # Some can have deaminase activity\n",
    "    'nitrilase' # Broader family, some members are deaminases\n",
    "]\n",
    "# Specific IPR IDs known or suspected to be related to deaminases\n",
    "# This list can be expanded significantly with more research\n",
    "deaminase_ipr_ids = {\n",
    "    'IPR002001', # Cytidine/deoxycytidylate deaminase zinc-binding region\n",
    "    'IPR018218', # Cytidine deaminase-like\n",
    "    'IPR000335', # Adenosine deaminase\n",
    "    'IPR001365', # Adenosine deaminase domain\n",
    "    'IPR002466', # Adenosine deaminase/editase\n",
    "    'IPR013659', # Adenosine/AMP deaminase N-terminal\n",
    "    'IPR006330', # Adenosine/adenine deaminase\n",
    "    'IPR006329', # AMP deaminase\n",
    "    'IPR006331', # Adenosine deaminase-related growth factor\n",
    "    'IPR028893', # Adenisone deaminase\n",
    "    'IPR042935', # tRNA-specific adenosine deaminase 1\n",
    "    'IPR012839', # Adenosine deaminase, A.thaliana-like\n",
    "    'IPR012340', # APOBEC, N-terminal\n",
    "    'IPR003592', # APOBEC, C-terminal catalytic domain\n",
    "    'IPR016292', # ADAR, dsRNA adenosine deaminase\n",
    "    'IPR001907', # Nitrilase/cyanide hydratase and apolipoprotein N-acyltransferase\n",
    "    'IPR036618', # Deaminase TadA-like domain\n",
    "    'IPR006250', # dCMP deaminase, N-terminal\n",
    "    'IPR006251', # dCMP deaminase, C-terminal\n",
    "    'IPR004118', # Cytidine/deoxycytidylate deaminase family\n",
    "    'IPR001570', # Phosphoribosyl-AMP cyclohydrolase/phosphoribosyl-ATP pyrophosphatase HisI\n",
    "    # Add more relevant IPR IDs here\n",
    "}\n",
    "print(f\"Searching for proteins related to {len(deaminase_keywords)} keywords and {len(deaminase_ipr_ids)} IPR IDs.\")\n",
    "\n",
    "# --- Search for Putative Deaminases ---\n",
    "# Create boolean masks for each search criterion\n",
    "\n",
    "# 1. Search in Broad Functional Category\n",
    "mask_broad_cat = pd.Series(False, index=df_full.index)\n",
    "if broad_func_cat_col in df_full.columns:\n",
    "    mask_broad_cat = df_full[broad_func_cat_col].astype(str).str.contains('|'.join(deaminase_keywords), case=False, na=False)\n",
    "    print(f\"Found {mask_broad_cat.sum()} proteins by Broad Functional Category keywords.\")\n",
    "\n",
    "# 2. Search in Specific Functional Category (if it exists and is used differently)\n",
    "mask_specific_cat = pd.Series(False, index=df_full.index)\n",
    "if specific_func_cat_col in df_full.columns and specific_func_cat_col != broad_func_cat_col:\n",
    "    mask_specific_cat = df_full[specific_func_cat_col].astype(str).str.contains('|'.join(deaminase_keywords), case=False, na=False)\n",
    "    print(f\"Found {mask_specific_cat.sum()} proteins by Specific Functional Category keywords.\")\n",
    "\n",
    "# 3. Search in Source Protein Annotation\n",
    "mask_source_annot = pd.Series(False, index=df_full.index)\n",
    "if source_annot_col in df_full.columns:\n",
    "    mask_source_annot = df_full[source_annot_col].astype(str).str.contains('|'.join(deaminase_keywords), case=False, na=False)\n",
    "    print(f\"Found {mask_source_annot.sum()} proteins by Source Annotation keywords.\")\n",
    "\n",
    "# 4. Search in Eukaryotic Hit Protein Name\n",
    "mask_euk_hit_name = pd.Series(False, index=df_full.index)\n",
    "if euk_hit_protein_name_col in df_full.columns and has_euk_hit_col in df_full.columns:\n",
    "    # Apply clean_protein_name before searching\n",
    "    cleaned_euk_names = df_full[df_full[has_euk_hit_col]][euk_hit_protein_name_col].astype(str).apply(clean_protein_name)\n",
    "    mask_euk_hit_name.loc[cleaned_euk_names.index] = cleaned_euk_names.str.contains('|'.join(deaminase_keywords), case=False, na=False)\n",
    "    print(f\"Found {mask_euk_hit_name.sum()} proteins by Eukaryotic Hit Protein Name keywords (after cleaning).\")\n",
    "\n",
    "# 5. Search in IPR Signatures (IDs and Descriptions)\n",
    "mask_ipr = pd.Series(False, index=df_full.index)\n",
    "if ipr_col in df_full.columns and ipr_lookup:\n",
    "    ipr_hits_indices = []\n",
    "    for index, row in df_full.iterrows():\n",
    "        if pd.notna(row[ipr_col]) and row[ipr_col].strip():\n",
    "            protein_ipr_ids = set(pid.strip() for pid in row[ipr_col].split(';') if pid.strip())\n",
    "            # Check direct IPR ID match\n",
    "            if not protein_ipr_ids.isdisjoint(deaminase_ipr_ids):\n",
    "                ipr_hits_indices.append(index)\n",
    "                continue \n",
    "            # Check IPR descriptions for keywords\n",
    "            for ipr_id_in_protein in protein_ipr_ids:\n",
    "                ipr_data = ipr_lookup.get(ipr_id_in_protein)\n",
    "                if ipr_data and 'Name' in ipr_data:\n",
    "                    if any(keyword in ipr_data['Name'].lower() for keyword in deaminase_keywords):\n",
    "                        ipr_hits_indices.append(index)\n",
    "                        break # Found a keyword match for this protein, move to next protein\n",
    "    if ipr_hits_indices:\n",
    "        mask_ipr.loc[list(set(ipr_hits_indices))] = True # Use set to handle duplicates if a protein hits multiple ways\n",
    "    print(f\"Found {mask_ipr.sum()} proteins by IPR ID match or IPR description keywords.\")\n",
    "\n",
    "# Combine masks: a protein is a putative deaminase if it matches any criterion\n",
    "combined_mask = mask_broad_cat | mask_specific_cat | mask_source_annot | mask_euk_hit_name | mask_ipr\n",
    "df_deaminases = df_full[combined_mask].copy()\n",
    "# Add a column indicating how it was identified (for review)\n",
    "df_deaminases['Deaminase_Evidence'] = ''\n",
    "if mask_broad_cat.any(): df_deaminases.loc[mask_broad_cat[combined_mask], 'Deaminase_Evidence'] += 'BroadFuncCat;'\n",
    "if mask_specific_cat.any(): df_deaminases.loc[mask_specific_cat[combined_mask], 'Deaminase_Evidence'] += 'SpecificFuncCat;'\n",
    "if mask_source_annot.any(): df_deaminases.loc[mask_source_annot[combined_mask], 'Deaminase_Evidence'] += 'SourceAnnot;'\n",
    "if mask_euk_hit_name.any(): df_deaminases.loc[mask_euk_hit_name[combined_mask], 'Deaminase_Evidence'] += 'EukHitName;'\n",
    "if mask_ipr.any(): df_deaminases.loc[mask_ipr[combined_mask], 'Deaminase_Evidence'] += 'IPR;'\n",
    "df_deaminases['Deaminase_Evidence'] = df_deaminases['Deaminase_Evidence'].str.rstrip(';')\n",
    "\n",
    "\n",
    "n_putative_deaminases = len(df_deaminases)\n",
    "print(f\"\\n--- Found {n_putative_deaminases} Putative Deaminase Proteins ---\")\n",
    "\n",
    "if n_putative_deaminases > 0:\n",
    "    # --- Characterize Putative Deaminases ---\n",
    "    print(\"\\n--- Characterization of Putative Deaminases ---\")\n",
    "\n",
    "    # 1. Distribution by Group (Asgard vs GV)\n",
    "    if group_col in df_deaminases.columns:\n",
    "        group_counts = df_deaminases[group_col].value_counts().reset_index()\n",
    "        group_counts.columns = ['Group', 'Count']\n",
    "        print(\"\\nDistribution by Group (Asgard/GV):\")\n",
    "        try: print(group_counts.to_markdown(index=False))\n",
    "        except ImportError: print(group_counts)\n",
    "        fig_group = px.bar(group_counts, x='Group', y='Count', color='Group', color_discrete_map=group_colors, title=\"Putative Deaminases by Group\")\n",
    "        fig_group.show()\n",
    "        fig_group.write_html(DEAMINASE_PLOT_OUTPUT_DIR / \"putative_deaminases_by_group.html\")\n",
    "\n",
    "    # 2. Broad Functional Category of these proteins\n",
    "    if broad_func_cat_col in df_deaminases.columns:\n",
    "        func_cat_counts = df_deaminases[broad_func_cat_col].value_counts().nlargest(15).reset_index() # Top 15\n",
    "        func_cat_counts.columns = [broad_func_cat_col, 'Count']\n",
    "        print(f\"\\nTop Broad Functional Categories of Putative Deaminases:\")\n",
    "        try: print(func_cat_counts.to_markdown(index=False))\n",
    "        except ImportError: print(func_cat_counts)\n",
    "        fig_func = px.bar(func_cat_counts, x='Count', y=broad_func_cat_col, orientation='h', color=broad_func_cat_col, color_discrete_map=broad_category_color_map,\n",
    "                          title=\"Functional Categories of Putative Deaminases\")\n",
    "        fig_func.update_layout(showlegend=False, yaxis_categoryorder='total ascending', height=max(400, len(func_cat_counts)*25))\n",
    "        fig_func.show()\n",
    "        fig_func.write_html(DEAMINASE_PLOT_OUTPUT_DIR / \"putative_deaminases_functional_categories.html\")\n",
    "\n",
    "    # 3. Eukaryotic Hits for these deaminases\n",
    "    if has_euk_hit_col in df_deaminases.columns:\n",
    "        df_deaminases_with_euk_hits = df_deaminases[df_deaminases[has_euk_hit_col] == True].copy()\n",
    "        print(f\"\\n{len(df_deaminases_with_euk_hits)} of these putative deaminases have a eukaryotic DIAMOND hit.\")\n",
    "        if not df_deaminases_with_euk_hits.empty:\n",
    "            # Top Euk Organisms hit by these deaminases\n",
    "            euk_org_hit_counts = df_deaminases_with_euk_hits[euk_hit_organism_col].value_counts().nlargest(15).reset_index()\n",
    "            euk_org_hit_counts.columns = [euk_hit_organism_col, 'Count']\n",
    "            print(f\"\\nTop Eukaryotic Organisms Hit by Putative Deaminases:\")\n",
    "            try: print(euk_org_hit_counts.to_markdown(index=False))\n",
    "            except ImportError: print(euk_org_hit_counts)\n",
    "            fig_euk_org = px.bar(euk_org_hit_counts, x='Count', y=euk_hit_organism_col, orientation='h', title=\"Top Euk. Organisms Hit by Putative Deaminases\")\n",
    "            fig_euk_org.update_layout(yaxis_categoryorder='total ascending', height=max(400, len(euk_org_hit_counts)*20))\n",
    "            fig_euk_org.show()\n",
    "            fig_euk_org.write_html(DEAMINASE_PLOT_OUTPUT_DIR / \"putative_deaminases_euk_org_hits.html\")\n",
    "\n",
    "            # Top Euk Protein Names hit by these deaminases\n",
    "            if euk_hit_protein_name_col in df_deaminases_with_euk_hits.columns:\n",
    "                df_deaminases_with_euk_hits['Cleaned_Euk_Hit_Name'] = df_deaminases_with_euk_hits[euk_hit_protein_name_col].apply(clean_protein_name)\n",
    "                euk_prot_name_counts = df_deaminases_with_euk_hits['Cleaned_Euk_Hit_Name'].value_counts().nlargest(20).reset_index()\n",
    "                euk_prot_name_counts.columns = ['Euk_Protein_Function', 'Count']\n",
    "                print(f\"\\nTop Eukaryotic Protein Functions Hit by Putative Deaminases:\")\n",
    "                try: print(euk_prot_name_counts.to_markdown(index=False))\n",
    "                except ImportError: print(euk_prot_name_counts)\n",
    "                fig_euk_func = px.bar(euk_prot_name_counts, x='Count', y='Euk_Protein_Function', orientation='h', title=\"Top Euk. Protein Functions Hit by Putative Deaminases\")\n",
    "                fig_euk_func.update_layout(yaxis_categoryorder='total ascending', height=max(400, len(euk_prot_name_counts)*20))\n",
    "                fig_euk_func.show()\n",
    "                fig_euk_func.write_html(DEAMINASE_PLOT_OUTPUT_DIR / \"putative_deaminases_euk_func_hits.html\")\n",
    "\n",
    "    # 4. Structural Darkness\n",
    "    if dark_col in df_deaminases.columns:\n",
    "        dark_counts = df_deaminases[dark_col].value_counts(normalize=True).mul(100).reset_index()\n",
    "        dark_counts.columns = ['Is_Structurally_Dark', 'Percentage']\n",
    "        print(f\"\\nStructural Darkness of Putative Deaminases:\")\n",
    "        try: print(dark_counts.to_markdown(index=False, floatfmt=\".1f\"))\n",
    "        except ImportError: print(dark_counts)\n",
    "        fig_dark = px.pie(dark_counts, names='Is_Structurally_Dark', values='Percentage', title=\"Structural Darkness of Putative Deaminases\")\n",
    "        fig_dark.show()\n",
    "        fig_dark.write_html(DEAMINASE_PLOT_OUTPUT_DIR / \"putative_deaminases_structural_darkness.html\")\n",
    "\n",
    "    # --- Save the identified deaminases to a CSV ---\n",
    "    output_csv_path = DEAMINASE_SUMMARY_DATA_DIR / \"putative_deaminase_proteins.csv\"\n",
    "    try:\n",
    "        # Select a subset of relevant columns for the output\n",
    "        cols_to_save_deaminase = [\n",
    "            protein_id_col, group_col, og_col, virus_name_col, asgard_phylum_col,\n",
    "            broad_func_cat_col, specific_func_cat_col, source_annot_col, ipr_col,\n",
    "            'Deaminase_Evidence', dark_col, is_esp_col,\n",
    "            has_euk_hit_col, euk_hit_sseqid_col, euk_hit_organism_col, euk_hit_protein_name_col,\n",
    "            euk_hit_pident_col, euk_hit_evalue_col, query_coverage_col, subject_coverage_col,\n",
    "            length_col, disorder_col, localization_col, sp_type_col, sequence_col\n",
    "        ]\n",
    "        # Filter for columns that actually exist in df_deaminases\n",
    "        existing_cols_to_save = [col for col in cols_to_save_deaminase if col in df_deaminases.columns]\n",
    "        df_deaminases[existing_cols_to_save].to_csv(output_csv_path, index=False, na_rep='NA')\n",
    "        print(f\"\\nSaved details of {n_putative_deaminases} putative deaminases to: {output_csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not save the putative deaminases CSV. Error: {e}\")\n",
    "else:\n",
    "    print(\"No putative deaminases found based on the defined criteria.\")\n",
    "\n",
    "print(\"\\n\\n--- Cell 22: Deaminase Exploration Complete ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b986064-66ae-46c1-9cf5-77f527ee0f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 23: Create Adenosine Deaminase Sub-database\n",
    "\n",
    "# This cell assumes Cell 1 (Setup for Deaminase Exploration) has been run for helper functions and ipr_lookup.\n",
    "# It also assumes Cell 22 has been run to generate \"putative_deaminase_proteins.csv\".\n",
    "# Required variables from Cell 1:\n",
    "# - DEAMINASE_SUMMARY_DATA_DIR (Path object for input/output)\n",
    "# - protein_id_col, group_col, broad_func_cat_col, specific_func_cat_col, \n",
    "#   source_annot_col, ipr_col, euk_hit_protein_name_col, has_euk_hit_col,\n",
    "#   virus_name_col, asgard_phylum_col, dark_col, is_esp_col,\n",
    "#   euk_hit_sseqid_col, euk_hit_organism_col, euk_hit_pident_col, euk_hit_evalue_col,\n",
    "#   query_coverage_col, subject_coverage_col, length_col, disorder_col, localization_col,\n",
    "#   sp_type_col, sequence_col\n",
    "# - ipr_lookup (dictionary for IPR descriptions)\n",
    "# - clean_protein_name (helper function)\n",
    "\n",
    "print(\"--- Cell 23: Creating Adenosine Deaminase Sub-database ---\")\n",
    "\n",
    "if 'DEAMINASE_SUMMARY_DATA_DIR' not in locals():\n",
    "    raise NameError(\"DEAMINASE_SUMMARY_DATA_DIR is not defined. Please run Cell 1 and Cell 22 setup first.\")\n",
    "if 'ipr_lookup' not in locals():\n",
    "    print(\"WARNING: ipr_lookup not found. IPR descriptions might be limited.\")\n",
    "    ipr_lookup = {} # Define as empty to prevent errors, though descriptions will be missing\n",
    "if 'clean_protein_name' not in locals():\n",
    "    raise NameError(\"Helper function 'clean_protein_name' is not defined. Please run Cell 1 setup.\")\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "PUTATIVE_DEAMINASES_CSV_PATH = DEAMINASE_SUMMARY_DATA_DIR / \"putative_deaminase_proteins.csv\"\n",
    "OUTPUT_ADENOSINE_DEAMINASE_CSV_PATH = DEAMINASE_SUMMARY_DATA_DIR / \"adenosine_deaminase_sub_database.csv\"\n",
    "\n",
    "# --- Define Adenosine Deaminase-Specific Keywords and IPR IDs ---\n",
    "adenosine_deaminase_keywords = [\n",
    "    'adenosine deaminase', 'adar', 'tad', 'adenosine aminohydrolase',\n",
    "    'adenine deaminase', 'amp deaminase' # AMP deaminase is related\n",
    "]\n",
    "# More specific IPRs for Adenosine Deaminases\n",
    "adenosine_deaminase_ipr_ids = {\n",
    "    'IPR000335', # Adenosine deaminase\n",
    "    'IPR013659', # Adenosine/AMP deaminase N-terminal domain\n",
    "    'IPR006330', # Adenosine/adenine deaminase catalytic domain\n",
    "    'IPR006329', # AMP deaminase\n",
    "    'IPR001365', # Adenosine deaminase domain\n",
    "    'IPR002466', # Adenosine deaminase/editase\n",
    "    'IPR013659', # Adenosine/AMP deaminase N-terminal\n",
    "    'IPR006330', # Adenosine/adenine deaminase\n",
    "    'IPR006329', # AMP deaminase\n",
    "    'IPR006331', # Adenosine deaminase-related growth factor\n",
    "    'IPR028893', # Adenisone deaminase\n",
    "    'IPR042935', # tRNA-specific adenosine deaminase 1\n",
    "    'IPR012839', # Adenosine deaminase, A.thaliana-like\n",
    "    'IPR016292', # ADAR, dsRNA adenosine deaminase\n",
    "    'IPR042935', # tRNA-specific adenosine deaminase 1\n",
    "    'IPR012839', # Adenosine deaminase, A.thaliana-like\n",
    "    'IPR016292', # ADAR, dsRNA adenosine deaminase catalytic domain\n",
    "    'IPR036618', # Deaminase TadA-like domain (TadA is tRNA adenosine deaminase)\n",
    "    'IPR002466', # Adenosine deaminase/editase domain\n",
    "    'IPR001365', # Adenosine deaminase domain\n",
    "    # Consider adding IPRs for ADATs (tRNA-specific adenosine deaminases) if relevant\n",
    "}\n",
    "print(f\"Searching for adenosine deaminases using {len(adenosine_deaminase_keywords)} keywords and {len(adenosine_deaminase_ipr_ids)} IPR IDs.\")\n",
    "\n",
    "# --- Load the Putative Deaminase Database ---\n",
    "print(f\"\\n--- Loading Putative Deaminase Data from: {PUTATIVE_DEAMINASES_CSV_PATH} ---\")\n",
    "if not PUTATIVE_DEAMINASES_CSV_PATH.is_file():\n",
    "    raise FileNotFoundError(f\"ERROR: Putative deaminases CSV file not found at '{PUTATIVE_DEAMINASES_CSV_PATH}'. Please run Cell 22 first.\")\n",
    "\n",
    "try:\n",
    "    df_putative_deaminases = pd.read_csv(PUTATIVE_DEAMINASES_CSV_PATH, low_memory=False)\n",
    "    print(f\"Successfully loaded {len(df_putative_deaminases)} putative deaminases.\")\n",
    "    # Ensure key columns are present and fill NaNs for string columns used in search\n",
    "    for col in [broad_func_cat_col, specific_func_cat_col, source_annot_col, ipr_col, euk_hit_protein_name_col, 'Deaminase_Evidence']:\n",
    "        if col in df_putative_deaminases.columns:\n",
    "            df_putative_deaminases[col] = df_putative_deaminases[col].fillna('').astype(str)\n",
    "        else:\n",
    "            print(f\"Warning: Column '{col}' not found in loaded putative deaminases. Search in this field will be skipped.\")\n",
    "            df_putative_deaminases[col] = '' # Add as empty string to prevent errors\n",
    "    if has_euk_hit_col in df_putative_deaminases.columns:\n",
    "         df_putative_deaminases[has_euk_hit_col] = df_putative_deaminases[has_euk_hit_col].fillna(False).astype(bool)\n",
    "    else:\n",
    "        print(f\"Warning: Column '{has_euk_hit_col}' not found. Assuming no euk hits for filtering.\")\n",
    "        df_putative_deaminases[has_euk_hit_col] = False\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading '{PUTATIVE_DEAMINASES_CSV_PATH}': {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Filter for Adenosine Deaminases ---\n",
    "print(\"\\n--- Filtering for Adenosine Deaminases ---\")\n",
    "\n",
    "# 1. Search in Broad Functional Category\n",
    "mask_ad_broad_cat = df_putative_deaminases[broad_func_cat_col].str.contains('|'.join(adenosine_deaminase_keywords), case=False, na=False)\n",
    "\n",
    "# 2. Search in Specific Functional Category\n",
    "mask_ad_specific_cat = pd.Series(False, index=df_putative_deaminases.index)\n",
    "if specific_func_cat_col in df_putative_deaminases.columns:\n",
    "    mask_ad_specific_cat = df_putative_deaminases[specific_func_cat_col].str.contains('|'.join(adenosine_deaminase_keywords), case=False, na=False)\n",
    "\n",
    "# 3. Search in Source Protein Annotation\n",
    "mask_ad_source_annot = df_putative_deaminases[source_annot_col].str.contains('|'.join(adenosine_deaminase_keywords), case=False, na=False)\n",
    "\n",
    "# 4. Search in Eukaryotic Hit Protein Name (if hit exists)\n",
    "mask_ad_euk_hit_name = pd.Series(False, index=df_putative_deaminases.index)\n",
    "if euk_hit_protein_name_col in df_putative_deaminases.columns and has_euk_hit_col in df_putative_deaminases.columns:\n",
    "    # Apply clean_protein_name before searching\n",
    "    idx_with_euk_hits = df_putative_deaminases[df_putative_deaminases[has_euk_hit_col]].index\n",
    "    if not idx_with_euk_hits.empty:\n",
    "        cleaned_euk_names_ad = df_putative_deaminases.loc[idx_with_euk_hits, euk_hit_protein_name_col].astype(str).apply(clean_protein_name)\n",
    "        mask_ad_euk_hit_name.loc[idx_with_euk_hits] = cleaned_euk_names_ad.str.contains('|'.join(adenosine_deaminase_keywords), case=False, na=False)\n",
    "\n",
    "# 5. Search in IPR Signatures (IDs and Descriptions)\n",
    "mask_ad_ipr = pd.Series(False, index=df_putative_deaminases.index)\n",
    "if ipr_col in df_putative_deaminases.columns:\n",
    "    ipr_hits_indices_ad = []\n",
    "    for index, row in df_putative_deaminases.iterrows():\n",
    "        if row[ipr_col].strip(): # Check if not empty\n",
    "            protein_ipr_ids = set(pid.strip() for pid in row[ipr_col].split(';') if pid.strip())\n",
    "            if not protein_ipr_ids.isdisjoint(adenosine_deaminase_ipr_ids):\n",
    "                ipr_hits_indices_ad.append(index)\n",
    "                continue\n",
    "            for ipr_id_in_protein in protein_ipr_ids:\n",
    "                ipr_data = ipr_lookup.get(ipr_id_in_protein)\n",
    "                if ipr_data and 'Name' in ipr_data:\n",
    "                    if any(keyword in ipr_data['Name'].lower() for keyword in adenosine_deaminase_keywords):\n",
    "                        ipr_hits_indices_ad.append(index)\n",
    "                        break \n",
    "    if ipr_hits_indices_ad:\n",
    "        mask_ad_ipr.loc[list(set(ipr_hits_indices_ad))] = True\n",
    "\n",
    "# Combine masks\n",
    "combined_ad_mask = mask_ad_broad_cat | mask_ad_specific_cat | mask_ad_source_annot | mask_ad_euk_hit_name | mask_ad_ipr\n",
    "df_adenosine_deaminases = df_putative_deaminases[combined_ad_mask].copy()\n",
    "\n",
    "# Add a column for adenosine deaminase specific evidence\n",
    "df_adenosine_deaminases['Adenosine_Deaminase_Evidence'] = ''\n",
    "if mask_ad_broad_cat.any(): df_adenosine_deaminases.loc[mask_ad_broad_cat[combined_ad_mask], 'Adenosine_Deaminase_Evidence'] += 'AD_BroadFuncCat;'\n",
    "if mask_ad_specific_cat.any(): df_adenosine_deaminases.loc[mask_ad_specific_cat[combined_ad_mask], 'Adenosine_Deaminase_Evidence'] += 'AD_SpecificFuncCat;'\n",
    "if mask_ad_source_annot.any(): df_adenosine_deaminases.loc[mask_ad_source_annot[combined_ad_mask], 'Adenosine_Deaminase_Evidence'] += 'AD_SourceAnnot;'\n",
    "if mask_ad_euk_hit_name.any(): df_adenosine_deaminases.loc[mask_ad_euk_hit_name[combined_ad_mask], 'Adenosine_Deaminase_Evidence'] += 'AD_EukHitName;'\n",
    "if mask_ad_ipr.any(): df_adenosine_deaminases.loc[mask_ad_ipr[combined_ad_mask], 'Adenosine_Deaminase_Evidence'] += 'AD_IPR;'\n",
    "df_adenosine_deaminases['Adenosine_Deaminase_Evidence'] = df_adenosine_deaminases['Adenosine_Deaminase_Evidence'].str.rstrip(';')\n",
    "\n",
    "\n",
    "n_adenosine_deaminases = len(df_adenosine_deaminases)\n",
    "print(f\"\\n--- Found {n_adenosine_deaminases} Putative Adenosine Deaminase Proteins ---\")\n",
    "\n",
    "if n_adenosine_deaminases > 0:\n",
    "    # --- Select Columns for the Sub-database ---\n",
    "    # These columns should mostly exist in putative_deaminase_proteins.csv\n",
    "    # The 'Deaminase_Evidence' column is from the broader search, 'Adenosine_Deaminase_Evidence' is specific.\n",
    "    sub_db_cols = [\n",
    "        protein_id_col, group_col, og_col, virus_name_col, asgard_phylum_col,\n",
    "        source_annot_col, broad_func_cat_col, specific_func_cat_col, \n",
    "        ipr_col, 'Deaminase_Evidence', 'Adenosine_Deaminase_Evidence', \n",
    "        dark_col, is_esp_col,\n",
    "        has_euk_hit_col, euk_hit_sseqid_col, euk_hit_organism_col, euk_hit_protein_name_col,\n",
    "        euk_hit_pident_col, euk_hit_evalue_col, query_coverage_col, subject_coverage_col,\n",
    "        length_col, disorder_col, localization_col, sp_type_col,\n",
    "        sequence_col # Include sequence if present and desired\n",
    "    ]\n",
    "    # Filter for columns that actually exist in the df_adenosine_deaminases DataFrame\n",
    "    existing_sub_db_cols = [col for col in sub_db_cols if col in df_adenosine_deaminases.columns]\n",
    "    df_adenosine_deaminase_sub_db = df_adenosine_deaminases[existing_sub_db_cols]\n",
    "\n",
    "    # --- Save the Sub-database ---\n",
    "    try:\n",
    "        df_adenosine_deaminase_sub_db.to_csv(OUTPUT_ADENOSINE_DEAMINASE_CSV_PATH, index=False, na_rep='NA')\n",
    "        print(f\"\\nSuccessfully saved adenosine deaminase sub-database ({len(df_adenosine_deaminase_sub_db)} entries) to: {OUTPUT_ADENOSINE_DEAMINASE_CSV_PATH}\")\n",
    "        \n",
    "        print(\"\\nFirst 5 rows of the Adenosine Deaminase Sub-database:\")\n",
    "        try:\n",
    "            print(df_adenosine_deaminase_sub_db.head().to_markdown(index=False, floatfmt=\".2f\"))\n",
    "        except ImportError:\n",
    "            print(df_adenosine_deaminase_sub_db.head())\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not save the adenosine deaminase sub-database. Error: {e}\")\n",
    "else:\n",
    "    print(\"No putative adenosine deaminases found based on the specific criteria.\")\n",
    "\n",
    "print(\"\\n\\n--- Cell 23: Adenosine Deaminase Sub-database Creation Complete ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2386b278-ebb7-4079-a0b9-f86b4a13ed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 24: Comprehensive Visualization of the Adenosine Deaminase Sub-database (with SVG Export)\n",
    "\n",
    "# This cell assumes Cell 1 (Setup for Deaminase Exploration) has been run.\n",
    "# Required variables from Cell 1:\n",
    "# - DEAMINASE_PLOT_OUTPUT_DIR, DEAMINASE_SUMMARY_DATA_DIR\n",
    "# - protein_id_col, group_col, broad_func_cat_col, specific_func_cat_col, \n",
    "#   source_annot_col, ipr_col, euk_hit_protein_name_col, has_euk_hit_col,\n",
    "#   dark_col, is_esp_col, length_col, disorder_col, localization_col,\n",
    "#   euk_hit_organism_col, euk_hit_pident_col, query_coverage_col, subject_coverage_col\n",
    "# - ipr_lookup, clean_protein_name, group_colors, broad_category_color_map, \n",
    "#   arcadia_primary_palette, arcadia_secondary_palette, arcadia_neutrals_palette\n",
    "# - plotly_layout_defaults (implicitly used by pio.templates.default)\n",
    "\n",
    "print(\"--- Cell 24: Visualizing the Adenosine Deaminase Sub-database (with SVG Export) ---\")\n",
    "\n",
    "if 'DEAMINASE_SUMMARY_DATA_DIR' not in locals():\n",
    "    raise NameError(\"DEAMINASE_SUMMARY_DATA_DIR is not defined. Please run Cell 1 setup first.\")\n",
    "if 'ipr_lookup' not in locals():\n",
    "    print(\"WARNING: ipr_lookup not found. IPR descriptions might be limited.\")\n",
    "    ipr_lookup = {} \n",
    "if 'clean_protein_name' not in locals():\n",
    "    raise NameError(\"Helper function 'clean_protein_name' is not defined. Please run Cell 1 setup.\")\n",
    "if 'DEAMINASE_PLOT_OUTPUT_DIR' not in locals(): # Ensure plot output dir is defined\n",
    "    raise NameError(\"DEAMINASE_PLOT_OUTPUT_DIR is not defined. Please run Cell 1 setup first.\")\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "ADENOSINE_DEAMINASE_SUB_DB_PATH = DEAMINASE_SUMMARY_DATA_DIR / \"adenosine_deaminase_sub_database.csv\"\n",
    "TOP_N_DISPLAY = 15 # For bar charts\n",
    "\n",
    "# --- Load the Adenosine Deaminase Sub-database ---\n",
    "print(f\"\\n--- Loading Adenosine Deaminase Sub-database from: {ADENOSINE_DEAMINASE_SUB_DB_PATH} ---\")\n",
    "if not ADENOSINE_DEAMINASE_SUB_DB_PATH.is_file():\n",
    "    raise FileNotFoundError(f\"ERROR: Adenosine Deaminase sub-database file not found at '{ADENOSINE_DEAMINASE_SUB_DB_PATH}'. Please run Cell 23 first.\")\n",
    "\n",
    "try:\n",
    "    df_ad = pd.read_csv(ADENOSINE_DEAMINASE_SUB_DB_PATH, low_memory=False)\n",
    "    print(f\"Successfully loaded {len(df_ad)} putative adenosine deaminases.\")\n",
    "    # Basic type conversions for safety\n",
    "    for col in [group_col, broad_func_cat_col, specific_func_cat_col, 'Adenosine_Deaminase_Evidence', localization_col, euk_hit_organism_col]:\n",
    "        if col in df_ad.columns:\n",
    "            df_ad[col] = df_ad[col].fillna('Unknown').astype('category')\n",
    "    for col in [ipr_col, source_annot_col, euk_hit_protein_name_col]:\n",
    "        if col in df_ad.columns:\n",
    "            df_ad[col] = df_ad[col].fillna('').astype(str)\n",
    "    for col in [has_euk_hit_col, dark_col, is_esp_col]:\n",
    "        if col in df_ad.columns:\n",
    "            df_ad[col] = df_ad[col].fillna(False).astype(bool)\n",
    "    for col in [length_col, disorder_col, euk_hit_pident_col, query_coverage_col, subject_coverage_col]:\n",
    "        if col in df_ad.columns:\n",
    "            df_ad[col] = pd.to_numeric(df_ad[col], errors='coerce')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading '{ADENOSINE_DEAMINASE_SUB_DB_PATH}': {e}\")\n",
    "    raise\n",
    "\n",
    "if df_ad.empty:\n",
    "    print(\"Adenosine deaminase sub-database is empty. No visualizations will be generated.\")\n",
    "else:\n",
    "    # --- 1. Overall Summary ---\n",
    "    print(f\"\\n--- 1. Overall Summary of {len(df_ad)} Putative Adenosine Deaminases ---\")\n",
    "\n",
    "    # A. Distribution by Group (Asgard vs GV)\n",
    "    if group_col in df_ad.columns:\n",
    "        ad_group_counts = df_ad[group_col].value_counts().reset_index()\n",
    "        ad_group_counts.columns = ['Group', 'Count']\n",
    "        print(\"\\nDistribution by Group:\")\n",
    "        try: print(ad_group_counts.to_markdown(index=False))\n",
    "        except ImportError: print(ad_group_counts)\n",
    "        fig_ad_group = px.bar(ad_group_counts, x='Group', y='Count', color='Group', \n",
    "                              color_discrete_map=group_colors, title=\"Adenosine Deaminases by Group\")\n",
    "        fig_ad_group.show()\n",
    "        fig_ad_group.write_html(DEAMINASE_PLOT_OUTPUT_DIR / \"ad_deaminase_by_group.html\")\n",
    "        try:\n",
    "            fig_ad_group.write_image(DEAMINASE_PLOT_OUTPUT_DIR / \"ad_deaminase_by_group.svg\")\n",
    "            print(f\"SVG saved: ad_deaminase_by_group.svg\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not export ad_deaminase_by_group to SVG. Ensure 'kaleido' is installed. Error: {e}\")\n",
    "\n",
    "\n",
    "    # B. Distribution by Adenosine_Deaminase_Evidence\n",
    "    if 'Adenosine_Deaminase_Evidence' in df_ad.columns:\n",
    "        s = df_ad['Adenosine_Deaminase_Evidence'].str.split(';').explode()\n",
    "        evidence_counts = s.value_counts().nlargest(TOP_N_DISPLAY).reset_index()\n",
    "        evidence_counts.columns = ['Evidence_Type', 'Count']\n",
    "        print(\"\\nDistribution by Identification Evidence:\")\n",
    "        try: print(evidence_counts.to_markdown(index=False))\n",
    "        except ImportError: print(evidence_counts)\n",
    "        fig_ad_evidence = px.bar(evidence_counts, x='Count', y='Evidence_Type', orientation='h',\n",
    "                                 title=\"Adenosine Deaminase Identification Evidence\")\n",
    "        fig_ad_evidence.update_layout(yaxis_categoryorder='total ascending', height=max(300, len(evidence_counts)*25))\n",
    "        fig_ad_evidence.show()\n",
    "        fig_ad_evidence.write_html(DEAMINASE_PLOT_OUTPUT_DIR / \"ad_deaminase_by_evidence.html\")\n",
    "        try:\n",
    "            fig_ad_evidence.write_image(DEAMINASE_PLOT_OUTPUT_DIR / \"ad_deaminase_by_evidence.svg\")\n",
    "            print(f\"SVG saved: ad_deaminase_by_evidence.svg\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not export ad_deaminase_by_evidence to SVG. Error: {e}\")\n",
    "\n",
    "    # --- 2. Functional Characterization ---\n",
    "    print(f\"\\n--- 2. Functional Characterization ---\")\n",
    "    # A. Broad Functional Categories\n",
    "    if broad_func_cat_col in df_ad.columns:\n",
    "        ad_func_cat_counts = df_ad[broad_func_cat_col].value_counts().nlargest(TOP_N_DISPLAY).reset_index()\n",
    "        ad_func_cat_counts.columns = [broad_func_cat_col, 'Count']\n",
    "        print(f\"\\nTop Broad Functional Categories:\")\n",
    "        try: print(ad_func_cat_counts.to_markdown(index=False))\n",
    "        except ImportError: print(ad_func_cat_counts)\n",
    "        fig_ad_func = px.bar(ad_func_cat_counts, x='Count', y=broad_func_cat_col, orientation='h',\n",
    "                             color=broad_func_cat_col, color_discrete_map=broad_category_color_map,\n",
    "                             title=\"Broad Functional Categories of Adenosine Deaminases\")\n",
    "        fig_ad_func.update_layout(showlegend=False, yaxis_categoryorder='total ascending', height=max(400, len(ad_func_cat_counts)*25))\n",
    "        fig_ad_func.show()\n",
    "        fig_ad_func.write_html(DEAMINASE_PLOT_OUTPUT_DIR / \"ad_deaminase_broad_func_cat.html\")\n",
    "        try:\n",
    "            fig_ad_func.write_image(DEAMINASE_PLOT_OUTPUT_DIR / \"ad_deaminase_broad_func_cat.svg\")\n",
    "            print(f\"SVG saved: ad_deaminase_broad_func_cat.svg\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not export ad_deaminase_broad_func_cat to SVG. Error: {e}\")\n",
    "\n",
    "    # B. Top IPR Signatures\n",
    "    if ipr_col in df_ad.columns and ipr_lookup:\n",
    "        all_iprs_ad = []\n",
    "        for ipr_string in df_ad[ipr_col].dropna():\n",
    "            all_iprs_ad.extend([ipr.strip() for ipr in ipr_string.split(';') if ipr.strip()])\n",
    "        ipr_counts_ad = Counter(all_iprs_ad)\n",
    "        top_iprs_data = []\n",
    "        for ipr_id, count in ipr_counts_ad.most_common(TOP_N_DISPLAY):\n",
    "            desc = ipr_lookup.get(ipr_id, {}).get('Name', 'Unknown IPR')\n",
    "            desc = desc[:60] + '...' if len(desc) > 63 else desc\n",
    "            top_iprs_data.append({'IPR_Domain': f\"{desc} ({ipr_id})\", 'Count': count, 'IPR_ID': ipr_id})\n",
    "        if top_iprs_data:\n",
    "            df_top_iprs_ad = pd.DataFrame(top_iprs_data)\n",
    "            print(f\"\\nTop IPR Domains/Families in Adenosine Deaminases:\")\n",
    "            try: print(df_top_iprs_ad[['IPR_Domain', 'Count']].to_markdown(index=False))\n",
    "            except ImportError: print(df_top_iprs_ad[['IPR_Domain', 'Count']])\n",
    "            fig_ad_ipr = px.bar(df_top_iprs_ad, x='Count', y='IPR_Domain', orientation='h',\n",
    "                                title=\"Top IPR Domains in Putative Adenosine Deaminases\")\n",
    "            fig_ad_ipr.update_layout(yaxis_categoryorder='total ascending', height=max(400, len(df_top_iprs_ad)*25))\n",
    "            fig_ad_ipr.show()\n",
    "            fig_ad_ipr.write_html(DEAMINASE_PLOT_OUTPUT_DIR / \"ad_deaminase_top_ipr.html\")\n",
    "            try:\n",
    "                fig_ad_ipr.write_image(DEAMINASE_PLOT_OUTPUT_DIR / \"ad_deaminase_top_ipr.svg\")\n",
    "                print(f\"SVG saved: ad_deaminase_top_ipr.svg\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not export ad_deaminase_top_ipr to SVG. Error: {e}\")\n",
    "\n",
    "    # --- 3. Eukaryotic Homology ---\n",
    "    print(f\"\\n--- 3. Eukaryotic Homology of Adenosine Deaminases ---\")\n",
    "    if has_euk_hit_col in df_ad.columns:\n",
    "        df_ad_with_euk_hits = df_ad[df_ad[has_euk_hit_col] == True].copy()\n",
    "        n_ad_with_euk_hits = len(df_ad_with_euk_hits)\n",
    "        print(f\"{n_ad_with_euk_hits} of {len(df_ad)} putative adenosine deaminases have a eukaryotic DIAMOND hit.\")\n",
    "        if n_ad_with_euk_hits > 0:\n",
    "            ad_euk_org_counts = df_ad_with_euk_hits[euk_hit_organism_col].value_counts().nlargest(TOP_N_DISPLAY).reset_index()\n",
    "            ad_euk_org_counts.columns = [euk_hit_organism_col, 'Count']\n",
    "            fig_ad_euk_org = px.bar(ad_euk_org_counts, x='Count', y=euk_hit_organism_col, orientation='h', \n",
    "                                    title=\"Top Euk. Organisms Hit by Adenosine Deaminases\")\n",
    "            fig_ad_euk_org.update_layout(yaxis_categoryorder='total ascending', height=max(400, len(ad_euk_org_counts)*20))\n",
    "            fig_ad_euk_org.show()\n",
    "            fig_ad_euk_org.write_html(DEAMINASE_PLOT_OUTPUT_DIR / \"ad_deaminase_euk_org_hits.html\")\n",
    "            try:\n",
    "                fig_ad_euk_org.write_image(DEAMINASE_PLOT_OUTPUT_DIR / \"ad_deaminase_euk_org_hits.svg\")\n",
    "                print(f\"SVG saved: ad_deaminase_euk_org_hits.svg\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not export ad_deaminase_euk_org_hits to SVG. Error: {e}\")\n",
    "\n",
    "            if euk_hit_protein_name_col in df_ad_with_euk_hits.columns:\n",
    "                df_ad_with_euk_hits['Cleaned_Euk_Hit_Name_AD'] = df_ad_with_euk_hits[euk_hit_protein_name_col].apply(clean_protein_name)\n",
    "                ad_euk_prot_counts = df_ad_with_euk_hits['Cleaned_Euk_Hit_Name_AD'].value_counts().nlargest(TOP_N_DISPLAY).reset_index()\n",
    "                ad_euk_prot_counts.columns = ['Euk_Protein_Function', 'Count']\n",
    "                fig_ad_euk_func = px.bar(ad_euk_prot_counts, x='Count', y='Euk_Protein_Function', orientation='h',\n",
    "                                         title=\"Top Euk. Protein Functions Hit by Adenosine Deaminases\")\n",
    "                fig_ad_euk_func.update_layout(yaxis_categoryorder='total ascending', height=max(400, len(ad_euk_prot_counts)*20))\n",
    "                fig_ad_euk_func.show()\n",
    "                fig_ad_euk_func.write_html(DEAMINASE_PLOT_OUTPUT_DIR / \"ad_deaminase_euk_func_hits.html\")\n",
    "                try:\n",
    "                    fig_ad_euk_func.write_image(DEAMINASE_PLOT_OUTPUT_DIR / \"ad_deaminase_euk_func_hits.svg\")\n",
    "                    print(f\"SVG saved: ad_deaminase_euk_func_hits.svg\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not export ad_deaminase_euk_func_hits to SVG. Error: {e}\")\n",
    "\n",
    "            hit_quality_cols = {euk_hit_pident_col: 'Euk Hit PIDENT (%)', query_coverage_col: 'Query Coverage', subject_coverage_col: 'Subject Coverage'}\n",
    "            for col, label in hit_quality_cols.items():\n",
    "                if col in df_ad_with_euk_hits.columns:\n",
    "                    fig_qual_filename_base = f\"ad_deaminase_euk_hit_{col.replace('(', '').replace(')', '').replace('%', 'pct').replace(' ', '_').lower()}_dist\"\n",
    "                    fig_qual = px.histogram(df_ad_with_euk_hits.dropna(subset=[col]), x=col, color=group_col,\n",
    "                                            marginal=\"box\", barmode=\"overlay\", title=f\"Distribution of {label} for Adenosine Deaminase Euk. Hits\")\n",
    "                    fig_qual.update_traces(opacity=0.75)\n",
    "                    fig_qual.show()\n",
    "                    fig_qual.write_html(DEAMINASE_PLOT_OUTPUT_DIR / f\"{fig_qual_filename_base}.html\")\n",
    "                    try:\n",
    "                        fig_qual.write_image(DEAMINASE_PLOT_OUTPUT_DIR / f\"{fig_qual_filename_base}.svg\")\n",
    "                        print(f\"SVG saved: {fig_qual_filename_base}.svg\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Could not export {fig_qual_filename_base} to SVG. Error: {e}\")\n",
    "    else:\n",
    "        print(\"No adenosine deaminases found with eukaryotic hits.\")\n",
    "\n",
    "    # --- 4. Other Protein Features ---\n",
    "    print(f\"\\n--- 4. Other Features of Putative Adenosine Deaminases ---\")\n",
    "    if dark_col in df_ad.columns:\n",
    "        ad_dark_counts = df_ad[dark_col].value_counts(normalize=True).mul(100).reset_index()\n",
    "        ad_dark_counts.columns = ['Is_Structurally_Dark', 'Percentage']\n",
    "        fig_ad_dark = px.pie(ad_dark_counts, names='Is_Structurally_Dark', values='Percentage', title=\"Structural Darkness of Adenosine Deaminases\")\n",
    "        fig_ad_dark.show()\n",
    "        fig_ad_dark.write_html(DEAMINASE_PLOT_OUTPUT_DIR / \"ad_deaminase_structural_darkness.html\")\n",
    "        try:\n",
    "            fig_ad_dark.write_image(DEAMINASE_PLOT_OUTPUT_DIR / \"ad_deaminase_structural_darkness.svg\")\n",
    "            print(f\"SVG saved: ad_deaminase_structural_darkness.svg\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not export ad_deaminase_structural_darkness to SVG. Error: {e}\")\n",
    "\n",
    "    if is_esp_col in df_ad.columns and group_col in df_ad.columns:\n",
    "        df_ad_asgard = df_ad[df_ad[group_col] == 'Asgard']\n",
    "        if not df_ad_asgard.empty:\n",
    "            ad_esp_counts = df_ad_asgard[is_esp_col].value_counts().reset_index()\n",
    "            ad_esp_counts.columns = ['Is_ESP', 'Count']\n",
    "            fig_ad_esp = px.bar(ad_esp_counts, x='Is_ESP', y='Count', color='Is_ESP', title=\"ESP Status of Asgard Adenosine Deaminases\")\n",
    "            fig_ad_esp.show()\n",
    "            fig_ad_esp.write_html(DEAMINASE_PLOT_OUTPUT_DIR / \"ad_deaminase_asgard_esp_status.html\")\n",
    "            try:\n",
    "                fig_ad_esp.write_image(DEAMINASE_PLOT_OUTPUT_DIR / \"ad_deaminase_asgard_esp_status.svg\")\n",
    "                print(f\"SVG saved: ad_deaminase_asgard_esp_status.svg\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not export ad_deaminase_asgard_esp_status to SVG. Error: {e}\")\n",
    "\n",
    "    if length_col in df_ad.columns:\n",
    "        fig_ad_len = px.histogram(df_ad.dropna(subset=[length_col]), x=length_col, color=group_col, marginal=\"box\",\n",
    "                                  title=\"Length Distribution of Adenosine Deaminases\")\n",
    "        fig_ad_len.show()\n",
    "        fig_ad_len.write_html(DEAMINASE_PLOT_OUTPUT_DIR / \"ad_deaminase_length_distribution.html\")\n",
    "        try:\n",
    "            fig_ad_len.write_image(DEAMINASE_PLOT_OUTPUT_DIR / \"ad_deaminase_length_distribution.svg\")\n",
    "            print(f\"SVG saved: ad_deaminase_length_distribution.svg\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not export ad_deaminase_length_distribution to SVG. Error: {e}\")\n",
    "\n",
    "    if disorder_col in df_ad.columns:\n",
    "        fig_ad_dis = px.histogram(df_ad.dropna(subset=[disorder_col]), x=disorder_col, color=group_col, marginal=\"box\",\n",
    "                                   title=\"Disorder (%) Distribution of Adenosine Deaminases\")\n",
    "        fig_ad_dis.show()\n",
    "        fig_ad_dis.write_html(DEAMINASE_PLOT_OUTPUT_DIR / \"ad_deaminase_disorder_distribution.html\")\n",
    "        try:\n",
    "            fig_ad_dis.write_image(DEAMINASE_PLOT_OUTPUT_DIR / \"ad_deaminase_disorder_distribution.svg\")\n",
    "            print(f\"SVG saved: ad_deaminase_disorder_distribution.svg\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not export ad_deaminase_disorder_distribution to SVG. Error: {e}\")\n",
    "\n",
    "    if localization_col in df_ad.columns:\n",
    "        ad_loc_counts = df_ad.groupby(group_col)[localization_col].value_counts(normalize=True).mul(100).rename('Percentage').reset_index()\n",
    "        fig_ad_loc = px.bar(ad_loc_counts, x=localization_col, y='Percentage', color=group_col, barmode='group',\n",
    "                            title=\"Predicted Subcellular Localization of Adenosine Deaminases\",\n",
    "                            category_orders={localization_col: sorted(df_ad[localization_col].cat.categories.tolist())}) \n",
    "        fig_ad_loc.update_xaxes(tickangle=30)\n",
    "        fig_ad_loc.show()\n",
    "        fig_ad_loc.write_html(DEAMINASE_PLOT_OUTPUT_DIR / \"ad_deaminase_localization.html\")\n",
    "        try:\n",
    "            fig_ad_loc.write_image(DEAMINASE_PLOT_OUTPUT_DIR / \"ad_deaminase_localization.svg\")\n",
    "            print(f\"SVG saved: ad_deaminase_localization.svg\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not export ad_deaminase_localization to SVG. Error: {e}\")\n",
    "\n",
    "print(\"\\n\\n--- Cell 24: Adenosine Deaminase Sub-database Visualization (with SVG Export) Complete ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a3d1f5-a81e-4838-b336-31e6629f84dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (asgard_gv_env)",
   "language": "python",
   "name": "asgard_gv_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
