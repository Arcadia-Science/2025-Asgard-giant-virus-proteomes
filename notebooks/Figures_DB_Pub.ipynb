{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910f65b1-5650-4237-af59-5c442399e1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup for Figures Notebook - Load Data and Define Helpers\n",
    "\n",
    "# --- Standard Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from collections import Counter\n",
    "import glob # For finding files in a folder\n",
    "import re # For parsing organism names\n",
    "import sys # For exit\n",
    "import logging # For detailed logging\n",
    "from pathlib import Path # For handling paths\n",
    "# Ensure Biopython is installed: pip install biopython\n",
    "try:\n",
    "    from Bio import SeqIO\n",
    "except ImportError:\n",
    "    print(\"ERROR: Biopython is required for this script. Please install it: pip install biopython\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# --- Plotly Imports ---\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "# --- Setup Logging ---\n",
    "# Get a named logger for this notebook\n",
    "logger = logging.getLogger(__name__)\n",
    "# Prevent adding handlers multiple times if script is re-run in interactive session\n",
    "if not logger.hasHandlers():\n",
    "    logger.setLevel(logging.INFO)\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    log_formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "    console_handler.setFormatter(log_formatter)\n",
    "    logger.addHandler(console_handler)\n",
    "else:\n",
    "    # Ensure level is set if handlers already exist (e.g., in notebook re-run)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "print(\"--- Figures Notebook Setup ---\")\n",
    "\n",
    "# --- Configuration ---\n",
    "# INPUT file path: Use the latest database version that includes Euk annotations,\n",
    "# DIAMOND hit details, coverage columns, RBH flag, and potentially merged APSI/Motif data.\n",
    "# Make sure this path points to your most recent, complete database file!\n",
    "database_path = 'proteome_database_v2.3.csv' # <-- Update if your latest file has a different name/path\n",
    "\n",
    "# Directory where the previous notebook saved summary data (APSI, Motifs, etc.)\n",
    "# This path needs to match the output_summary_dir_phase1 from your analysis notebook\n",
    "# This is needed to load APSI/Motif data if it's not already in the main database file.\n",
    "output_summary_dir_phase1 = Path(\"./output_summary_data_hit_validation_phase1\") # <-- Update this path if different\n",
    "\n",
    "# Directory to save plots and summary data generated in this notebook\n",
    "# Use distinct directory names for figure outputs vs. analysis outputs\n",
    "output_figure_dir = 'publication_figures'\n",
    "output_figure_summary_dir = 'publication_figure_data' # For tables/data backing figures\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "# Use Path objects here for consistency\n",
    "Path(output_figure_dir).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Ensured plot output directory exists: {output_figure_dir}\")\n",
    "Path(output_figure_summary_dir).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Ensured summary data directory exists: {output_figure_summary_dir}\")\n",
    "\n",
    "# Path to InterPro entry list file (ensure this file is in the correct location)\n",
    "# This is needed for translating IPR/domain names in helper functions\n",
    "interpro_entry_path = 'interpro_entry.txt' # <-- Update path if needed\n",
    "\n",
    "\n",
    "# --- Define Key Column Names (Based on your database structure) ---\n",
    "# Define all relevant column names here for easy access and consistency\n",
    "protein_id_col = 'ProteinID'\n",
    "sequence_col = 'Sequence'\n",
    "source_dataset_col = 'Source_Dataset'\n",
    "source_genome_accession_col = 'Source_Genome_Assembly_Accession'\n",
    "source_protein_annotation_col = 'Source_Protein_Annotation'\n",
    "ncbi_taxid_col = 'NCBI_TaxID'\n",
    "asgard_phylum_col = 'Asgard_Phylum'\n",
    "virus_family_col = 'Virus_Family'\n",
    "virus_name_col = 'Virus_Name'\n",
    "orthogroup_col = 'Orthogroup'\n",
    "ipr_col = 'IPR_Signatures'\n",
    "ipr_go_terms_col = 'IPR_GO_Terms'\n",
    "uniprotkb_ac_col = 'UniProtKB_AC'\n",
    "num_domains_col = 'Num_Domains'\n",
    "domain_arch_col = 'Domain_Architecture'\n",
    "type_col = 'Type' # As in 'Annotated', 'Uncharacterized'\n",
    "is_hypothetical_col = 'Is_Hypothetical'\n",
    "has_known_structure_col = 'Has_Known_Structure'\n",
    "percent_disorder_col = 'Percent_Disorder'\n",
    "specific_func_cat_col = 'Specific_Functional_Category'\n",
    "broad_func_cat_col = 'Broad_Functional_Category'\n",
    "category_trigger_col = 'Category_Trigger'\n",
    "signal_peptide_col = 'Signal_Peptide_USPNet'\n",
    "sp_cleavage_site_col = 'SP_Cleavage_Site_USPNet'\n",
    "original_seq_length_col = 'Original_Seq_Length'\n",
    "group_col = 'Group' # 'Asgard' or 'GV'\n",
    "seqsearch_pdb_hit_col = 'SeqSearch_PDB_Hit'\n",
    "seqsearch_afdb_hit_col = 'SeqSearch_AFDB_Hit'\n",
    "has_reference_structure_col = 'Has_Reference_Structure'\n",
    "localization_col = 'Predicted_Subcellular_Localization'\n",
    "mature_protein_sequence_col = 'Mature_Protein_Sequence'\n",
    "mature_seq_length_col = 'Mature_Seq_Length'\n",
    "seqsearch_mgnify_hit_col = 'SeqSearch_MGnify_Hit'\n",
    "seqsearch_esma_hit_col = 'SeqSearch_ESMA_Hit'\n",
    "structurally_dark_col = 'Is_Structurally_Dark' # Derived column\n",
    "esp_col = 'Is_ESP' # Derived column\n",
    "hit_flag_col = 'Has_Euk_DIAMOND_Hit' # Flag for any Euk hit passing initial e-value\n",
    "euk_hit_sseqid_col = 'Euk_Hit_SSEQID' # Best Euk hit SSEQID\n",
    "euk_hit_organism_col = 'Euk_Hit_Organism' # Best Euk hit organism\n",
    "euk_hit_pident_col = 'Euk_Hit_PIDENT' # Best Euk hit PIDENT\n",
    "euk_hit_evalue_col = 'Euk_Hit_EVALUE' # Best Euk hit EVALUE\n",
    "euk_hit_protein_name_col = 'Euk_Hit_Protein_Name' # Best Euk hit protein name\n",
    "euk_hit_qstart_col = 'Euk_Hit_Qstart' # Alignment start on query (Asgard)\n",
    "euk_hit_qend_col = 'Euk_Hit_Qend' # Alignment end on query (Asgard)\n",
    "euk_hit_sstart_col = 'Euk_Hit_Sstart' # Alignment start on subject (Euk)\n",
    "euk_hit_send_col = 'Euk_Hit_Send' # Alignment end on subject (Euk)\n",
    "euk_hit_slen_diamond_col = 'Euk_Hit_Slen_Diamond' # Length of Euk hit from DIAMOND output\n",
    "query_coverage_col = 'Query_Coverage' # Calculated query coverage\n",
    "subject_coverage_col = 'Subject_Coverage' # Calculated subject coverage\n",
    "rbh_flag_col = 'Is_RBH' # Flag for Reciprocal Best Hit (if RBH analysis was included)\n",
    "\n",
    "# Add columns for APSI and Motifs if they were added to the main DB\n",
    "apsi_col = 'Intra_OG_APSI' # Assuming this column name if merged\n",
    "motif_col = 'Conserved_Motifs' # Assuming this column name if merged (might be list/string)\n",
    "num_og_sequences_col = 'Num_OG_Sequences' # Number of sequences in the OG (used for MSA)\n",
    "\n",
    "# --- Define Arcadia Color Palettes (from Style Guide PDF) ---\n",
    "print(\"\\n--- Defining Manual Arcadia Color Palettes (from Style Guide) ---\")\n",
    "# Define all colors from the style guide PDF for flexibility\n",
    "arcadia_colors_manual = {\n",
    "    \"aegean\": \"#5088C5\", \"amber\": \"#F28360\", \"seaweed\": \"#3B9886\", \"canary\": \"#F7B846\",\n",
    "    \"aster\": \"#7A77AB\", \"rose\": \"#F898AE\", \"vital\": \"#73B5E3\", \"tangerine\": \"#FFB984\", # Corrected Tangerine from PDF: FFb883 -> FFB984 (common web palette) - using FFB984 as per previous code\n",
    "    \"oat\": \"#F5E4BE\", \"wish\": \"#BABEE0\", \"lime\": \"#97CD78\", \"dragon\": \"#C85152\",\n",
    "    \"sky\": \"#C6E7F4\", \"dress\": \"#F8C5C1\", \"taupe\": \"#DBD1C3\", \"denim\": \"#B6C8D4\",\n",
    "    \"sage\": \"#B5BEA4\", \"mars\": \"#DA9085\", \"marine\": \"#8A99AD\", \"shell\": \"#EDE0D6\",\n",
    "    \"white\": \"#FFFFFF\", \"gray\": \"#EBEDE8\", \"chateau\": \"#B9AFA7\", # Corrected Chateau from PDF: BAB0A8 -> B9AFA7\n",
    "    \"bark\": \"#8F8885\", \"slate\": \"#43413F\", \"charcoal\": \"#484B50\", \"crow\": \"#292928\", \"black\": \"#09090A\",\n",
    "    \"forest\": \"#596F74\", # Forest is in Neutrals in PDF\n",
    "    \"parchment\": \"#FEF7F1\", \"zephyr\": \"#F4FBFF\", # Corrected Zephyr from PDF: F4FBFE -> F4FBFF\n",
    "    \"lichen\": \"#F7FBEF\", \"dawn\": \"#F8F4F1\"\n",
    "}\n",
    "# Define the specific palettes as lists of hex codes for easy cycling/use\n",
    "arcadia_primary_palette = [ arcadia_colors_manual[c] for c in [\"aegean\", \"amber\", \"seaweed\", \"canary\", \"aster\", \"rose\", \"vital\", \"tangerine\", \"oat\", \"wish\", \"lime\", \"dragon\"] ]\n",
    "arcadia_secondary_palette = [ arcadia_colors_manual[c] for c in [\"sky\", \"dress\", \"taupe\", \"denim\", \"sage\", \"mars\", \"marine\", \"shell\"] ]\n",
    "arcadia_neutrals_palette = [ arcadia_colors_manual[c] for c in [\"gray\", \"chateau\", \"bark\", \"slate\", \"charcoal\", \"forest\", \"crow\"] ]\n",
    "arcadia_background_palette = [ arcadia_colors_manual[c] for c in [\"parchment\", \"zephyr\", \"lichen\", \"dawn\"] ] # Added background colors\n",
    "\n",
    "print(\"Manual Arcadia palettes created.\")\n",
    "\n",
    "# --- Configure Plotly Defaults (Adhering to Style Guide) ---\n",
    "print(\"\\n--- Configuring Plotly Defaults (Adhering to Style Guide) ---\")\n",
    "pio.templates.default = \"plotly_white\" # Start with a clean white background\n",
    "\n",
    "# Define default layout for bold axis titles, NO gridlines, strong black axis lines, and NO plot titles\n",
    "# THIS VARIABLE NEEDS TO BE DEFINED *BEFORE* RUNNING FIGURE CELLS\n",
    "plotly_layout_defaults = go.Layout(\n",
    "    xaxis=dict(\n",
    "        title=dict(font=dict(size=15, color='black', family='Arial', weight='bold')), # Bold axis title, 15pt\n",
    "        showgrid=False, # Remove x-axis gridlines\n",
    "        zeroline=False, # Optional: Remove zero line as well\n",
    "        showline=True, # Show axis line\n",
    "        linecolor='black', # Axis line color\n",
    "        linewidth=1.5, # Axis line width (adjust as needed for \"strong\")\n",
    "        mirror=False, # Draw line on opposite side (False means only on the side where ticks/labels are)\n",
    "        ticks=\"outside\", # Show ticks outside the axis line\n",
    "        ticklen=5, # Tick length (5px as per style guide)\n",
    "        tickwidth=1.5, # Tick width (match axis line width for prominence)\n",
    "        tickcolor='black' # Tick color\n",
    "        # Tick label font can be set here too if needed, e.g., tickfont=dict(size=15, family='Arial', color='black')\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title=dict(font=dict(size=15, color='black', family='Arial', weight='bold')), # Bold axis title, 15pt\n",
    "        showgrid=False, # Remove y-axis gridlines\n",
    "        zeroline=False, # Optional: Remove zero line as well\n",
    "        showline=True, # Show axis line\n",
    "        linecolor='black', # Axis line color\n",
    "        linewidth=1.5, # Axis line width\n",
    "        mirror=False, # Draw line on opposite side\n",
    "        ticks=\"outside\", # Show ticks outside the axis line\n",
    "        ticklen=5, # Tick length\n",
    "        tickwidth=1.5, # Tick width\n",
    "        tickcolor='black' # Tick color\n",
    "        # Tick label font can be set here too if needed\n",
    "    ),\n",
    "    title=None, # Ensure no plot title by default\n",
    "    # Optional: Configure default font for tick labels if needed\n",
    "    font=dict(family=\"Arial\", size=15, color=\"black\"), # Example for 15pt tick labels\n",
    "    # Optional: Configure default legend title font\n",
    "    # legend=dict(title=dict(font=dict(weight='bold')))\n",
    ")\n",
    "\n",
    "# Apply these default layout settings directly to the plotly_white template\n",
    "# This should ensure they are applied to all px plots by default\n",
    "pio.templates['plotly_white'].layout.xaxis.title.font.update(size=15, weight='bold')\n",
    "pio.templates['plotly_white'].layout.yaxis.title.font.update(size=15, weight='bold')\n",
    "pio.templates['plotly_white'].layout.xaxis.showgrid = False\n",
    "pio.templates['plotly_white'].layout.yaxis.showgrid = False\n",
    "pio.templates['plotly_white'].layout.xaxis.showline = True # Ensure line is shown\n",
    "pio.templates['plotly_white'].layout.xaxis.linecolor = 'black'\n",
    "pio.templates['plotly_white'].layout.xaxis.linewidth = 1.5\n",
    "pio.templates['plotly_white'].layout.xaxis.mirror = False # Draw line on one side\n",
    "pio.templates['plotly_white'].layout.xaxis.ticks = \"outside\" # Show ticks outside\n",
    "pio.templates['plotly_white'].layout.xaxis.ticklen = 5 # Tick length\n",
    "pio.templates['plotly_white'].layout.xaxis.tickwidth = 1.5 # Tick width\n",
    "pio.templates['plotly_white'].layout.xaxis.tickcolor = 'black' # Tick color\n",
    "\n",
    "pio.templates['plotly_white'].layout.yaxis.showline = True # Ensure line is shown\n",
    "pio.templates['plotly_white'].layout.yaxis.linecolor = 'black'\n",
    "pio.templates['plotly_white'].layout.yaxis.linewidth = 1.5\n",
    "pio.templates['plotly_white'].layout.yaxis.mirror = False # Draw line on one side\n",
    "pio.templates['plotly_white'].layout.yaxis.ticks = \"outside\" # Show ticks outside\n",
    "pio.templates['plotly_white'].layout.yaxis.ticklen = 5 # Tick length\n",
    "pio.templates['plotly_white'].layout.yaxis.tickwidth = 1.5 # Tick width\n",
    "pio.templates['plotly_white'].layout.yaxis.tickcolor = 'black' # Tick color\n",
    "\n",
    "\n",
    "pio.templates['plotly_white'].layout.title = None # Set no title in template\n",
    "# Note: Modifying default templates can have broad effects. Applying to individual figures or using update_layout might be safer.\n",
    "\n",
    "print(\"Plotly default template set to 'plotly_white'.\")\n",
    "print(\"Default layout settings configured for bold axis titles (15pt), NO gridlines, strong black axis lines (1.5pt), tick marks (5px, 1.5pt, black), and NO plot titles.\")\n",
    "\n",
    "\n",
    "# --- Load InterPro Entry Data ---\n",
    "# This is needed for translating IPR IDs to names in domain architectures/IPRs\n",
    "print(f\"\\n--- Loading InterPro Entry Data from '{interpro_entry_path}' ---\")\n",
    "ipr_lookup = {}\n",
    "start_time_ipr = time.time()\n",
    "try:\n",
    "    # Adjust usecols/names if your interpro_entry.txt file format is different\n",
    "    ipr_info_df = pd.read_csv( interpro_entry_path, sep='\\t', usecols=[0, 1, 2], names=['IPR_ID', 'Type', 'Name'], header=0, comment='#', on_bad_lines='warn' )\n",
    "    # Ensure column names match what's expected after loading\n",
    "    if 'ENTRY_AC' in ipr_info_df.columns: ipr_info_df.rename(columns={'ENTRY_AC': 'IPR_ID'}, inplace=True)\n",
    "    if 'ENTRY_TYPE' in ipr_info_df.columns: ipr_info_df.rename(columns={'ENTRY_TYPE': 'Type'}, inplace=True)\n",
    "    if 'ENTRY_NAME' in ipr_info_df.columns: ipr_info_df.rename(columns={'ENTRY_NAME': 'Name'}, inplace=True)\n",
    "\n",
    "    if {'IPR_ID', 'Name', 'Type'}.issubset(ipr_info_df.columns):\n",
    "        ipr_info_df['IPR_ID'] = ipr_info_df['IPR_ID'].astype(str).str.strip()\n",
    "        ipr_lookup = ipr_info_df.set_index('IPR_ID')[['Type', 'Name']].to_dict('index')\n",
    "        print(f\"Loaded InterPro entry data for {len(ipr_lookup)} entries in {time.time() - start_time_ipr:.2f} seconds.\")\n",
    "    else:\n",
    "        print(f\"Warning: Expected columns ('IPR_ID', 'Name', 'Type') not all found in '{interpro_entry_path}' after loading.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: InterPro entry file not found at '{interpro_entry_path}'. Domain name translations will not be available.\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: An error occurred loading or processing '{interpro_entry_path}': {e}\")\n",
    "\n",
    "if not ipr_lookup:\n",
    "    print(\"Warning: ipr_lookup is empty. Domain name translations will not be available for plots/tables.\")\n",
    "\n",
    "\n",
    "# --- Define Helper Functions ---\n",
    "print(\"\\n--- Defining Helper Functions ---\")\n",
    "# Include necessary helper functions from your previous notebook here.\n",
    "# You might need: translate_architecture, truncate_string, clean_protein_name, etc.\n",
    "# Only include functions actually needed for figure generation and data manipulation *within* this notebook.\n",
    "\n",
    "# Example: translate_architecture (requires ipr_lookup)\n",
    "def translate_architecture(arch_string, lookup_dict=ipr_lookup):\n",
    "    \"\"\"Translates IPR IDs in a domain architecture string to names using a lookup dictionary.\"\"\"\n",
    "    if not isinstance(arch_string, str) or not arch_string or not lookup_dict:\n",
    "        return arch_string # Return original if invalid input or no lookup\n",
    "\n",
    "    processed_arch_string = str(arch_string).replace('|', ';')\n",
    "    ipr_ids = processed_arch_string.split(';')\n",
    "    translated_parts = []\n",
    "    for ipr_id in ipr_ids:\n",
    "        ipr_id_clean = ipr_id.strip()\n",
    "        if ipr_id_clean in lookup_dict:\n",
    "            name = lookup_dict[ipr_id_clean].get('Name', ipr_id_clean)\n",
    "            # Truncate long names for display\n",
    "            name = name[:40] + '...' if len(name) > 43 else name\n",
    "            translated_parts.append(f\"{name} ({ipr_id_clean})\")\n",
    "        elif ipr_id_clean:\n",
    "            translated_parts.append(ipr_id_clean) # Keep unknown IDs\n",
    "\n",
    "    full_translation = \"; \".join(translated_parts)\n",
    "    max_len_display = 150 # Max length for display\n",
    "    if len(full_translation) > max_len_display:\n",
    "        full_translation = full_translation[:max_len_display-3] + \"...\"\n",
    "\n",
    "    return full_translation\n",
    "\n",
    "# Example: truncate_string\n",
    "def truncate_string(text, max_len):\n",
    "    \"\"\"Truncates a string if it exceeds max_len, adding '...'.\"\"\"\n",
    "    if isinstance(text, str) and len(text) > max_len:\n",
    "        return text[:max_len-3] + \"...\"\n",
    "    return text\n",
    "\n",
    "# Example: clean_protein_name\n",
    "def clean_protein_name(name):\n",
    "    \"\"\"Cleans common annotations from protein names.\"\"\"\n",
    "    if pd.isna(name): return \"Unknown/Not Found\"; name = str(name).strip()\n",
    "    name = re.sub(r'\\s*\\|\\s*.*','', name); name = re.sub(r'\\bisoform\\s+[\\w-]+\\b', '', name, flags=re.IGNORECASE).strip()\n",
    "    name = re.sub(r'\\bpartial\\b', '', name, flags=re.IGNORECASE).strip(); name = re.sub(r'\\bputative\\b', '', name, flags=re.IGNORECASE).strip()\n",
    "    name = re.sub(r'\\bpredicted protein\\b', '', name, flags=re.IGNORECASE).strip(); name = re.sub(r'\\btype\\s+\\w+\\b', '', name, flags=re.IGNORECASE).strip()\n",
    "    name = re.sub(r'\\bprotein\\b', '', name, flags=re.IGNORECASE).strip(); name = re.sub(r'\\buncharacterized\\b', 'Uncharacterized', name, flags=re.IGNORECASE).strip()\n",
    "    name = re.sub(r'[;,]$', '', name).strip(); name = re.sub(r'\\s*\\(Fragment\\)$', '', name, flags=re.IGNORECASE).strip()\n",
    "    name = re.sub(r'\\s+', ' ', name).strip(); name = name[0].upper() + name[1:] if len(name) > 0 else name\n",
    "    return name if name else \"Unknown/Not Found\"\n",
    "\n",
    "# Add other necessary helper functions here...\n",
    "# e.g., get_ipr_counts if you plot IPR frequencies directly\n",
    "\n",
    "\n",
    "# --- Load Main Database ---\n",
    "print(f\"\\n--- Loading Data from '{database_path}' ---\")\n",
    "try:\n",
    "    # Load the database that already contains all annotations,\n",
    "    # including Euk hit details, coverage, and RBH flag (if added).\n",
    "    df_full = pd.read_csv(database_path, low_memory=False)\n",
    "    # Ensure ProteinID is string type right after loading\n",
    "    if protein_id_col in df_full.columns:\n",
    "         df_full[protein_id_col] = df_full[protein_id_col].astype(str)\n",
    "\n",
    "    # --- Merge APSI and Motif Data ---\n",
    "    # Assuming APSI and Motif results were saved to CSVs in the previous notebook\n",
    "    # and are NOT already merged into the main database file.\n",
    "    # If they ARE already in your database_path file, you can skip this merge step.\n",
    "\n",
    "    # Use the defined output_summary_dir_phase1 variable\n",
    "    apsi_file_path = output_summary_dir_phase1 / \"intra_og_apsi_values.csv\" # Path from previous notebook\n",
    "    motifs_file_path = output_summary_dir_phase1 / \"intra_og_conserved_motifs.csv\" # Path from previous notebook\n",
    "\n",
    "    print(f\"\\nAttempting to merge APSI data from: {apsi_file_path}\") # Added print statement\n",
    "    if apsi_file_path.is_file():\n",
    "        print(\"APSI file found.\") # Added print statement\n",
    "        try:\n",
    "            df_apsi = pd.read_csv(apsi_file_path)\n",
    "            print(f\"APSI file read successfully. Shape: {df_apsi.shape}. Columns: {df_apsi.columns.tolist()}\") # Added print statement\n",
    "            # Rename 'Num_Sequences' from APSI file to avoid conflict if needed, or use it\n",
    "            df_apsi.rename(columns={'Num_Sequences': num_og_sequences_col}, inplace=True)\n",
    "            # Ensure Orthogroup column exists in df_apsi before merging\n",
    "            if orthogroup_col in df_apsi.columns:\n",
    "                # Rename the 'APSI' column in df_apsi to match the desired column name 'Intra_OG_APSI'\n",
    "                if 'APSI' in df_apsi.columns:\n",
    "                    df_apsi.rename(columns={'APSI': apsi_col}, inplace=True)\n",
    "                    print(f\"Renamed 'APSI' column to '{apsi_col}' in APSI data.\") # Added print statement\n",
    "                else:\n",
    "                    print(f\"Warning: 'APSI' column not found in APSI data. Cannot rename to '{apsi_col}'.\")\n",
    "\n",
    "\n",
    "                # Merge APSI data into df_full based on Orthogroup\n",
    "                # Note: APSI is per OG, so merge will add APSI to all proteins in that OG\n",
    "                # Use a left merge to keep all proteins from df_full\n",
    "                print(f\"Merging APSI data on column '{orthogroup_col}'. df_full shape before merge: {df_full.shape}\") # Added print statement\n",
    "                # Select only the columns needed from df_apsi for the merge\n",
    "                cols_to_merge_from_apsi = [orthogroup_col, apsi_col, num_og_sequences_col]\n",
    "                # Filter to include only columns that actually exist in df_apsi after potential renaming\n",
    "                cols_to_merge_from_apsi_existing = [col for col in cols_to_merge_from_apsi if col in df_apsi.columns]\n",
    "\n",
    "                if cols_to_merge_from_apsi_existing:\n",
    "                     df_full = df_full.merge(df_apsi[cols_to_merge_from_apsi_existing], on=orthogroup_col, how='left')\n",
    "                     print(f\"Merged APSI data. df_full shape after merge: {df_full.shape}. Columns: {df_full.columns.tolist()}\") # Added print statement\n",
    "                else:\n",
    "                     print(\"Warning: No relevant columns found in APSI data for merging.\")\n",
    "\n",
    "\n",
    "            else:\n",
    "                 print(f\"Warning: Orthogroup column '{orthogroup_col}' not found in APSI data. Skipping APSI merge.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to merge APSI data: {e}\")\n",
    "    else:\n",
    "        print(f\"Warning: APSI file not found at '{apsi_file_path}'. APSI data will not be available.\")\n",
    "        # Add empty APSI column if not present to prevent downstream errors\n",
    "        if apsi_col not in df_full.columns: df_full[apsi_col] = np.nan\n",
    "        if num_og_sequences_col not in df_full.columns: df_full[num_og_sequences_col] = np.nan\n",
    "\n",
    "\n",
    "    print(f\"\\nAttempting to load and merge Motif data from: {motifs_file_path}\") # Added print statement\n",
    "    if motifs_file_path.is_file():\n",
    "        print(\"Motif file found.\") # Added print statement\n",
    "        try:\n",
    "            df_motifs = pd.read_csv(motifs_file_path)\n",
    "            print(f\"Motif file read successfully. Shape: {df_motifs.shape}. Columns: {df_motifs.columns.tolist()}\") # Added print statement\n",
    "            # You can now use df_motifs directly for motif analysis/plotting\n",
    "            print(f\"Loaded {len(df_motifs)} motifs for {len(df_motifs['Orthogroup'].unique())} orthogroups.\")\n",
    "            # Example: Add a flag to df_full if an OG has any conserved motif\n",
    "            if orthogroup_col in df_motifs.columns:\n",
    "                 ogs_with_motifs = df_motifs[orthogroup_col].unique()\n",
    "                 df_full['Has_Conserved_Motif'] = df_full[orthogroup_col].isin(ogs_with_motifs)\n",
    "                 print(f\"Added 'Has_Conserved_Motif' flag to df_full. Columns: {df_full.columns.tolist()}\") # Added print statement\n",
    "            else:\n",
    "                 print(f\"Warning: Orthogroup column '{orthogroup_col}' not found in Motif data. Skipping 'Has_Conserved_Motif' flag.\")\n",
    "                 if 'Has_Conserved_Motif' not in df_full.columns: df_full['Has_Conserved_Motif'] = False # Ensure column exists\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load motif data: {e}\")\n",
    "            # Add empty flag column if not present\n",
    "            if 'Has_Conserved_Motif' not in df_full.columns: df_full['Has_Conserved_Motif'] = False\n",
    "    else:\n",
    "        print(f\"Warning: Motif file not found at '{motifs_file_path}'. Motif data will not be available.\")\n",
    "        if 'Has_Conserved_Motif' not in df_full.columns: df_full['Has_Conserved_Motif'] = False\n",
    "\n",
    "\n",
    "    print(f\"\\nSuccessfully loaded and prepared data. Final df_full shape: {df_full.shape}. Columns: {df_full.columns.tolist()}\") # Added print statement\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Database file not found at '{database_path}'. Please check the path.\")\n",
    "    # Define df_full as empty to prevent downstream errors\n",
    "    df_full = pd.DataFrame()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading or processing the database: {e}\")\n",
    "    # Define df_full as empty\n",
    "    df_full = pd.DataFrame()\n",
    "\n",
    "\n",
    "# --- Create Color Maps for Plotting (after df_full is loaded) ---\n",
    "# Re-create color maps based on the loaded data to ensure categories match\n",
    "print(\"\\n--- Creating Color Maps ---\")\n",
    "# Asgard Phylum Colors\n",
    "asgard_phylum_color_map = {}\n",
    "if asgard_phylum_col in df_full.columns and group_col in df_full.columns:\n",
    "    # Filter for Asgard group before getting unique phyla\n",
    "    asgard_phyla_cat = df_full[df_full[group_col] == 'Asgard'][asgard_phylum_col].dropna().unique(); asgard_phyla_cat.sort()\n",
    "    if len(asgard_phyla_cat) > 0: asgard_phylum_color_map = {phylum: arcadia_primary_palette[i % len(arcadia_primary_palette)] for i, phylum in enumerate(asgard_phyla_cat)}\n",
    "    asgard_phylum_color_map['Unknown Phylum'] = arcadia_colors_manual.get('gray', '#bdbdbd')\n",
    "print(f\"Asgard phylum color map created for {len(asgard_phylum_color_map)} phyla.\")\n",
    "\n",
    "# Localization Colors\n",
    "localization_color_map = {}\n",
    "if localization_col in df_full.columns:\n",
    "    # Get all unique localization values, including potential NaNs, from Asgard and GV groups\n",
    "    all_localizations = df_full[df_full[group_col].isin(['Asgard', 'GV'])][localization_col].unique()\n",
    "    # Convert to list, handling NaN explicitly if needed for mapping\n",
    "    all_localizations_list = [str(loc) if pd.isna(loc) else loc for loc in all_localizations]\n",
    "    all_localizations_list.sort() # Sort for consistent color assignment\n",
    "    localization_assignments = {\n",
    "        'Archaea: Cytoplasmic/Membrane (non-SP)': arcadia_colors_manual.get('aegean', '#5088C5'),\n",
    "        'Archaea: Membrane-associated (Lipoprotein/Pilin)': arcadia_colors_manual.get('amber', '#F28360'),\n",
    "        'Archaea: Secreted/Membrane (Sec/Tat pathway)': arcadia_colors_manual.get('seaweed', '#3B9886'),\n",
    "        'Host: Cytoplasm/Nucleus/Virus Factory': arcadia_colors_manual.get('vital', '#73B5E3'),\n",
    "        'Host: Membrane-associated (Lipoprotein/Pilin-like)': arcadia_colors_manual.get('tangerine', '#FFB984'),\n",
    "        'Host: Secretory Pathway (Secreted/Membrane/Organelle)': arcadia_colors_manual.get('lime', '#97CD78'),\n",
    "        'CYTOPLASMIC': arcadia_colors_manual.get('vital', '#73B5E3'), # Include simplified terms if present\n",
    "        'MEMBRANE': arcadia_colors_manual.get('tangerine', '#FFB984'),\n",
    "        'EXTRACELLULAR': arcadia_colors_manual.get('lime', '#97CD78'),\n",
    "        'Unknown': arcadia_colors_manual.get('gray', '#EBEDE8'),\n",
    "        'nan': arcadia_colors_manual.get('gray', '#EBEDE8') # Handle NaN explicitly\n",
    "    }\n",
    "    fallback_palette_loc = arcadia_neutrals_palette + arcadia_secondary_palette # Use defined palettes\n",
    "    fallback_idx_loc = 0\n",
    "    for loc in all_localizations_list:\n",
    "        if loc not in localization_color_map:\n",
    "             localization_color_map[loc] = localization_assignments.get(loc, fallback_palette_loc[fallback_idx_loc % len(fallback_palette_loc)])\n",
    "             if loc not in localization_assignments:\n",
    "                  fallback_idx_loc += 1\n",
    "print(f\"Localization color map created for {len(localization_color_map)} categories.\")\n",
    "\n",
    "# Broad Functional Category Colors\n",
    "broad_category_color_map = {}\n",
    "if broad_func_cat_col in df_full.columns:\n",
    "    # Get all unique broad categories, including potential NaNs, from Asgard and GV groups\n",
    "    all_broad_categories = df_full[df_full[group_col].isin(['Asgard', 'GV'])][broad_func_cat_col].unique()\n",
    "    # Convert to list, handling NaN explicitly\n",
    "    all_broad_categories_list = [str(cat) if pd.isna(cat) else cat for cat in all_broad_categories]\n",
    "    all_broad_categories_list.sort() # Sort for consistent color assignment\n",
    "\n",
    "    category_assignments = {\n",
    "        'Cytoskeleton': arcadia_colors_manual.get('aegean', '#5088C5'),\n",
    "        'Membrane Trafficking/Vesicles': arcadia_colors_manual.get('amber', '#F28360'),\n",
    "        'ESCRT/Endosomal Sorting': arcadia_colors_manual.get('seaweed', '#3B9886'),\n",
    "        'Ubiquitin System': arcadia_colors_manual.get('aster', '#7A77AB'),\n",
    "        'N-glycosylation': arcadia_colors_manual.get('rose', '#F898AE'),\n",
    "        'Nuclear Transport/Pore': arcadia_colors_manual.get('vital', '#73B5E3'),\n",
    "        'DNA Info Processing': arcadia_colors_manual.get('canary', '#F7B846'),\n",
    "        'RNA Info Processing': arcadia_colors_manual.get('lime', '#97CD78'),\n",
    "        'Translation': arcadia_colors_manual.get('tangerine', '#FFB984'),\n",
    "        'Signal Transduction': arcadia_colors_manual.get('aster', '#7A77AB'),\n",
    "        'Metabolism': arcadia_colors_manual.get('sky', '#C6E7F4'), # Using sky as per previous code\n",
    "        'Other Specific Annotation': arcadia_colors_manual.get('denim', '#B6C8D4'),\n",
    "        'Unknown/Unclassified': arcadia_colors_manual.get('gray', '#EBEDE8'),\n",
    "        'nan': arcadia_colors_manual.get('gray', '#EBEDE8') # Handle NaN explicitly\n",
    "    }\n",
    "    fallback_palette_cat = arcadia_primary_palette + arcadia_secondary_palette + arcadia_neutrals_palette # Use defined palettes\n",
    "    fallback_idx_cat = 0\n",
    "    for category in all_broad_categories_list:\n",
    "        if category not in broad_category_color_map:\n",
    "            broad_category_color_map[category] = category_assignments.get(category, fallback_palette_cat[fallback_idx_cat % len(fallback_palette_cat)])\n",
    "            if category not in category_assignments:\n",
    "                fallback_idx_cat += 1\n",
    "print(f\"Broad functional category color map created for {len(broad_category_color_map)} categories.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Figures Notebook Setup Complete ---\")\n",
    "print(\"DataFrame 'df_full' is loaded and prepared for figure generation.\")\n",
    "print(\"Color maps and helper functions are defined.\")\n",
    "print(\"You can now proceed with generating figures in subsequent cells.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06eb6b0-35e9-402c-ba97-ae0095792297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Figure 2 - General Database Characterization\n",
    "\n",
    "# --- Imports ---\n",
    "# Assumes pandas, numpy, plotly.express, plotly.graph_objects, plotly.io,\n",
    "# and necessary variables/helper functions (like df_full, group_col,\n",
    "# protein_id_col, orthogroup_col, broad_func_cat_col, structurally_dark_col,\n",
    "# esp_col, hit_flag_col, euk_hit_protein_name_col, clean_protein_name,\n",
    "# arcadia_colors_manual, arcadia_primary_palette, arcadia_secondary_palette,\n",
    "# arcadia_neutrals_palette, output_figure_dir, output_figure_summary_dir,\n",
    "# plotly_layout_defaults) are defined in the setup cell (Cell 1).\n",
    "\n",
    "print(\"\\n\\n--- Generating Figure 2: General Database Characterization ---\")\n",
    "\n",
    "# Ensure df_full is loaded\n",
    "if 'df_full' not in locals() or df_full.empty:\n",
    "    print(\"ERROR: df_full not loaded. Please run the setup cell (Cell 1).\")\n",
    "else:\n",
    "    # --- Filter out groups other than 'Asgard' and 'GV' ---\n",
    "    # Create a filtered DataFrame containing only 'Asgard' and 'GV' groups\n",
    "    if group_col in df_full.columns:\n",
    "        df_filtered_groups = df_full[df_full[group_col].isin(['Asgard', 'GV'])].copy()\n",
    "        print(f\"Filtered out groups other than 'Asgard' and 'GV'. Remaining proteins: {len(df_filtered_groups)}\")\n",
    "        if df_filtered_groups.empty:\n",
    "            print(\"WARNING: No 'Asgard' or 'GV' proteins remaining after filtering. Cannot generate Figure 2.\")\n",
    "    else:\n",
    "        print(f\"ERROR: Group column '{group_col}' not found in df_full. Cannot filter groups.\")\n",
    "        df_filtered_groups = pd.DataFrame() # Create empty df if group column is missing\n",
    "\n",
    "\n",
    "    # Proceed with plotting only if filtered data is available\n",
    "    if not df_filtered_groups.empty:\n",
    "\n",
    "        # Define colors for Asgard and GV using the specified hex codes\n",
    "        group_colors = {\n",
    "            'Asgard': arcadia_colors_manual.get('aegean', '#5088C5'), # Using aegean as specified\n",
    "            'GV': arcadia_colors_manual.get('amber', '#F28360') # Using amber as specified\n",
    "        }\n",
    "        print(f\"\\nUsing Group Colors: Asgard={group_colors.get('Asgard')}, GV={group_colors.get('GV')}\")\n",
    "\n",
    "\n",
    "        # --- Figure 2A Part 1: Number of Genomes per Group ---\n",
    "        print(\"\\n--- Figure 2A Part 1: Number of Genomes per Group ---\")\n",
    "\n",
    "        # Calculate number of genomes per group - using .nunique() on filtered data\n",
    "        if source_genome_accession_col in df_filtered_groups.columns and group_col in df_filtered_groups.columns:\n",
    "            genome_counts = df_filtered_groups.groupby(group_col)[source_genome_accession_col].nunique().reset_index()\n",
    "            genome_counts.columns = ['Group', 'Num_Genomes']\n",
    "            print(\"\\nGenome Counts:\")\n",
    "            print(genome_counts.to_markdown(index=False))\n",
    "\n",
    "            fig_2a_genomes = px.bar(\n",
    "                genome_counts,\n",
    "                x='Group',\n",
    "                y='Num_Genomes',\n",
    "                color='Group',\n",
    "                # title='Number of Genomes by Group', # No title\n",
    "                labels={'Group': 'Group', 'Num_Genomes': 'Number of Genomes'},\n",
    "                color_discrete_map=group_colors,\n",
    "                template='plotly_white'\n",
    "            )\n",
    "            # Apply default layout and remove gridlines\n",
    "            fig_2a_genomes.update_layout(plotly_layout_defaults)\n",
    "            fig_2a_genomes.update_xaxes(title_text='Group', showgrid=False)\n",
    "            fig_2a_genomes.update_yaxes(title_text='Number of Genomes', showgrid=False)\n",
    "            fig_2a_genomes.update_layout(showlegend=False) # Hide legend if colors are obvious from x-axis\n",
    "\n",
    "            # Export Figure 2A Part 1 (HTML and PDF/SVG)\n",
    "            fig_2a_genomes_path_html = Path(output_figure_dir) / \"figure2a_genomes_count.html\"\n",
    "            fig_2a_genomes.write_html(str(fig_2a_genomes_path_html))\n",
    "            print(f\"Figure 2A (Genomes) HTML saved to: {fig_2a_genomes_path_html}\")\n",
    "            \n",
    "            # Export to PDF/SVG (requires kaleido)\n",
    "            try:\n",
    "                fig_2a_genomes_path_pdf = Path(output_figure_dir) / \"figure2a_genomes_count.pdf\"\n",
    "                fig_2a_genomes.write_image(str(fig_2a_genomes_path_pdf))\n",
    "                print(f\"Figure 2A (Genomes) PDF saved to: {fig_2a_genomes_path_pdf}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not export Figure 2A (Genomes) to PDF. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "            try:\n",
    "                fig_2a_genomes_path_svg = Path(output_figure_dir) / \"figure2a_genomes_count.svg\"\n",
    "                fig_2a_genomes.write_image(str(fig_2a_genomes_path_svg))\n",
    "                print(f\"Figure 2A (Genomes) SVG saved to: {fig_2a_genomes_path_svg}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not export Figure 2A (Genomes) to SVG. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "\n",
    "\n",
    "        else:\n",
    "            print(f\"Skipping Figure 2A Part 1 due to missing columns in filtered data: '{source_genome_accession_col}' or '{group_col}'.\")\n",
    "\n",
    "\n",
    "        # --- Figure 2A Part 2: Number of Proteins per Group ---\n",
    "        print(\"\\n--- Figure 2A Part 2: Number of Proteins per Group ---\")\n",
    "\n",
    "        # Calculate number of proteins per group - using .count() on filtered data\n",
    "        if protein_id_col in df_filtered_groups.columns and group_col in df_filtered_groups.columns:\n",
    "            protein_counts = df_filtered_groups.groupby(group_col)[protein_id_col].count().reset_index()\n",
    "            protein_counts.columns = ['Group', 'Num_Proteins']\n",
    "            print(\"\\nProtein Counts:\")\n",
    "            print(protein_counts.to_markdown(index=False))\n",
    "\n",
    "            fig_2a_proteins = px.bar(\n",
    "                protein_counts,\n",
    "                x='Group',\n",
    "                y='Num_Proteins',\n",
    "                color='Group',\n",
    "                # title='Number of Proteins by Group', # No title\n",
    "                labels={'Group': 'Group', 'Num_Proteins': 'Number of Proteins'},\n",
    "                color_discrete_map=group_colors,\n",
    "                template='plotly_white'\n",
    "            )\n",
    "            # Apply default layout and remove gridlines\n",
    "            fig_2a_proteins.update_layout(plotly_layout_defaults)\n",
    "            fig_2a_proteins.update_xaxes(title_text='Group', showgrid=False)\n",
    "            fig_2a_proteins.update_yaxes(title_text='Number of Proteins', showgrid=False)\n",
    "            fig_2a_proteins.update_layout(showlegend=False) # Hide legend\n",
    "\n",
    "            # Export Figure 2A Part 2 (HTML and PDF/SVG)\n",
    "            fig_2a_proteins_path_html = Path(output_figure_dir) / \"figure2a_proteins_count.html\"\n",
    "            fig_2a_proteins.write_html(str(fig_2a_proteins_path_html))\n",
    "            print(f\"Figure 2A (Proteins) HTML saved to: {fig_2a_proteins_path_html}\")\n",
    "            \n",
    "            # Export to PDF/SVG (requires kaleido)\n",
    "            try:\n",
    "                fig_2a_proteins_path_pdf = Path(output_figure_dir) / \"figure2a_proteins_count.pdf\"\n",
    "                fig_2a_proteins.write_image(str(fig_2a_proteins_path_pdf))\n",
    "                print(f\"Figure 2A (Proteins) PDF saved to: {fig_2a_proteins_path_pdf}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not export Figure 2A (Proteins) to PDF. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "            try:\n",
    "                fig_2a_proteins_path_svg = Path(output_figure_dir) / \"figure2a_proteins_count.svg\"\n",
    "                fig_2a_proteins.write_image(str(fig_2a_proteins_path_svg))\n",
    "                print(f\"Figure 2A (Proteins) SVG saved to: {fig_2a_proteins_path_svg}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not export Figure 2A (Proteins) to SVG. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "\n",
    "\n",
    "        else:\n",
    "            print(f\"Skipping Figure 2A Part 2 due to missing columns in filtered data: '{protein_id_col}' or '{group_col}'.\")\n",
    "\n",
    "\n",
    "        # --- Figure 2B: Orthogroup Size Distribution (Histogram) ---\n",
    "        print(\"\\n--- Figure 2B: Orthogroup Size Distribution (Histogram) ---\")\n",
    "\n",
    "        # Calculate OG sizes and group information using the filtered data\n",
    "        if orthogroup_col in df_filtered_groups.columns and protein_id_col in df_filtered_groups.columns and group_col in df_filtered_groups.columns:\n",
    "            # Get OG sizes by counting proteins per OG in the filtered data\n",
    "            og_sizes = df_filtered_groups.groupby(orthogroup_col)[protein_id_col].count().reset_index()\n",
    "            og_sizes.columns = [orthogroup_col, 'OG_Size']\n",
    "\n",
    "            # Add group information to OG sizes (take group from the first protein in the OG)\n",
    "            # This assumes all proteins in an OG belong to the same group (Asgard or GV)\n",
    "            og_group = df_filtered_groups.drop_duplicates(subset=[orthogroup_col], keep='first')[[orthogroup_col, group_col]]\n",
    "            og_sizes = pd.merge(og_sizes, og_group, on=orthogroup_col, how='left')\n",
    "\n",
    "            # Filter OGs used for MSA (size >= 5)\n",
    "            min_og_size_for_msa = 5 # Should match the criterion used in Cell 16\n",
    "            ogs_for_msa = og_sizes[og_sizes['OG_Size'] >= min_og_size_for_msa]\n",
    "            num_ogs_for_msa = len(ogs_for_msa)\n",
    "            num_proteins_in_msa_ogs = ogs_for_msa['OG_Size'].sum()\n",
    "\n",
    "            print(f\"\\nNumber of Orthogroups with size >= {min_og_size_for_msa} (used for MSA) in filtered data: {num_ogs_for_msa:,}\")\n",
    "            print(f\"Total proteins in these OGs: {num_proteins_in_msa_ogs:,}\")\n",
    "\n",
    "            # Use Histogram for Orthogroup Size Distribution\n",
    "            fig_2b = px.histogram(\n",
    "                og_sizes,\n",
    "                x='OG_Size',\n",
    "                color='Group',\n",
    "                barmode='overlay', # Overlay bars for Asgard and GV\n",
    "                nbins=50, # Adjust number of bins as needed\n",
    "                # title='Orthogroup Size Distribution Histogram', # No title\n",
    "                labels={'OG_Size': 'Orthogroup Size (Number of Proteins)', 'Group': 'Group'},\n",
    "                color_discrete_map=group_colors,\n",
    "                template='plotly_white'\n",
    "            )\n",
    "            # Apply default layout and remove gridlines\n",
    "            fig_2b.update_layout(plotly_layout_defaults)\n",
    "            fig_2b.update_xaxes(title_text='Orthogroup Size (Number of Proteins)', showgrid=False)\n",
    "            fig_2b.update_yaxes(title_text='Count (Number of Orthogroups)', showgrid=False)\n",
    "            fig_2b.update_layout(legend_title_text='Group') # Add legend title\n",
    "            fig_2b.update_traces(opacity=0.75) # Make bars slightly transparent for overlay\n",
    "\n",
    "            # Export Figure 2B Histogram (HTML and PDF/SVG)\n",
    "            fig_2b_path_html = Path(output_figure_dir) / \"figure2b_og_size_distribution_histogram.html\"\n",
    "            fig_2b.write_html(str(fig_2b_path_html))\n",
    "            print(f\"Figure 2B (Histogram) HTML saved to: {fig_2b_path_html}\")\n",
    "            \n",
    "            # Export to PDF/SVG (requires kaleido)\n",
    "            try:\n",
    "                fig_2b_path_pdf = Path(output_figure_dir) / \"figure2b_og_size_distribution_histogram.pdf\"\n",
    "                fig_2b.write_image(str(fig_2b_path_pdf))\n",
    "                print(f\"Figure 2B (Histogram) PDF saved to: {fig_2b_path_pdf}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not export Figure 2B (Histogram) to PDF. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "            try:\n",
    "                fig_2b_path_svg = Path(output_figure_dir) / \"figure2b_og_size_distribution_histogram.svg\"\n",
    "                fig_2b.write_image(str(fig_2b_path_svg))\n",
    "                print(f\"Figure 2B (Histogram) SVG saved to: {fig_2b_path_svg}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not export Figure 2B (Histogram) to SVG. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "\n",
    "\n",
    "        else:\n",
    "            print(\"Skipping Figure 2B due to missing data in filtered data.\")\n",
    "\n",
    "\n",
    "        # --- Figure 2C: High-Level Annotation Summary & Euk Hit Protein Types ---\n",
    "        print(\"\\n--- Figure 2C: High-Level Annotation Summary & Euk Hit Protein Types ---\")\n",
    "\n",
    "        # Define annotation columns to summarize\n",
    "        # Ensure these columns exist before adding to the list in the filtered data\n",
    "        annotation_cols_to_summarize = []\n",
    "        if 'Has_Conserved_Motif' in df_filtered_groups.columns: annotation_cols_to_summarize.append('Has_Conserved_Motif')\n",
    "        if structurally_dark_col in df_filtered_groups.columns: annotation_cols_to_summarize.append(structurally_dark_col)\n",
    "        if esp_col in df_filtered_groups.columns: annotation_cols_to_summarize.append(esp_col)\n",
    "        if hit_flag_col in df_filtered_groups.columns: annotation_cols_to_summarize.append(hit_flag_col)\n",
    "        # Add other relevant boolean flags if available, e.g., has any domain (check column existence)\n",
    "        if num_domains_col in df_filtered_groups.columns:\n",
    "             # Create a boolean column for having any domain\n",
    "             df_filtered_groups['Has_Any_Domain'] = df_filtered_groups[num_domains_col].notna()\n",
    "             annotation_cols_to_summarize.append('Has_Any_Domain')\n",
    "\n",
    "\n",
    "        # Calculate percentages for each annotation type by group using the filtered data\n",
    "        annotation_summary_list = []\n",
    "        for col in annotation_cols_to_summarize:\n",
    "            # Ensure group column exists in filtered data (already checked above, but safe)\n",
    "            if group_col in df_filtered_groups.columns:\n",
    "                # Ensure column is boolean for value_counts\n",
    "                df_filtered_groups[col] = df_filtered_groups[col].astype(bool)\n",
    "\n",
    "                # Calculate value counts (True/False) for the current annotation column and group\n",
    "                summary_counts = df_filtered_groups.groupby(group_col)[col].value_counts().reset_index(name='Count')\n",
    "                summary_counts.rename(columns={col: 'Value'}, inplace=True) # Rename boolean col to 'Value'\n",
    "\n",
    "                # Calculate percentages\n",
    "                summary_percentages = df_filtered_groups.groupby(group_col)[col].value_counts(normalize=True).mul(100).reset_index(name='Percentage')\n",
    "                summary_percentages.rename(columns={col: 'Value'}, inplace=True) # Rename boolean col to 'Value'\n",
    "\n",
    "                # Merge counts and percentages\n",
    "                summary_combined = pd.merge(summary_counts, summary_percentages, on=[group_col, 'Value'])\n",
    "\n",
    "                # Filter for 'True' values and handle ESP specifically for GV\n",
    "                summary_true = summary_combined[summary_combined['Value'] == True].copy()\n",
    "\n",
    "                # Remove ESP entry for GV\n",
    "                if col == esp_col:\n",
    "                     summary_true = summary_true[summary_true[group_col] != 'GV'].copy() # Filter out GV for ESP\n",
    "\n",
    "                if not summary_true.empty:\n",
    "                     summary_true['Annotation_Type'] = col.replace('_', ' ').replace('Is ', '').strip() # Clean up name for plotting\n",
    "                     annotation_summary_list.append(summary_true)\n",
    "                # If summary_true is empty after filtering (e.g., no True values or ESP for GV), it's not added\n",
    "\n",
    "            else:\n",
    "                print(f\"Warning: Group column '{group_col}' not found in filtered data. Skipping summary for all annotations.\")\n",
    "                break # Exit loop if group column is missing\n",
    "\n",
    "\n",
    "        if annotation_summary_list:\n",
    "            df_annotation_summary = pd.concat(annotation_summary_list, ignore_index=True)\n",
    "            # Add a 'False' row with 0% for types that were all False (so they appear in the plot categories)\n",
    "            # Get all unique annotation types that were processed (based on columns present in filtered data)\n",
    "            all_processed_annotation_types = [s.replace('_', ' ').replace('Is ', '').strip() for s in annotation_cols_to_summarize if s in df_filtered_groups.columns]\n",
    "            # Get types present in the summary (i.e., had at least one True value in at least one group)\n",
    "            present_annotation_types = df_annotation_summary['Annotation_Type'].unique()\n",
    "            # Find types that were processed but had no True values\n",
    "            missing_annotation_types = [atype for atype in all_processed_annotation_types if atype not in present_annotation_types]\n",
    "\n",
    "            if missing_annotation_types:\n",
    "                 zero_percentage_rows = []\n",
    "                 # Use the unique groups from the filtered data for adding zero rows\n",
    "                 for atype in missing_annotation_types:\n",
    "                      for group in df_filtered_groups[group_col].unique():\n",
    "                           # Add a row with 0% for each group for the missing annotation type\n",
    "                           zero_percentage_rows.append({'Group': group, 'Percentage': 0.0, 'Count': 0, 'Annotation_Type': atype})\n",
    "                 df_annotation_summary = pd.concat([df_annotation_summary, pd.DataFrame(zero_percentage_rows)], ignore_index=True)\n",
    "\n",
    "\n",
    "            print(\"\\nHigh-Level Annotation Summary (% True by Group):\")\n",
    "            # Pivot for display, showing all processed types including those with 0%\n",
    "            # Use the unique groups from the filtered data for the pivot columns\n",
    "            df_annotation_display = df_annotation_summary.pivot_table(index='Annotation_Type', columns='Group', values='Percentage', fill_value=0).round(1)\n",
    "            print(df_annotation_display.to_markdown())\n",
    "\n",
    "\n",
    "            # --- Stacked Bar Chart (Grouped Bars) ---\n",
    "            # Ensure category order for plotting matches the order defined in annotation_cols_to_summarize\n",
    "            plot_category_order = [col.replace('_', ' ').replace('Is ', '').strip() for col in annotation_cols_to_summarize if col in df_filtered_groups.columns]\n",
    "\n",
    "            fig_2c_stacked = px.bar(\n",
    "                df_annotation_summary,\n",
    "                x='Annotation_Type',\n",
    "                y='Percentage',\n",
    "                color='Group',\n",
    "                barmode='group', # Use group mode for side-by-side bars per annotation\n",
    "                # title='High-Level Annotation Summary by Group', # No title\n",
    "                labels={'Annotation_Type': 'Annotation Type', 'Percentage': 'Percentage of Proteins (%)', 'Group': 'Group'},\n",
    "                color_discrete_map=group_colors,\n",
    "                category_orders={'Annotation_Type': plot_category_order}, # Apply consistent order\n",
    "                template='plotly_white'\n",
    "            )\n",
    "            # Apply default layout and remove gridlines\n",
    "            fig_2c_stacked.update_layout(plotly_layout_defaults)\n",
    "            fig_2c_stacked.update_xaxes(title_text='Annotation Type', showgrid=False)\n",
    "            fig_2c_stacked.update_yaxes(title_text='Percentage of Proteins (%)', showgrid=False)\n",
    "            fig_2c_stacked.update_layout(legend_title_text='Group') # Add legend title\n",
    "            fig_2c_stacked.update_xaxes(tickangle=45) # Angle labels if needed\n",
    "\n",
    "            # Export Figure 2C Stacked Bar (HTML and PDF/SVG)\n",
    "            fig_2c_stacked_path_html = Path(output_figure_dir) / \"figure2c_annotation_summary_grouped_bar.html\"\n",
    "            fig_2c_stacked.write_html(str(fig_2c_stacked_path_html))\n",
    "            print(f\"Figure 2C (Grouped Bar) HTML saved to: {fig_2c_stacked_path_html}\")\n",
    "            \n",
    "            # Export to PDF/SVG (requires kaleido)\n",
    "            try:\n",
    "                fig_2c_stacked_path_pdf = Path(output_figure_dir) / \"figure2c_annotation_summary_grouped_bar.pdf\"\n",
    "                fig_2c_stacked.write_image(str(fig_2c_stacked_path_pdf))\n",
    "                print(f\"Figure 2C (Grouped Bar) PDF saved to: {fig_2c_stacked_path_pdf}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not export Figure 2C (Grouped Bar) to PDF. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "            try:\n",
    "                fig_2c_stacked_path_svg = Path(output_figure_dir) / \"figure2c_annotation_summary_grouped_bar.svg\"\n",
    "                fig_2c_stacked.write_image(str(fig_2c_stacked_path_svg))\n",
    "                print(f\"Figure 2C (Grouped Bar) SVG saved to: {fig_2c_stacked_path_svg}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not export Figure 2C (Grouped Bar) to SVG. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "\n",
    "\n",
    "            # --- Euk Hit Protein Name Analysis (Top Names) ---\n",
    "            print(\"\\n--- Euk Hit Protein Name Analysis (Top Names for Proteins with Euk Hits) ---\")\n",
    "            # Analyze proteins flagged with Has_Euk_DIAMOND_Hit within the filtered groups\n",
    "            if hit_flag_col in df_filtered_groups.columns and euk_hit_protein_name_col in df_filtered_groups.columns and 'clean_protein_name' in locals():\n",
    "                 df_euk_hits_annotated = df_filtered_groups[df_filtered_groups[hit_flag_col] == True].copy()\n",
    "\n",
    "                 if not df_euk_hits_annotated.empty:\n",
    "                      # Apply cleaning function (defined in Cell 1/Setup)\n",
    "                      df_euk_hits_annotated['Cleaned_Euk_Prot_Name'] = df_euk_hits_annotated[euk_hit_protein_name_col].apply(clean_protein_name)\n",
    "                      euk_hit_name_counts = df_euk_hits_annotated['Cleaned_Euk_Prot_Name'].value_counts()\n",
    "\n",
    "                      # Filter out generic names like 'Uncharacterized' or 'Unknown' for plotting top names\n",
    "                      euk_hit_name_counts_filtered = euk_hit_name_counts[~euk_hit_name_counts.index.isin(['Uncharacterized', 'Unknown/Not Found', 'nan'])]\n",
    "\n",
    "                      top_n_euk_names = 20 # Define how many top names to show\n",
    "                      df_top_euk_names = euk_hit_name_counts_filtered.head(top_n_euk_names).reset_index()\n",
    "                      df_top_euk_names.columns = ['Cleaned_Euk_Prot_Name', 'Count']\n",
    "\n",
    "                      print(f\"\\nTop {top_n_euk_names} Cleaned Eukaryotic Hit Protein Names (Functionally Annotated Hits):\")\n",
    "                      print(df_top_euk_names.to_markdown(index=False))\n",
    "\n",
    "                      # Plot Top Euk Hit Names using a different color palette\n",
    "                      if not df_top_euk_names.empty:\n",
    "                           # Use a sequential or different discrete palette for different names\n",
    "                           euk_name_colors = px.colors.qualitative.Plotly # Example palette\n",
    "                           # Create a mapping for the top N names\n",
    "                           euk_name_color_map = {name: euk_name_colors[i % len(euk_name_colors)] for i, name in enumerate(df_top_euk_names['Cleaned_Euk_Prot_Name'])}\n",
    "\n",
    "                           fig_2c_euk_names = px.bar(\n",
    "                               df_top_euk_names,\n",
    "                               x='Cleaned_Euk_Prot_Name',\n",
    "                               y='Count',\n",
    "                               color='Cleaned_Euk_Prot_Name', # Color by protein name\n",
    "                               color_discrete_map=euk_name_color_map, # Apply the name-specific colors\n",
    "                               # title=f'Top {top_n_euk_names} Eukaryotic Hit Protein Names (Functionally Annotated Hits)', # No title\n",
    "                               labels={'Cleaned_Euk_Prot_Name': 'Cleaned Eukaryotic Hit Protein Name', 'Count': 'Number of Asgard/GV Proteins Hit'},\n",
    "                               template='plotly_white'\n",
    "                           )\n",
    "                           # Apply default layout and remove gridlines\n",
    "                           fig_2c_euk_names.update_layout(plotly_layout_defaults)\n",
    "                           fig_2c_euk_names.update_xaxes(title_text='Cleaned Eukaryotic Hit Protein Name', showgrid=False)\n",
    "                           fig_2c_euk_names.update_yaxes(title_text='Number of Asgard/GV Proteins Hit', showgrid=False)\n",
    "                           fig_2c_euk_names.update_xaxes(tickangle=45, categoryorder='total descending') # Angle labels and order by count\n",
    "                           fig_2c_euk_names.update_layout(showlegend=False) # Hide legend if too many names\n",
    "\n",
    "                           # Export Figure 2C Euk Names (HTML and PDF/SVG)\n",
    "                           fig_2c_euk_names_path_html = Path(output_figure_dir) / \"figure2c_top_euk_hit_protein_names.html\"\n",
    "                           fig_2c_euk_names.write_html(str(fig_2c_euk_names_path_html))\n",
    "                           print(f\"Figure 2C (Top Euk Hit Names) HTML saved to: {fig_2c_euk_names_path_html}\")\n",
    "                           \n",
    "                           # Export to PDF/SVG (requires kaleido)\n",
    "                           try:\n",
    "                               fig_2c_euk_names_path_pdf = Path(output_figure_dir) / \"figure2c_top_euk_hit_protein_names.pdf\"\n",
    "                               fig_2c_euk_names.write_image(str(fig_2c_euk_names_path_pdf))\n",
    "                               print(f\"Figure 2C (Top Euk Hit Names) PDF saved to: {fig_2c_euk_names_path_pdf}\")\n",
    "                           except Exception as e:\n",
    "                               print(f\"Warning: Could not export Figure 2C (Top Euk Hit Names) to PDF. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "                           try:\n",
    "                               fig_2c_euk_names_path_svg = Path(output_figure_dir) / \"figure2c_top_euk_hit_protein_names.svg\"\n",
    "                               fig_2c_euk_names.write_image(str(fig_2c_euk_names_path_svg))\n",
    "                               print(f\"Figure 2C (Top Euk Hit Names) SVG saved to: {fig_2c_euk_names_path_svg}\")\n",
    "                           except Exception as e:\n",
    "                               print(f\"Warning: Could not export Figure 2C (Top Euk Hit Names) to SVG. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "\n",
    "\n",
    "                 else:\n",
    "                      print(\"No proteins with Eukaryotic hits found in filtered groups to analyze protein names.\")\n",
    "            else:\n",
    "                 print(f\"Skipping Euk Hit Protein Name analysis: Columns '{hit_flag_col}' or '{euk_hit_protein_name_col}' not found in filtered data, or 'clean_protein_name' function is missing.\")\n",
    "\n",
    "\n",
    "        else:\n",
    "            print(\"Skipping Figure 2C annotation summary due to missing data in filtered data.\")\n",
    "\n",
    "\n",
    "        # --- Figure 2D: Broad Functional Category Distribution ---\n",
    "        print(\"\\n--- Figure 2D: Broad Functional Category Distribution ---\")\n",
    "\n",
    "        # Calculate counts and percentages using the filtered data\n",
    "        if broad_func_cat_col in df_filtered_groups.columns and group_col in df_filtered_groups.columns:\n",
    "            # Filter out unwanted categories for plotting\n",
    "            categories_to_exclude = ['Unknown/Unclassified', 'Other Specific Annotation']\n",
    "            df_category_plot_data = df_filtered_groups[\n",
    "                (~df_filtered_groups[broad_func_cat_col].isin(categories_to_exclude))\n",
    "            ].copy()\n",
    "\n",
    "            # Check if df_category_plot_data is empty after filtering categories\n",
    "            if df_category_plot_data.empty:\n",
    "                print(\"Skipping Figure 2D plotting as no functional categories remain after excluding 'Unknown/Unclassified' and 'Other Specific Annotation'.\")\n",
    "            else:\n",
    "                # Calculate counts for each category by group in the FILTERED data for plotting\n",
    "                # Use value_counts to get counts for all categories (including 0 if they exist after filtering)\n",
    "                category_counts_plot = df_category_plot_data.groupby([group_col, broad_func_cat_col]).size().reset_index(name='Count')\n",
    "\n",
    "                # Calculate percentages for plotting (based on the FILTERED data for plotting)\n",
    "                # Need to calculate total count in the filtered data (for plotting) for normalization\n",
    "                total_filtered_proteins_plot_by_group = df_category_plot_data.groupby(group_col).size().reset_index(name='Total_Filtered_Plot')\n",
    "\n",
    "                # Merge counts with total counts to calculate percentages\n",
    "                df_category_plot_summary = pd.merge(category_counts_plot, total_filtered_proteins_plot_by_group, on=group_col, how='left')\n",
    "                df_category_plot_summary['Percentage'] = (df_category_plot_summary['Count'] / df_category_plot_summary['Total_Filtered_Plot']) * 100\n",
    "                df_category_plot_summary = df_category_plot_summary.drop(columns=['Total_Filtered_Plot']) # Drop the temporary total column\n",
    "\n",
    "\n",
    "                # Save full category counts (including excluded) to a summary file\n",
    "                # Use the original df_full, filtered only by group for the full summary\n",
    "                df_full_filtered_groups_only = df_full[df_full[group_col].isin(['Asgard', 'GV'])].copy()\n",
    "                original_category_summary = df_full_filtered_groups_only.groupby(group_col)[broad_func_cat_col].value_counts().reset_index(name='Count')\n",
    "                original_category_percentages = df_full_filtered_groups_only.groupby(group_col)[broad_func_cat_col].value_counts(normalize=True).mul(100).reset_index(name='Percentage')\n",
    "                df_full_category_summary = pd.merge(original_category_summary, original_category_percentages, on=[group_col, broad_func_cat_col])\n",
    "\n",
    "\n",
    "                print(\"\\nBroad Functional Category Distribution (Counts and Percentages - FULL SUMMARY):\")\n",
    "                print(df_full_category_summary.sort_values(['Group', 'Count'], ascending=[True, False]).to_markdown(index=False))\n",
    "\n",
    "                full_category_summary_path = Path(output_figure_summary_dir) / \"figure2d_broad_functional_category_full_summary.csv\"\n",
    "                try:\n",
    "                     df_full_category_summary.to_csv(str(full_category_summary_path), index=False)\n",
    "                     print(f\"Full functional category summary saved to: {full_category_summary_path}\")\n",
    "                except Exception as e:\n",
    "                     print(f\"Error saving full category summary: {e}\")\n",
    "\n",
    "\n",
    "                # --- Filter by Count > 0 and Plot ---\n",
    "                # Check if df_category_plot_summary is not empty and has 'Count' column before filtering\n",
    "                # This check should now pass reliably if df_category_plot_data was not empty\n",
    "                if not df_category_plot_summary.empty and 'Count' in df_category_plot_summary.columns:\n",
    "                    # Only filter by Count > 0 if there are any counts > 0 to filter\n",
    "                    if (df_category_plot_summary['Count'] > 0).any():\n",
    "                        df_category_plot_summary_filtered_by_count = df_category_plot_summary[df_category_plot_summary['Count'] > 0].copy()\n",
    "                    else:\n",
    "                        # If all counts are 0, the filtered dataframe will be empty\n",
    "                        df_category_plot_summary_filtered_by_count = pd.DataFrame(columns=df_category_plot_summary.columns)\n",
    "\n",
    "\n",
    "                    # Define category order for plotting based on frequency in the PLOTTED data\n",
    "                    if not df_category_plot_summary_filtered_by_count.empty:\n",
    "                         plot_category_order = df_category_plot_summary_filtered_by_count.groupby(broad_func_cat_col)['Count'].sum().sort_values(ascending=False).index.tolist()\n",
    "\n",
    "                         fig_2d = px.bar(\n",
    "                             df_category_plot_summary_filtered_by_count, # Use the DataFrame filtered by count\n",
    "                             x=broad_func_cat_col,\n",
    "                             y='Percentage',\n",
    "                             color=group_col,\n",
    "                             barmode='group', # Side-by-side bars for Asgard vs GV\n",
    "                             # title='Broad Functional Category Distribution by Group', # No title\n",
    "                             labels={broad_func_cat_col: 'Broad Functional Category', 'Percentage': 'Percentage of Proteins (%)', group_col: 'Group'},\n",
    "                             color_discrete_map=group_colors,\n",
    "                             category_orders={broad_func_cat_col: plot_category_order}, # Apply consistent order based on plotted data\n",
    "                             template='plotly_white'\n",
    "                         )\n",
    "                         # Apply default layout and remove gridlines\n",
    "                         fig_2d.update_layout(plotly_layout_defaults)\n",
    "                         fig_2d.update_xaxes(title_text='Broad Functional Category', showgrid=False)\n",
    "                         fig_2d.update_yaxes(title_text='Percentage of Proteins (%)', showgrid=False)\n",
    "                         fig_2d.update_layout(legend_title_text='Group') # Add legend title\n",
    "                         fig_2d.update_xaxes(tickangle=45) # Angle labels if needed\n",
    "\n",
    "                         # Export Figure 2D (HTML and PDF/SVG)\n",
    "                         fig_2d_path_html = Path(output_figure_dir) / \"figure2d_broad_functional_category_distribution_filtered.html\"\n",
    "                         fig_2d.write_html(str(fig_2d_path_html))\n",
    "                         print(f\"Figure 2D HTML saved to: {fig_2d_path_html}\")\n",
    "                         \n",
    "                         # Export to PDF/SVG (requires kaleido)\n",
    "                         try:\n",
    "                             fig_2d_path_pdf = Path(output_figure_dir) / \"figure2d_broad_functional_category_distribution_filtered.pdf\"\n",
    "                             fig_2d.write_image(str(fig_2d_path_pdf))\n",
    "                             print(f\"Figure 2D PDF saved to: {fig_2d_path_pdf}\")\n",
    "                         except Exception as e:\n",
    "                             print(f\"Warning: Could not export Figure 2D to PDF. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "                         try:\n",
    "                             fig_2d_path_svg = Path(output_figure_dir) / \"figure2d_broad_functional_category_distribution_filtered.svg\"\n",
    "                             fig_2d.write_image(str(fig_2d_path_svg))\n",
    "                             print(f\"Figure 2D SVG saved to: {fig_2d_path_svg}\")\n",
    "                         except Exception as e:\n",
    "                             print(f\"Warning: Could not export Figure 2D to SVG. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "\n",
    "                    else:\n",
    "                         print(\"No functional categories remaining after filtering for plotting Figure 2D (after removing zero counts).\")\n",
    "                else:\n",
    "                     print(\"Skipping Figure 2D plot as df_category_plot_summary is empty or missing 'Count' column before filtering by count.\")\n",
    "\n",
    "\n",
    "        else:\n",
    "            print(\"Skipping Figure 2D due to missing data in filtered groups.\")\n",
    "\n",
    "    else:\n",
    "        print(\"Skipping Figure 2 generation as no 'Asgard' or 'GV' data is available after initial filtering.\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n--- Figure 2 Generation Complete ---\")\n",
    "print(f\"Figures saved to the '{output_figure_dir}' directory (HTML, PDF, SVG).\")\n",
    "print(f\"Full functional category summary saved to '{output_figure_summary_dir}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aee702-e6b3-4245-b4f4-d106325ce5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Revised Figure 2C - Annotation Summary Bar Chart\n",
    "\n",
    "# --- Imports ---\n",
    "# Assumes pandas, numpy, plotly.express, plotly.graph_objects, plotly.io,\n",
    "# and necessary variables/helper functions (like df_full, group_col,\n",
    "# protein_id_col, orthogroup_col, broad_func_cat_col, structurally_dark_col,\n",
    "# esp_col, hit_flag_col, euk_hit_protein_name_col, clean_protein_name,\n",
    "# arcadia_colors_manual, arcadia_primary_palette, arcadia_secondary_palette,\n",
    "# arcadia_neutrals_palette, output_figure_dir, output_figure_summary_dir,\n",
    "# plotly_layout_defaults) are defined in the setup cell (Cell 1).\n",
    "# Assumes df_filtered_groups is available from Cell 2 (Figure 2 generation) and\n",
    "# correctly filters for 'Asgard' and 'GV' groups.\n",
    "\n",
    "print(\"\\n\\n--- Generating Revised Figure 2C: Annotation Summary Bar Chart (Cell 4) ---\")\n",
    "print(\"Note: Orthogroup size distribution figure (Figure 2B) is being reconsidered due to skewness.\")\n",
    "print(\"Note: Top Eukaryotic Hit Protein Names are presented as a table (in Cell 3).\")\n",
    "\n",
    "\n",
    "# Ensure df_filtered_groups is available and not empty\n",
    "if 'df_filtered_groups' not in locals() or df_filtered_groups.empty:\n",
    "    print(\"ERROR: df_filtered_groups not available or is empty. Please run Cell 2 first.\")\n",
    "else:\n",
    "    # Define colors for Asgard and GV using the specified hex codes (re-defined for clarity in this cell)\n",
    "    group_colors = {\n",
    "        'Asgard': arcadia_colors_manual.get('aegean', '#5088C5'),\n",
    "        'GV': arcadia_colors_manual.get('amber', '#F28360')\n",
    "    }\n",
    "    print(f\"\\nUsing Group Colors: Asgard={group_colors.get('Asgard')}, GV={group_colors.get('GV')}\")\n",
    "\n",
    "\n",
    "    # --- Revised Figure 2C: High-Level Annotation Summary Bar Chart ---\n",
    "    print(\"\\n--- Revised Figure 2C: High-Level Annotation Summary Bar Chart ---\")\n",
    "\n",
    "    # Define annotation columns to include in THIS specific bar chart\n",
    "    # Exclude 'Has_Conserved_Motif' as requested for this plot\n",
    "    annotation_cols_for_plot = []\n",
    "    if structurally_dark_col in df_filtered_groups.columns: annotation_cols_for_plot.append(structurally_dark_col)\n",
    "    if esp_col in df_filtered_groups.columns: annotation_cols_for_plot.append(esp_col)\n",
    "    if hit_flag_col in df_filtered_groups.columns: annotation_cols_for_plot.append(hit_flag_col)\n",
    "    # Add 'Has_Any_Domain' if it was created in Cell 2\n",
    "    if 'Has_Any_Domain' in df_filtered_groups.columns: annotation_cols_for_plot.append('Has_Any_Domain')\n",
    "    # Add other relevant boolean flags if available and desired for this plot\n",
    "\n",
    "\n",
    "    annotation_plot_data = []\n",
    "    if group_col in df_filtered_groups.columns:\n",
    "        # Calculate total proteins per group in the filtered data for percentage calculation\n",
    "        total_proteins_per_group = df_filtered_groups.groupby(group_col).size()\n",
    "\n",
    "        for col in annotation_cols_for_plot:\n",
    "            # Ensure column is boolean\n",
    "            df_filtered_groups[col] = df_filtered_groups[col].astype(bool)\n",
    "\n",
    "            # Calculate counts of True values per group\n",
    "            true_counts = df_filtered_groups.groupby(group_col)[col].sum().reset_index(name='Count')\n",
    "\n",
    "            # Calculate percentages of True values per group\n",
    "            percentage_data = []\n",
    "            for index, row in true_counts.iterrows():\n",
    "                 group = row['Group']\n",
    "                 count = row['Count']\n",
    "                 total = total_proteins_per_group.get(group, 0) # Get total for this group, default to 0 if group not found\n",
    "                 percentage = (count / total * 100) if total > 0 else 0.0\n",
    "                 percentage_data.append({'Group': group, 'Percentage': percentage})\n",
    "\n",
    "            df_percentages = pd.DataFrame(percentage_data)\n",
    "\n",
    "            # Merge counts and percentages\n",
    "            summary_combined = pd.merge(true_counts, df_percentages, on='Group')\n",
    "\n",
    "            # Add annotation type column\n",
    "            summary_combined['Annotation_Type'] = col.replace('_', ' ').replace('Is ', '').strip()\n",
    "\n",
    "            # Handle ESP specifically for GV (remove if Group is GV and Annotation is ESP)\n",
    "            if col == esp_col:\n",
    "                 summary_combined = summary_combined[summary_combined['Group'] != 'GV'].copy()\n",
    "\n",
    "            if not summary_combined.empty:\n",
    "                 annotation_plot_data.append(summary_combined)\n",
    "\n",
    "    else:\n",
    "         print(f\"Warning: Group column '{group_col}' not found in filtered data. Skipping annotation summary plot.\")\n",
    "\n",
    "\n",
    "    if annotation_plot_data:\n",
    "        df_annotation_summary_plot = pd.concat(annotation_plot_data, ignore_index=True)\n",
    "\n",
    "        # Ensure all combinations of Annotation_Type and Group are present, fill missing with 0\n",
    "        # Use the original list of columns for the plot for consistent categories\n",
    "        all_plot_annotation_types = [col.replace('_', ' ').replace('Is ', '').strip() for col in annotation_cols_for_plot]\n",
    "        all_groups = df_filtered_groups[group_col].unique() # Use groups from filtered data\n",
    "\n",
    "        # Create a multi-index to ensure all combinations are covered\n",
    "        multi_index = pd.MultiIndex.from_product([all_plot_annotation_types, all_groups], names=['Annotation_Type', 'Group'])\n",
    "        df_annotation_summary_plot = df_annotation_summary_plot.set_index(['Annotation_Type', 'Group']).reindex(multi_index, fill_value=0).reset_index()\n",
    "\n",
    "\n",
    "        # Filter out rows where percentage is 0 for plotting (optional, but cleans plot)\n",
    "        # Or keep them to show 0 bars\n",
    "        # Let's keep them to show 0 bars for completeness unless specifically requested to remove\n",
    "        # df_annotation_summary_plot = df_annotation_summary_plot[df_annotation_summary_plot['Percentage'] > 0].copy()\n",
    "\n",
    "\n",
    "        # Define category order for plotting based on the order defined in annotation_cols_for_plot\n",
    "        plot_category_order = [col.replace('_', ' ').replace('Is ', '').strip() for col in annotation_cols_for_plot]\n",
    "\n",
    "        if not df_annotation_summary_plot.empty:\n",
    "             fig_2c_revised = px.bar(\n",
    "                 df_annotation_summary_plot,\n",
    "                 x='Annotation_Type',\n",
    "                 y='Percentage',\n",
    "                 color='Group',\n",
    "                 barmode='group', # Side-by-side bars for Asgard vs GV\n",
    "                 # title='High-Level Annotation Summary by Group', # No title\n",
    "                 labels={'Annotation_Type': 'Annotation Type', 'Percentage': 'Percentage of Proteins (%)', 'Group': 'Group'},\n",
    "                 color_discrete_map=group_colors,\n",
    "                 category_orders={'Annotation_Type': plot_category_order}, # Apply consistent order\n",
    "                 template='plotly_white'\n",
    "             )\n",
    "             # Apply default layout and remove gridlines\n",
    "             fig_2c_revised.update_layout(plotly_layout_defaults)\n",
    "             fig_2c_revised.update_xaxes(title_text='Annotation Type', showgrid=False)\n",
    "             fig_2c_revised.update_yaxes(title_text='Percentage of Proteins (%)', showgrid=False)\n",
    "             fig_2c_revised.update_layout(legend_title_text='Group') # Add legend title\n",
    "             fig_2c_revised.update_xaxes(tickangle=45) # Angle labels if needed\n",
    "\n",
    "             # Export Revised Figure 2C (HTML, PDF, SVG)\n",
    "             fig_2c_revised_path_html = Path(output_figure_dir) / \"figure2c_annotation_summary_revised_bar.html\"\n",
    "             fig_2c_revised.write_html(str(fig_2c_revised_path_html))\n",
    "             print(f\"Revised Figure 2C HTML saved to: {fig_2c_revised_path_html}\")\n",
    "\n",
    "             # Export to PDF/SVG (requires kaleido)\n",
    "             try:\n",
    "                 fig_2c_revised_path_pdf = Path(output_figure_dir) / \"figure2c_annotation_summary_revised_bar.pdf\"\n",
    "                 fig_2c_revised.write_image(str(fig_2c_revised_path_pdf))\n",
    "                 print(f\"Revised Figure 2C PDF saved to: {fig_2c_revised_path_pdf}\")\n",
    "             except Exception as e:\n",
    "                 print(f\"Warning: Could not export Revised Figure 2C to PDF. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "             try:\n",
    "                 fig_2c_revised_path_svg = Path(output_figure_dir) / \"figure2c_annotation_summary_revised_bar.svg\"\n",
    "                 fig_2c_revised.write_image(str(fig_2c_revised_path_svg))\n",
    "                 print(f\"Revised Figure 2C SVG saved to: {fig_2c_revised_path_svg}\")\n",
    "             except Exception as e:\n",
    "                 print(f\"Warning: Could not export Revised Figure 2C to SVG. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "\n",
    "        else:\n",
    "             print(\"No annotation data remaining after processing for plotting Revised Figure 2C.\")\n",
    "\n",
    "    else:\n",
    "        print(\"Skipping Revised Figure 2C plotting as no annotation columns were defined or data is missing.\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n--- Cell 4 (Revised Figure 2C) Complete ---\")\n",
    "print(f\"Revised Figure 2C saved to the '{output_figure_dir}' directory (HTML, PDF, SVG).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707423e5-28f2-40ac-80ae-da2662119861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Figure 3 - Localization and Functional Category Distribution\n",
    "\n",
    "# --- Imports ---\n",
    "# Assumes pandas, numpy, plotly.express, plotly.graph_objects, plotly.io,\n",
    "# and necessary variables/helper functions (like df_full, group_col,\n",
    "# protein_id_col, localization_col, broad_func_cat_col,\n",
    "# arcadia_colors_manual, arcadia_primary_palette, arcadia_secondary_palette,\n",
    "# arcadia_neutrals_palette, output_figure_dir, output_figure_summary_dir,\n",
    "# plotly_layout_defaults, localization_color_map, broad_category_color_map)\n",
    "# are defined in the setup cell (Cell 1).\n",
    "# Assumes df_filtered_groups is available from Cell 2 (Figure 2 generation) and\n",
    "# correctly filters for 'Asgard' and 'GV' groups.\n",
    "\n",
    "print(\"\\n\\n--- Generating Figure 3: Localization and Functional Category Distribution (Cell 5) ---\")\n",
    "\n",
    "# Ensure df_filtered_groups is available and not empty\n",
    "if 'df_filtered_groups' not in locals() or df_filtered_groups.empty:\n",
    "    print(\"ERROR: df_filtered_groups not available or is empty. Please run Cell 2 first.\")\n",
    "else:\n",
    "    # Define colors for Asgard and GV using the specified hex codes (re-defined for clarity in this cell)\n",
    "    group_colors = {\n",
    "        'Asgard': arcadia_colors_manual.get('aegean', '#5088C5'),\n",
    "        'GV': arcadia_colors_manual.get('amber', '#F28360')\n",
    "    }\n",
    "    print(f\"\\nUsing Group Colors: Asgard={group_colors.get('Asgard')}, GV={group_colors.get('GV')}\")\n",
    "\n",
    "\n",
    "    # --- Figure 3A: Predicted Subcellular Localization Distribution ---\n",
    "    print(\"\\n--- Figure 3A: Predicted Subcellular Localization Distribution ---\")\n",
    "\n",
    "    if localization_col in df_filtered_groups.columns and group_col in df_filtered_groups.columns:\n",
    "        # Calculate counts for each localization category by group\n",
    "        localization_counts = df_filtered_groups.groupby(group_col)[localization_col].value_counts().reset_index(name='Count')\n",
    "\n",
    "        # Calculate percentages for plotting\n",
    "        localization_percentages = df_filtered_groups.groupby(group_col)[localization_col].value_counts(normalize=True).mul(100).reset_index(name='Percentage')\n",
    "\n",
    "        # Merge counts and percentages\n",
    "        df_localization_summary = pd.merge(localization_counts, localization_percentages, on=[group_col, localization_col])\n",
    "\n",
    "        print(\"\\nPredicted Subcellular Localization Distribution (Counts and Percentages):\")\n",
    "        print(df_localization_summary.sort_values(['Group', 'Count'], ascending=[True, False]).to_markdown(index=False))\n",
    "\n",
    "        # Save full localization summary to a summary file\n",
    "        full_localization_summary_path = Path(output_figure_summary_dir) / \"figure3a_localization_full_summary.csv\"\n",
    "        try:\n",
    "             df_localization_summary.to_csv(str(full_localization_summary_path), index=False)\n",
    "             print(f\"Full localization summary saved to: {full_localization_summary_path}\")\n",
    "        except Exception as e:\n",
    "             print(f\"Error saving full localization summary: {e}\")\n",
    "\n",
    "        # Define category order based on overall frequency for consistent plotting\n",
    "        overall_localization_order = df_filtered_groups[localization_col].value_counts().index.tolist()\n",
    "\n",
    "        if not df_localization_summary.empty:\n",
    "             fig_3a = px.bar(\n",
    "                 df_localization_summary,\n",
    "                 x=group_col, # X-axis is Group\n",
    "                 y='Percentage',\n",
    "                 color=localization_col, # Color is Localization Category\n",
    "                 barmode='group', # Group bars by X-axis value (Group)\n",
    "                 # title='Predicted Subcellular Localization Distribution by Group', # No title\n",
    "                 labels={localization_col: 'Predicted Subcellular Localization', 'Percentage': 'Percentage of Proteins (%)', group_col: 'Group'},\n",
    "                 color_discrete_map=localization_color_map, # Use localization color map\n",
    "                 category_orders={group_col: ['Asgard', 'GV'], localization_col: overall_localization_order}, # Apply consistent order\n",
    "                 template='plotly_white'\n",
    "             )\n",
    "             # Apply default layout and remove gridlines\n",
    "             fig_3a.update_layout(plotly_layout_defaults)\n",
    "             fig_3a.update_xaxes(title_text='Group', showgrid=False) # X-axis title is Group\n",
    "             fig_3a.update_yaxes(title_text='Percentage of Proteins (%)', showgrid=False)\n",
    "             fig_3a.update_layout(legend_title_text='Predicted Subcellular Localization') # Add legend title\n",
    "             fig_3a.update_xaxes(tickangle=0) # No angle needed for Group labels\n",
    "\n",
    "\n",
    "             # Export Figure 3A (HTML, PDF, SVG)\n",
    "             fig_3a_path_html = Path(output_figure_dir) / \"figure3a_localization_distribution.html\"\n",
    "             fig_3a.write_html(str(fig_3a_path_html))\n",
    "             print(f\"Figure 3A HTML saved to: {fig_3a_path_html}\")\n",
    "\n",
    "             # Export to PDF/SVG (requires kaleido)\n",
    "             try:\n",
    "                 fig_3a_path_pdf = Path(output_figure_dir) / \"figure3a_localization_distribution.pdf\"\n",
    "                 fig_3a.write_image(str(fig_3a_path_pdf))\n",
    "                 print(f\"Figure 3A PDF saved to: {fig_3a_path_pdf}\")\n",
    "             except Exception as e:\n",
    "                 print(f\"Warning: Could not export Figure 3A to PDF. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "             try:\n",
    "                 fig_3a_path_svg = Path(output_figure_dir) / \"figure3a_localization_distribution.svg\"\n",
    "                 fig_3a.write_image(str(fig_3a_path_svg))\n",
    "                 print(f\"Figure 3A SVG saved to: {fig_3a_path_svg}\")\n",
    "             except Exception as e:\n",
    "                 print(f\"Warning: Could not export Figure 3A to SVG. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "\n",
    "        else:\n",
    "             print(\"No localization data available for plotting Figure 3A.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Skipping Figure 3A due to missing columns in filtered data: '{localization_col}' or '{group_col}'.\")\n",
    "\n",
    "\n",
    "    # --- Figure 3B: Broad Functional Category Distribution ---\n",
    "    print(\"\\n--- Figure 3B: Broad Functional Category Distribution ---\")\n",
    "\n",
    "    if broad_func_cat_col in df_filtered_groups.columns and group_col in df_filtered_groups.columns:\n",
    "        # Filter out unwanted categories for plotting (same as Figure 2D)\n",
    "        categories_to_exclude = ['Unknown/Unclassified', 'Other Specific Annotation']\n",
    "        df_category_plot_data = df_filtered_groups[\n",
    "            (~df_filtered_groups[broad_func_cat_col].isin(categories_to_exclude))\n",
    "        ].copy()\n",
    "\n",
    "        # Check if df_category_plot_data is empty after filtering categories\n",
    "        if df_category_plot_data.empty:\n",
    "            print(\"Skipping Figure 3B plotting as no functional categories remain after excluding 'Unknown/Unclassified' and 'Other Specific Annotation'.\")\n",
    "        else:\n",
    "            # Calculate counts for each category by group in the FILTERED data for plotting\n",
    "            # Use value_counts to get counts for all categories (including 0 if they exist after filtering)\n",
    "            category_counts_plot = df_category_plot_data.groupby([group_col, broad_func_cat_col]).size().reset_index(name='Count')\n",
    "\n",
    "            # Calculate percentages for plotting (based on the FILTERED data for plotting)\n",
    "            # Need to calculate total count in the filtered data (for plotting) for normalization\n",
    "            total_filtered_proteins_plot_by_group = df_category_plot_data.groupby(group_col).size().reset_index(name='Total_Filtered_Plot')\n",
    "\n",
    "            # Merge counts with total counts to calculate percentages\n",
    "            df_category_plot_summary = pd.merge(category_counts_plot, total_filtered_proteins_plot_by_group, on=group_col, how='left')\n",
    "            df_category_plot_summary['Percentage'] = (df_category_plot_summary['Count'] / df_category_plot_summary['Total_Filtered_Plot']) * 100\n",
    "            df_category_plot_summary = df_category_plot_summary.drop(columns=['Total_Filtered_Plot']) # Drop the temporary total column\n",
    "\n",
    "\n",
    "            # --- Filter by Count > 0 and Plot ---\n",
    "            # Check if df_category_plot_summary is not empty and has 'Count' column before filtering\n",
    "            if not df_category_plot_summary.empty and 'Count' in df_category_plot_summary.columns:\n",
    "                # Only filter by Count > 0 if there are any counts > 0 to filter\n",
    "                if (df_category_plot_summary['Count'] > 0).any():\n",
    "                    df_category_plot_summary_filtered_by_count = df_category_plot_summary[df_category_plot_summary['Count'] > 0].copy()\n",
    "                else:\n",
    "                    # If all counts are 0, the filtered dataframe will be empty\n",
    "                    df_category_plot_summary_filtered_by_count = pd.DataFrame(columns=df_category_plot_summary.columns)\n",
    "\n",
    "\n",
    "                # Define category order for plotting based on frequency in the PLOTTED data\n",
    "                if not df_category_plot_summary_filtered_by_count.empty:\n",
    "                     plot_category_order = df_category_plot_summary_filtered_by_count.groupby(broad_func_cat_col)['Count'].sum().sort_values(ascending=False).index.tolist()\n",
    "\n",
    "                     fig_3b = px.bar(\n",
    "                         df_category_plot_summary_filtered_by_count, # Use the DataFrame filtered by count\n",
    "                         x=group_col, # X-axis is Group\n",
    "                         y='Percentage',\n",
    "                         color=broad_func_cat_col, # Color is Functional Category\n",
    "                         barmode='group', # Group bars by X-axis value (Group)\n",
    "                         # title='Broad Functional Category Distribution by Group', # No title\n",
    "                         labels={broad_func_cat_col: 'Broad Functional Category', 'Percentage': 'Percentage of Proteins (%)', group_col: 'Group'},\n",
    "                         color_discrete_map=broad_category_color_map, # Use functional category color map\n",
    "                         category_orders={group_col: ['Asgard', 'GV'], broad_func_cat_col: plot_category_order}, # Apply consistent order\n",
    "                         template='plotly_white'\n",
    "                     )\n",
    "                     # Apply default layout and remove gridlines\n",
    "                     fig_3b.update_layout(plotly_layout_defaults)\n",
    "                     fig_3b.update_xaxes(title_text='Group', showgrid=False) # X-axis title is Group\n",
    "                     fig_3b.update_yaxes(title_text='Percentage of Proteins (%)', showgrid=False)\n",
    "                     fig_3b.update_layout(legend_title_text='Broad Functional Category') # Add legend title\n",
    "                     fig_3b.update_xaxes(tickangle=0) # No angle needed for Group labels\n",
    "\n",
    "\n",
    "                     # Export Figure 3B (HTML, PDF, SVG)\n",
    "                     fig_3b_path_html = Path(output_figure_dir) / \"figure3b_broad_functional_category_distribution_filtered.html\"\n",
    "                     fig_3b.write_html(str(fig_3b_path_html))\n",
    "                     print(f\"Figure 3B HTML saved to: {fig_3b_path_html}\")\n",
    "\n",
    "                     # Export to PDF/SVG (requires kaleido)\n",
    "                     try:\n",
    "                         fig_3b_path_pdf = Path(output_figure_dir) / \"figure3b_broad_functional_category_distribution_filtered.pdf\"\n",
    "                         fig_3b.write_image(str(fig_3b_path_pdf))\n",
    "                         print(f\"Figure 3B PDF saved to: {fig_3b_path_pdf}\")\n",
    "                     except Exception as e:\n",
    "                         print(f\"Warning: Could not export Figure 3B to PDF. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "                     try:\n",
    "                         fig_3b_path_svg = Path(output_figure_dir) / \"figure3b_broad_functional_category_distribution_filtered.svg\"\n",
    "                         fig_3b.write_image(str(fig_3b_path_svg))\n",
    "                         print(f\"Figure 3B SVG saved to: {fig_3b_path_svg}\")\n",
    "                     except Exception as e:\n",
    "                         print(f\"Warning: Could not export Figure 3B to SVG. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "\n",
    "print(\"\\n\\n--- Figure 3 Generation Complete ---\")\n",
    "print(f\"Figures saved to the '{output_figure_dir}' directory (HTML, PDF, SVG).\")\n",
    "# Note: Full functional category summary is saved in Cell 2 (Figure 2D)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e22cdf-fd65-431a-932f-4b2d6c6f7dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Figure 4 - Genome and Protein Counts by Taxonomy\n",
    "\n",
    "# --- Imports ---\n",
    "# Assumes pandas, numpy, plotly.express, plotly.graph_objects, plotly.io,\n",
    "# and necessary variables/helper functions (like df_full, group_col,\n",
    "# protein_id_col, source_genome_accession_col, asgard_phylum_col,\n",
    "# virus_family_col, arcadia_colors_manual, arcadia_primary_palette,\n",
    "# arcadia_secondary_palette, arcadia_neutrals_palette, output_figure_dir,\n",
    "# plotly_layout_defaults)\n",
    "# are defined in the setup cell (Cell 1).\n",
    "# Assumes df_filtered_groups is available from Cell 2 (Figure 2 generation) and\n",
    "# correctly filters for 'Asgard' and 'GV' groups.\n",
    "\n",
    "print(\"\\n\\n--- Generating Figure 4: Genome and Protein Counts by Taxonomy (Cell 6) ---\")\n",
    "\n",
    "# Ensure df_filtered_groups is available and not empty\n",
    "if 'df_filtered_groups' not in locals() or df_filtered_groups.empty:\n",
    "    print(\"ERROR: df_filtered_groups not available or is empty. Please run Cell 2 first.\")\n",
    "else:\n",
    "    # Define colors for Asgard and GV groups (re-defined for context):\n",
    "    group_colors = {\n",
    "        'Asgard': arcadia_colors_manual.get('aegean', '#5088C5'),\n",
    "        'GV': arcadia_colors_manual.get('amber', '#F28360')\n",
    "    }\n",
    "    print(f\"\\nUsing Group Colors (for context): Asgard={group_colors.get('Asgard')}, GV={group_colors.get('Amber')}\")\n",
    "\n",
    "\n",
    "    # --- Calculate Genome and Protein Counts by Taxonomy ---\n",
    "    print(\"\\n--- Calculating Genome and Protein Counts by Taxonomy ---\")\n",
    "\n",
    "    genome_counts_by_group_taxonomy = []\n",
    "    protein_counts_by_group_taxonomy = []\n",
    "    all_taxonomic_units_present = set() # Collect all unique units from both datasets\n",
    "\n",
    "    if source_genome_accession_col in df_filtered_groups.columns and group_col in df_filtered_groups.columns and asgard_phylum_col in df_filtered_groups.columns and virus_family_col in df_filtered_groups.columns and protein_id_col in df_filtered_groups.columns:\n",
    "\n",
    "        # Process Asgard counts by Phylum\n",
    "        df_asgard = df_filtered_groups[df_filtered_groups[group_col] == 'Asgard'].copy()\n",
    "        if not df_asgard.empty:\n",
    "            if asgard_phylum_col in df_asgard.columns:\n",
    "                # Genome counts\n",
    "                asgard_genome_counts = df_asgard.groupby([group_col, asgard_phylum_col])[source_genome_accession_col].nunique().reset_index(name='Count')\n",
    "                asgard_genome_counts.rename(columns={asgard_phylum_col: 'Taxonomic_Unit'}, inplace=True)\n",
    "                genome_counts_by_group_taxonomy.append(asgard_genome_counts)\n",
    "                all_taxonomic_units_present.update(asgard_genome_counts['Taxonomic_Unit'].dropna().unique())\n",
    "\n",
    "                # Protein counts\n",
    "                asgard_protein_counts = df_asgard.groupby([group_col, asgard_phylum_col])[protein_id_col].count().reset_index(name='Count')\n",
    "                asgard_protein_counts.rename(columns={asgard_phylum_col: 'Taxonomic_Unit'}, inplace=True)\n",
    "                protein_counts_by_group_taxonomy.append(asgard_protein_counts)\n",
    "                all_taxonomic_units_present.update(asgard_protein_counts['Taxonomic_Unit'].dropna().unique())\n",
    "            else:\n",
    "                 print(f\"Warning: Skipping Asgard counts. Column '{asgard_phylum_col}' missing.\")\n",
    "        else:\n",
    "             print(\"Warning: Skipping Asgard counts. Data is empty.\")\n",
    "\n",
    "\n",
    "        # Process GV counts by Family\n",
    "        df_gv = df_filtered_groups[df_filtered_groups[group_col] == 'GV'].copy()\n",
    "        if not df_gv.empty:\n",
    "            if virus_family_col in df_gv.columns:\n",
    "                # Genome counts\n",
    "                gv_genome_counts = df_gv.groupby([group_col, virus_family_col])[source_genome_accession_col].nunique().reset_index(name='Count')\n",
    "                gv_genome_counts.rename(columns={virus_family_col: 'Taxonomic_Unit'}, inplace=True)\n",
    "                genome_counts_by_group_taxonomy.append(gv_genome_counts)\n",
    "                all_taxonomic_units_present.update(gv_genome_counts['Taxonomic_Unit'].dropna().unique())\n",
    "\n",
    "                # Protein counts\n",
    "                gv_protein_counts = df_gv.groupby([group_col, virus_family_col])[protein_id_col].count().reset_index(name='Count')\n",
    "                gv_protein_counts.rename(columns={virus_family_col: 'Taxonomic_Unit'}, inplace=True)\n",
    "                protein_counts_by_group_taxonomy.append(gv_protein_counts)\n",
    "                all_taxonomic_units_present.update(gv_protein_counts['Taxonomic_Unit'].dropna().unique())\n",
    "            else:\n",
    "                 print(f\"Warning: Skipping GV counts. Column '{virus_family_col}' missing.\")\n",
    "        else:\n",
    "             print(\"Warning: Skipping GV counts. Data is empty.\")\n",
    "\n",
    "\n",
    "        # Combine results into DataFrames\n",
    "        df_genome_counts_taxonomy = pd.concat(genome_counts_by_group_taxonomy, ignore_index=True) if genome_counts_by_group_taxonomy else pd.DataFrame()\n",
    "        df_protein_counts_taxonomy = pd.concat(protein_counts_by_group_taxonomy, ignore_index=True) if protein_counts_by_group_taxonomy else pd.DataFrame()\n",
    "\n",
    "        print(\"\\nGenome Counts by Group and Taxonomy (Partial):\")\n",
    "        print(df_genome_counts_taxonomy.to_markdown(index=False))\n",
    "        print(\"\\nProtein Counts by Group and Taxonomy (Partial):\")\n",
    "        print(df_protein_counts_taxonomy.to_markdown(index=False))\n",
    "\n",
    "\n",
    "        # --- Create Comprehensive Taxonomic Unit Color Map ---\n",
    "        # Use a combination of Arcadia palettes for the color map\n",
    "        # Prioritize primary, then secondary, then neutrals/monochromatic shades\n",
    "        # Use a larger pool of colors to minimize repetition\n",
    "        full_arcadia_palette = arcadia_primary_palette + arcadia_secondary_palette + arcadia_neutrals_palette + [\n",
    "            arcadia_colors_manual.get('lapis', '#2B66A2'), arcadia_colors_manual.get('dusk', '#094468'), # Blue shades\n",
    "            arcadia_colors_manual.get('melon', '#FFCFAF'), arcadia_colors_manual.get('cinnabar', '#9E3F41'), # Orange/Red shades\n",
    "            arcadia_colors_manual.get('sun', '#FFD364'), arcadia_colors_manual.get('mustard', '#D68D22'), # Yellow shades\n",
    "            arcadia_colors_manual.get('iris', '#DCDFEF'), arcadia_colors_manual.get('tanzanite', '#54448C'), # Purple shades\n",
    "            arcadia_colors_manual.get('glass', '#C3E2DB'), arcadia_colors_manual.get('asparagus', '#2A6B5E'), # Teal shades\n",
    "            arcadia_colors_manual.get('putty', '#FFE3D4'), arcadia_colors_manual.get('candy', '#E2718F'), # Pink shades\n",
    "            arcadia_colors_manual.get('stone', '#EDE6DA'), arcadia_colors_manual.get('dove', '#CAD4DB'), # Warm/Cool gray shades\n",
    "            arcadia_colors_manual.get('steel', '#687787'), arcadia_colors_manual.get('mud', '#635C5A') # More neutrals\n",
    "        ]\n",
    "\n",
    "        # Assign colors to ALL unique taxonomic units found in either dataset\n",
    "        combined_taxonomy_color_map = {}\n",
    "        sorted_all_taxonomic_units = sorted(list(all_taxonomic_units_present)) # Sort for consistent assignment\n",
    "        for i, unit in enumerate(sorted_all_taxonomic_units):\n",
    "            combined_taxonomy_color_map[unit] = full_arcadia_palette[i % len(full_arcadia_palette)]\n",
    "\n",
    "        # Add 'Unknown Phylum'/'Unknown Family' if they exist and weren't in the unique list\n",
    "        if 'Unknown Phylum' not in combined_taxonomy_color_map:\n",
    "             combined_taxonomy_color_map['Unknown Phylum'] = arcadia_colors_manual.get('gray', '#bdbdbd')\n",
    "        if 'Unknown Family' not in combined_taxonomy_color_map:\n",
    "             combined_taxonomy_color_map['Unknown Family'] = arcadia_colors_manual.get('gray', '#bdbdbd')\n",
    "\n",
    "\n",
    "        print(f\"\\nCombined taxonomic unit color map created for {len(combined_taxonomy_color_map)} units.\")\n",
    "\n",
    "\n",
    "        # --- Figure 4A: Number of Genomes by Taxonomy (Stacked Bar and Pie Charts) ---\n",
    "        print(\"\\n--- Figure 4A: Number of Genomes by Taxonomy ---\")\n",
    "\n",
    "        if not df_genome_counts_taxonomy.empty:\n",
    "            # --- Figure 4A Part 1: Stacked Bar Chart ---\n",
    "            # Calculate total count for each Taxonomic_Unit across both groups for sorting\n",
    "            taxonomic_unit_total_counts_genomes = df_genome_counts_taxonomy.groupby('Taxonomic_Unit')['Count'].sum().sort_values(ascending=False)\n",
    "            # Get the order of taxonomic units based on total counts\n",
    "            taxonomic_unit_order_genomes = taxonomic_unit_total_counts_genomes.index.tolist()\n",
    "\n",
    "\n",
    "            fig_4a_bar = px.bar(\n",
    "                df_genome_counts_taxonomy,\n",
    "                x=group_col, # X-axis is Group\n",
    "                y='Count',\n",
    "                color='Taxonomic_Unit', # Color is Phylum or Family\n",
    "                barmode='stack', # Stack bars by Taxonomic_Unit\n",
    "                # title='Number of Genomes by Group and Taxonomy (Stacked Bar)', # No title\n",
    "                labels={group_col: 'Group', 'Count': 'Number of Genomes', 'Taxonomic_Unit': 'Phylum or Family'},\n",
    "                color_discrete_map=combined_taxonomy_color_map, # Use combined color map\n",
    "                category_orders={group_col: ['Asgard', 'GV'], 'Taxonomic_Unit': taxonomic_unit_order_genomes}, # Ensure Group order and Taxonomic Unit order\n",
    "                template='plotly_white'\n",
    "            )\n",
    "            # Apply default layout and remove gridlines\n",
    "            fig_4a_bar.update_layout(plotly_layout_defaults)\n",
    "            fig_4a_bar.update_xaxes(title_text='Group', showgrid=False)\n",
    "            fig_4a_bar.update_yaxes(title_text='Number of Genomes', showgrid=False)\n",
    "            fig_4a_bar.update_layout(legend_title_text='Taxonomic Unit') # Add legend title\n",
    "\n",
    "\n",
    "            # Export Figure 4A Part 1 (HTML, PDF, SVG)\n",
    "            fig_4a_bar_path_html = Path(output_figure_dir) / \"figure4a_genomes_by_taxonomy_stackedbar.html\"\n",
    "            fig_4a_bar.write_html(str(fig_4a_bar_path_html))\n",
    "            print(f\"Figure 4A (Stacked Bar) HTML saved to: {fig_4a_bar_path_html}\")\n",
    "\n",
    "            # Export to PDF/SVG (requires kaleido)\n",
    "            try:\n",
    "                fig_4a_bar_path_pdf = Path(output_figure_dir) / \"figure4a_genomes_by_taxonomy_stackedbar.pdf\"\n",
    "                fig_4a_bar.write_image(str(fig_4a_bar_path_pdf))\n",
    "                print(f\"Figure 4A (Stacked Bar) PDF saved to: {fig_4a_bar_path_pdf}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not export Figure 4A (Stacked Bar) to PDF. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "            try:\n",
    "                fig_4a_bar_path_svg = Path(output_figure_dir) / \"figure4a_genomes_by_taxonomy_stackedbar.svg\"\n",
    "                fig_4a_bar.write_image(str(fig_4a_bar_path_svg))\n",
    "                print(f\"Figure 4A (Stacked Bar) SVG saved to: {fig_4a_bar_path_svg}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not export Figure 4A (Stacked Bar) to SVG. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "\n",
    "\n",
    "            # --- Figure 4A Part 2: Pie Charts (Asgard and GV) ---\n",
    "            print(\"\\n--- Figure 4A Part 2: Genome Counts Pie Charts (Asgard and GV) ---\")\n",
    "\n",
    "            # Asgard Genome Pie Chart\n",
    "            df_asgard_genome_pie = df_genome_counts_taxonomy[df_genome_counts_taxonomy[group_col] == 'Asgard'].copy()\n",
    "            if not df_asgard_genome_pie.empty:\n",
    "                 # Sort Asgard phyla by count for pie chart order\n",
    "                 df_asgard_genome_pie = df_asgard_genome_pie.sort_values('Count', ascending=False).copy()\n",
    "                 fig_4a_pie_asgard = go.Figure(data=[go.Pie(\n",
    "                     labels=df_asgard_genome_pie['Taxonomic_Unit'],\n",
    "                     values=df_asgard_genome_pie['Count'],\n",
    "                     hole=.3, # Creates a donut chart\n",
    "                     marker=dict(colors=[combined_taxonomy_color_map.get(unit, 'gray') for unit in df_asgard_genome_pie['Taxonomic_Unit']]) # Apply colors\n",
    "                 )])\n",
    "                 # fig_4a_pie_asgard.update_layout(title_text='Asgard Genome Counts by Phylum') # No title\n",
    "                 fig_4a_pie_asgard.update_layout(plotly_layout_defaults) # Apply defaults (no title, etc.)\n",
    "                 fig_4a_pie_asgard.update_layout(margin=dict(t=0, b=0, l=0, r=0)) # Adjust margins\n",
    "                 fig_4a_pie_asgard.update_traces(textinfo='percent+label', insidetextorientation='radial')\n",
    "\n",
    "                 # Export Figure 4A Pie Asgard\n",
    "                 fig_4a_pie_asgard_path_html = Path(output_figure_dir) / \"figure4a_genomes_by_phylum_pie_asgard.html\"\n",
    "                 fig_4a_pie_asgard.write_html(str(fig_4a_pie_asgard_path_html))\n",
    "                 print(f\"Figure 4A (Asgard Pie) HTML saved to: {fig_4a_pie_asgard_path_html}\")\n",
    "                 try:\n",
    "                     fig_4a_pie_asgard_path_pdf = Path(output_figure_dir) / \"figure4a_genomes_by_phylum_pie_asgard.pdf\"\n",
    "                     fig_4a_pie_asgard.write_image(str(fig_4a_pie_asgard_path_pdf))\n",
    "                     print(f\"Figure 4A (Asgard Pie) PDF saved to: {fig_4a_pie_asgard_path_pdf}\")\n",
    "                 except Exception as e:\n",
    "                     print(f\"Warning: Could not export Figure 4A (Asgard Pie) to PDF. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "                 try:\n",
    "                     fig_4a_pie_asgard_path_svg = Path(output_figure_dir) / \"figure4a_genomes_by_phylum_pie_asgard.svg\"\n",
    "                     fig_4a_pie_asgard.write_image(str(fig_4a_pie_asgard_path_svg))\n",
    "                     print(f\"Figure 4A (Asgard Pie) SVG saved to: {fig_4a_pie_asgard_path_svg}\")\n",
    "                 except Exception as e:\n",
    "                     print(f\"Warning: Could not export Figure 4A (Asgard Pie) to SVG. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "\n",
    "            else:\n",
    "                 print(\"No Asgard genome data for pie chart.\")\n",
    "\n",
    "            # GV Genome Pie Chart\n",
    "            df_gv_genome_pie = df_genome_counts_taxonomy[df_genome_counts_taxonomy[group_col] == 'GV'].copy()\n",
    "            if not df_gv_genome_pie.empty:\n",
    "                 # Sort GV families by count for pie chart order\n",
    "                 df_gv_genome_pie = df_gv_genome_pie.sort_values('Count', ascending=False).copy()\n",
    "                 fig_4a_pie_gv = go.Figure(data=[go.Pie(\n",
    "                     labels=df_gv_genome_pie['Taxonomic_Unit'],\n",
    "                     values=df_gv_genome_pie['Count'],\n",
    "                     hole=.3, # Creates a donut chart\n",
    "                     marker=dict(colors=[combined_taxonomy_color_map.get(unit, 'gray') for unit in df_gv_genome_pie['Taxonomic_Unit']]) # Apply colors\n",
    "                 )])\n",
    "                 # fig_4a_pie_gv.update_layout(title_text='GV Genome Counts by Family') # No title\n",
    "                 fig_4a_pie_gv.update_layout(plotly_layout_defaults) # Apply defaults (no title, etc.)\n",
    "                 fig_4a_pie_gv.update_layout(margin=dict(t=0, b=0, l=0, r=0)) # Adjust margins\n",
    "                 fig_4a_pie_gv.update_traces(textinfo='percent+label', insidetextorientation='radial')\n",
    "\n",
    "                 # Export Figure 4A Pie GV\n",
    "                 fig_4a_pie_gv_path_html = Path(output_figure_dir) / \"figure4a_genomes_by_family_pie_gv.html\"\n",
    "                 fig_4a_pie_gv.write_html(str(fig_4a_pie_gv_path_html))\n",
    "                 print(f\"Figure 4A (GV Pie) HTML saved to: {fig_4a_pie_gv_path_html}\")\n",
    "                 try:\n",
    "                     fig_4a_pie_gv_path_pdf = Path(output_figure_dir) / \"figure4a_genomes_by_family_pie_gv.pdf\"\n",
    "                     fig_4a_pie_gv.write_image(str(fig_4a_pie_gv_path_pdf))\n",
    "                     print(f\"Figure 4A (GV Pie) PDF saved to: {fig_4a_pie_gv_path_pdf}\")\n",
    "                 except Exception as e:\n",
    "                     print(f\"Warning: Could not export Figure 4A (GV Pie) to PDF. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "                 try:\n",
    "                     fig_4a_pie_gv_path_svg = Path(output_figure_dir) / \"figure4a_genomes_by_family_pie_gv.svg\"\n",
    "                     fig_4a_pie_gv.write_image(str(fig_4a_pie_gv_path_svg))\n",
    "                     print(f\"Figure 4A (GV Pie) SVG saved to: {fig_4a_pie_gv_path_svg}\")\n",
    "                 except Exception as e:\n",
    "                     print(f\"Warning: Could not export Figure 4A (GV Pie) to SVG. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "\n",
    "            else:\n",
    "                 print(\"No GV genome data for pie chart.\")\n",
    "\n",
    "        else:\n",
    "            print(\"No genome counts by taxonomy available for plotting Figure 4A.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Skipping Figure 4A due to missing columns in filtered data: '{source_genome_accession_col}', '{group_col}', '{asgard_phylum_col}', or '{virus_family_col}'.\")\n",
    "\n",
    "\n",
    "    # --- Figure 4B: Number of Proteins by Taxonomy (Stacked Bar and Pie Charts) ---\n",
    "    print(\"\\n--- Figure 4B: Number of Proteins by Taxonomy ---\")\n",
    "\n",
    "    if protein_id_col in df_filtered_groups.columns and group_col in df_filtered_groups.columns and asgard_phylum_col in df_filtered_groups.columns and virus_family_col in df_filtered_groups.columns:\n",
    "\n",
    "        # Calculate protein counts by Group and then by Phylum/Family\n",
    "        protein_counts_by_group_taxonomy = []\n",
    "\n",
    "        # Process Asgard proteins by Phylum\n",
    "        df_asgard_proteins = df_filtered_groups[df_filtered_groups[group_col] == 'Asgard'].copy()\n",
    "        if not df_asgard_proteins.empty and asgard_phylum_col in df_asgard_proteins.columns:\n",
    "            asgard_protein_counts = df_asgard_proteins.groupby([group_col, asgard_phylum_col])[protein_id_col].count().reset_index(name='Count')\n",
    "            asgard_protein_counts.rename(columns={asgard_phylum_col: 'Taxonomic_Unit'}, inplace=True)\n",
    "            protein_counts_by_group_taxonomy.append(asgard_protein_counts)\n",
    "        else:\n",
    "             print(f\"Warning: Skipping Asgard protein counts by Phylum. Data empty or column '{asgard_phylum_col}' missing.\")\n",
    "\n",
    "\n",
    "        # Process GV proteins by Family\n",
    "        df_gv_proteins = df_filtered_groups[df_filtered_groups[group_col] == 'GV'].copy()\n",
    "        if not df_gv_proteins.empty and virus_family_col in df_gv_proteins.columns:\n",
    "            gv_protein_counts = df_gv_proteins.groupby([group_col, virus_family_col])[protein_id_col].count().reset_index(name='Count')\n",
    "            gv_protein_counts.rename(columns={virus_family_col: 'Taxonomic_Unit'}, inplace=True)\n",
    "            protein_counts_by_group_taxonomy.append(gv_protein_counts)\n",
    "        else:\n",
    "             print(f\"Warning: Skipping GV protein counts by Family. Data empty or column '{virus_family_col}' missing.\")\n",
    "\n",
    "\n",
    "        if protein_counts_by_group_taxonomy:\n",
    "            df_protein_counts_taxonomy = pd.concat(protein_counts_by_group_taxonomy, ignore_index=True)\n",
    "            print(\"\\nProtein Counts by Group and Taxonomy:\")\n",
    "            print(df_protein_counts_taxonomy.to_markdown(index=False))\n",
    "\n",
    "            # Combine color maps for plotting (same as for genomes)\n",
    "            # This is already done above based on ALL unique units from both datasets\n",
    "\n",
    "            # --- Figure 4B Part 1: Stacked Bar Chart ---\n",
    "            # Calculate total count for each Taxonomic_Unit across both groups for sorting\n",
    "            taxonomic_unit_total_counts = df_protein_counts_taxonomy.groupby('Taxonomic_Unit')['Count'].sum().sort_values(ascending=False)\n",
    "            # Get the order of taxonomic units based on total counts\n",
    "            taxonomic_unit_order = taxonomic_unit_total_counts.index.tolist()\n",
    "\n",
    "            fig_4b_bar = px.bar(\n",
    "                df_protein_counts_taxonomy,\n",
    "                x=group_col, # X-axis is Group\n",
    "                y='Count',\n",
    "                color='Taxonomic_Unit', # Color is Phylum or Family\n",
    "                barmode='stack', # Stack bars by Taxonomic_Unit\n",
    "                # title='Number of Proteins by Group and Taxonomy (Stacked Bar)', # No title\n",
    "                labels={group_col: 'Group', 'Count': 'Number of Proteins', 'Taxonomic_Unit': 'Phylum or Family'},\n",
    "                color_discrete_map=combined_taxonomy_color_map, # Use combined color map\n",
    "                category_orders={group_col: ['Asgard', 'GV'], 'Taxonomic_Unit': taxonomic_unit_order}, # Ensure Group order and Taxonomic Unit order\n",
    "                template='plotly_white'\n",
    "            )\n",
    "            # Apply default layout and remove gridlines\n",
    "            fig_4b_bar.update_layout(plotly_layout_defaults)\n",
    "            fig_4b_bar.update_xaxes(title_text='Group', showgrid=False)\n",
    "            fig_4b_bar.update_yaxes(title_text='Number of Proteins', showgrid=False)\n",
    "            fig_4b_bar.update_layout(legend_title_text='Taxonomic Unit') # Add legend title\n",
    "\n",
    "\n",
    "            # Export Figure 4B Part 1 (HTML, PDF, SVG)\n",
    "            fig_4b_bar_path_html = Path(output_figure_dir) / \"figure4b_proteins_by_taxonomy_stackedbar.html\"\n",
    "            fig_4b_bar.write_html(str(fig_4b_bar_path_html))\n",
    "            print(f\"Figure 4B (Stacked Bar) HTML saved to: {fig_4b_bar_path_html}\")\n",
    "\n",
    "            # Export to PDF/SVG (requires kaleido)\n",
    "            try:\n",
    "                fig_4b_bar_path_pdf = Path(output_figure_dir) / \"figure4b_proteins_by_taxonomy_stackedbar.pdf\"\n",
    "                fig_4b_bar.write_image(str(fig_4b_bar_path_pdf))\n",
    "                print(f\"Figure 4B (Stacked Bar) PDF saved to: {fig_4b_bar_path_pdf}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not export Figure 4B (Stacked Bar) to PDF. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "            try:\n",
    "                fig_4b_bar_path_svg = Path(output_figure_dir) / \"figure4b_proteins_by_taxonomy_stackedbar.svg\"\n",
    "                fig_4b_bar.write_image(str(fig_4b_bar_path_svg))\n",
    "                print(f\"Figure 4B (Stacked Bar) SVG saved to: {fig_4b_bar_path_svg}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not export Figure 4B (Stacked Bar) to SVG. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "\n",
    "\n",
    "            # --- Figure 4B Part 2: Pie Charts (Asgard and GV) ---\n",
    "            print(\"\\n--- Figure 4B Part 2: Protein Counts Pie Charts (Asgard and GV) ---\")\n",
    "\n",
    "            # Asgard Protein Pie Chart\n",
    "            df_asgard_protein_pie = df_protein_counts_taxonomy[df_protein_counts_taxonomy[group_col] == 'Asgard'].copy()\n",
    "            if not df_asgard_protein_pie.empty:\n",
    "                 # Sort Asgard phyla by count for pie chart order\n",
    "                 df_asgard_protein_pie = df_asgard_protein_pie.sort_values('Count', ascending=False).copy()\n",
    "                 fig_4b_pie_asgard = go.Figure(data=[go.Pie(\n",
    "                     labels=df_asgard_protein_pie['Taxonomic_Unit'],\n",
    "                     values=df_asgard_protein_pie['Count'],\n",
    "                     hole=.3, # Creates a donut chart\n",
    "                     marker=dict(colors=[combined_taxonomy_color_map.get(unit, 'gray') for unit in df_asgard_protein_pie['Taxonomic_Unit']]) # Apply colors\n",
    "                 )])\n",
    "                 # fig_4b_pie_asgard.update_layout(title_text='Asgard Protein Counts by Phylum') # No title\n",
    "                 fig_4b_pie_asgard.update_layout(plotly_layout_defaults) # Apply defaults (no title, etc.)\n",
    "                 fig_4b_pie_asgard.update_layout(margin=dict(t=0, b=0, l=0, r=0)) # Adjust margins\n",
    "                 fig_4b_pie_asgard.update_traces(textinfo='percent+label', insidetextorientation='radial')\n",
    "\n",
    "                 # Export Figure 4B Pie Asgard\n",
    "                 fig_4b_pie_asgard_path_html = Path(output_figure_dir) / \"figure4b_proteins_by_phylum_pie_asgard.html\"\n",
    "                 fig_4b_pie_asgard.write_html(str(fig_4b_pie_asgard_path_html))\n",
    "                 print(f\"Figure 4B (Asgard Pie) HTML saved to: {fig_4b_pie_asgard_path_html}\")\n",
    "                 try:\n",
    "                     fig_4b_pie_asgard_path_pdf = Path(output_figure_dir) / \"figure4b_proteins_by_phylum_pie_asgard.pdf\"\n",
    "                     fig_4b_pie_asgard.write_image(str(fig_4b_pie_asgard_path_pdf))\n",
    "                     print(f\"Figure 4B (Asgard Pie) PDF saved to: {fig_4b_pie_asgard_path_pdf}\")\n",
    "                 except Exception as e:\n",
    "                     print(f\"Warning: Could not export Figure 4B (Asgard Pie) to PDF. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "                 try:\n",
    "                     fig_4b_pie_asgard_path_svg = Path(output_figure_dir) / \"figure4b_proteins_by_phylum_pie_asgard.svg\"\n",
    "                     fig_4b_pie_asgard.write_image(str(fig_4b_pie_asgard_path_svg))\n",
    "                     print(f\"Figure 4B (Asgard Pie) SVG saved to: {fig_4b_pie_asgard_path_svg}\")\n",
    "                 except Exception as e:\n",
    "                     print(f\"Warning: Could not export Figure 4B (Asgard Pie) to SVG. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "\n",
    "            else:\n",
    "                 print(\"No Asgard protein data for pie chart.\")\n",
    "\n",
    "            # GV Protein Pie Chart\n",
    "            df_gv_protein_pie = df_protein_counts_taxonomy[df_protein_counts_taxonomy[group_col] == 'GV'].copy()\n",
    "            if not df_gv_protein_pie.empty:\n",
    "                 # Sort GV families by count for pie chart order\n",
    "                 df_gv_protein_pie = df_gv_protein_pie.sort_values('Count', ascending=False).copy()\n",
    "                 fig_4b_pie_gv = go.Figure(data=[go.Pie(\n",
    "                     labels=df_gv_protein_pie['Taxonomic_Unit'],\n",
    "                     values=df_gv_protein_pie['Count'],\n",
    "                     hole=.3, # Creates a donut chart\n",
    "                     marker=dict(colors=[combined_taxonomy_color_map.get(unit, 'gray') for unit in df_gv_protein_pie['Taxonomic_Unit']]) # Apply colors\n",
    "                 )])\n",
    "                 # fig_4b_pie_gv.update_layout(title_text='GV Protein Counts by Family') # No title\n",
    "                 fig_4b_pie_gv.update_layout(plotly_layout_defaults) # Apply defaults (no title, etc.)\n",
    "                 fig_4b_pie_gv.update_layout(margin=dict(t=0, b=0, l=0, r=0)) # Adjust margins\n",
    "                 fig_4b_pie_gv.update_traces(textinfo='percent+label', insidetextorientation='radial')\n",
    "\n",
    "                 # Export Figure 4B Pie GV\n",
    "                 fig_4b_pie_gv_path_html = Path(output_figure_dir) / \"figure4b_proteins_by_family_pie_gv.html\"\n",
    "                 fig_4b_pie_gv.write_html(str(fig_4b_pie_gv_path_html))\n",
    "                 print(f\"Figure 4B (GV Pie) HTML saved to: {fig_4b_pie_gv_path_html}\")\n",
    "                 try:\n",
    "                     fig_4b_pie_gv_path_pdf = Path(output_figure_dir) / \"figure4b_proteins_by_family_pie_gv.pdf\"\n",
    "                     fig_4b_pie_gv.write_image(str(fig_4b_pie_gv_path_pdf))\n",
    "                     print(f\"Figure 4B (GV Pie) PDF saved to: {fig_4b_pie_gv_path_pdf}\")\n",
    "                 except Exception as e:\n",
    "                     print(f\"Warning: Could not export Figure 4B (GV Pie) to PDF. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "                 try:\n",
    "                     fig_4b_pie_gv_path_svg = Path(output_figure_dir) / \"figure4b_proteins_by_family_pie_gv.svg\"\n",
    "                     fig_4b_pie_gv.write_image(str(fig_4b_pie_gv_path_svg))\n",
    "                     print(f\"Figure 4B (GV Pie) SVG saved to: {fig_4b_pie_gv_path_svg}\")\n",
    "                 except Exception as e:\n",
    "                     print(f\"Warning: Could not export Figure 4B (GV Pie) to SVG. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "\n",
    "            else:\n",
    "                 print(\"No GV protein data for pie chart.\")\n",
    "\n",
    "\n",
    "        else:\n",
    "            print(\"No protein counts by taxonomy available for plotting Figure 4B.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Skipping Figure 4B due to missing columns in filtered data: '{protein_id_col}', '{group_col}', '{asgard_phylum_col}', or '{virus_family_col}'.\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n--- Figure 4 Generation Complete ---\")\n",
    "print(f\"Figures saved to the '{output_figure_dir}' directory (HTML, PDF, SVG).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f93aa62-2042-4ad6-a3fe-ad1f6b421a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Figure 5 - Intra-Orthogroup Divergence (APSI) Landscape\n",
    "\n",
    "# --- Imports ---\n",
    "# Assumes pandas, numpy, plotly.express, plotly.graph_objects, plotly.io,\n",
    "# and necessary variables/helper functions (like df_full, group_col,\n",
    "# orthogroup_col, apsi_col, num_og_sequences_col, broad_func_cat_col,\n",
    "# structurally_dark_col, esp_col, original_seq_length_col, percent_disorder_col,\n",
    "# arcadia_colors_manual, arcadia_primary_palette, arcadia_secondary_palette,\n",
    "# arcadia_neutrals_palette, output_figure_dir, output_figure_summary_dir,\n",
    "# plotly_layout_defaults, broad_category_color_map)\n",
    "# are defined in the setup cell (Cell 1).\n",
    "# Assumes df_filtered_groups is available from Cell 2 (Figure 2 generation) and\n",
    "# correctly filters for 'Asgard' and 'GV' groups.\n",
    "# Assumes APSI data and 'Has_Conserved_Motif' flag are intended to be merged into df_full in Cell 1.\n",
    "\n",
    "print(\"\\n\\n--- Generating Figure 5: Intra-Orthogroup Divergence (APSI) Landscape (Cell 7) ---\")\n",
    "\n",
    "# Ensure df_full is loaded and filtered groups are available\n",
    "if 'df_full' not in locals() or df_full.empty:\n",
    "    print(\"ERROR: df_full not loaded. Please run the setup cell (Cell 1).\")\n",
    "elif 'df_filtered_groups' not in locals() or df_filtered_groups.empty:\n",
    "     print(\"ERROR: df_filtered_groups not available or is empty. Please run Cell 2 first.\")\n",
    "else:\n",
    "    # Define colors for Asgard and GV groups (re-defined for clarity in this cell)\n",
    "    group_colors = {\n",
    "        'Asgard': arcadia_colors_manual.get('aegean', '#5088C5'),\n",
    "        'GV': arcadia_colors_manual.get('amber', '#F28360')\n",
    "    }\n",
    "    print(f\"\\nUsing Group Colors: Asgard={group_colors.get('Asgard')}, GV={group_colors.get('Amber')}\")\n",
    "\n",
    "    # --- Check for required APSI columns before proceeding ---\n",
    "    # Check for core APSI columns and the group column\n",
    "    required_core_cols = [orthogroup_col, apsi_col, num_og_sequences_col, group_col]\n",
    "    missing_core_cols = [col for col in required_core_cols if col not in df_full.columns]\n",
    "\n",
    "    # Also check for the 'Has_Conserved_Motif' column needed for Figure 5G\n",
    "    if 'Has_Conserved_Motif' not in df_full.columns:\n",
    "         missing_core_cols.append('Has_Conserved_Motif')\n",
    "\n",
    "    # Also check for columns needed for specific plots (5D, 5E, 5F)\n",
    "    if broad_func_cat_col not in df_full.columns: missing_core_cols.append(broad_func_cat_col)\n",
    "    if structurally_dark_col not in df_full.columns: missing_core_cols.append(structurally_dark_col)\n",
    "    if original_seq_length_col not in df_full.columns: missing_core_cols.append(original_seq_length_col)\n",
    "    if percent_disorder_col not in df_full.columns: missing_core_cols.append(percent_disorder_col)\n",
    "\n",
    "\n",
    "    if missing_core_cols:\n",
    "        print(f\"ERROR: Required columns for Figure 5 analysis not found in df_full: {', '.join(missing_core_cols)}\")\n",
    "        print(\"Please ensure the main database is loaded correctly and includes these columns.\")\n",
    "        print(\"Also, ensure APSI data and 'Has_Conserved_Motif' flag were correctly merged into df_full in the setup cell (Cell 1).\")\n",
    "        print(\"Consider restarting the kernel and running all cells.\")\n",
    "    else:\n",
    "        # Filter df_full to include only OGs with calculated APSI (size >= 5)\n",
    "        # Also filter by group (Asgard/GV) using df_filtered_groups as a base\n",
    "        # Ensure Orthogroup column exists in df_full before filtering\n",
    "        if orthogroup_col in df_full.columns:\n",
    "            df_apsi_data = df_full[\n",
    "                df_full[orthogroup_col].isin(df_filtered_groups[orthogroup_col].unique()) & # Only OGs present in filtered groups\n",
    "                df_full[apsi_col].notna() # Only OGs where APSI was calculated\n",
    "            ].drop_duplicates(subset=[orthogroup_col]).copy() # Ensure one row per OG\n",
    "        else:\n",
    "            print(f\"ERROR: Orthogroup column '{orthogroup_col}' not found in df_full. Cannot filter for APSI data.\")\n",
    "            df_apsi_data = pd.DataFrame() # Create empty df\n",
    "\n",
    "\n",
    "        if df_apsi_data.empty:\n",
    "            print(\"WARNING: No orthogroups with calculated APSI found after filtering. Cannot generate Figure 5 plots.\")\n",
    "        else:\n",
    "            print(f\"\\nFound {len(df_apsi_data)} orthogroups with calculated APSI for plotting.\")\n",
    "\n",
    "            # --- Figure 5A: Overall APSI Distribution (Histogram) ---\n",
    "            print(\"\\n--- Figure 5A: Overall APSI Distribution (Histogram) ---\")\n",
    "            print(\"Note: The distribution may be skewed; consider transformations or alternative plot types if needed.\")\n",
    "\n",
    "            fig_5a = px.histogram(\n",
    "                df_apsi_data,\n",
    "                x=apsi_col,\n",
    "                nbins=50, # Adjust bins as needed\n",
    "                # title='Overall APSI Distribution', # No title\n",
    "                labels={apsi_col: 'Average Pairwise Sequence Identity (APSI)', 'count': 'Number of Orthogroups'},\n",
    "                template='plotly_white'\n",
    "            )\n",
    "            # Apply default layout and remove gridlines\n",
    "            fig_5a.update_layout(plotly_layout_defaults)\n",
    "            fig_5a.update_xaxes(title_text='Average Pairwise Sequence Identity (APSI)', showgrid=False)\n",
    "            fig_5a.update_yaxes(title_text='Number of Orthogroups', showgrid=False)\n",
    "\n",
    "\n",
    "            # Export Figure 5A (HTML, PDF, SVG)\n",
    "            fig_5a_path_html = Path(output_figure_dir) / \"figure5a_overall_apsi_distribution_histogram.html\"\n",
    "            fig_5a.write_html(str(fig_5a_path_html))\n",
    "            print(f\"Figure 5A HTML saved to: {fig_5a_path_html}\")\n",
    "            try:\n",
    "                fig_5a_path_pdf = Path(output_figure_dir) / \"figure5a_overall_apsi_distribution_histogram.pdf\"\n",
    "                fig_5a.write_image(str(fig_5a_path_pdf))\n",
    "                print(f\"Figure 5A PDF saved to: {fig_5a_path_pdf}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not export Figure 5A to PDF. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "            try:\n",
    "                fig_5a_path_svg = Path(output_figure_dir) / \"figure5a_overall_apsi_distribution_histogram.svg\"\n",
    "                fig_5a.write_image(str(fig_5a_path_svg))\n",
    "                print(f\"Figure 5A SVG saved to: {fig_5a_path_svg}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not export Figure 5A to SVG. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "\n",
    "\n",
    "            # --- Figure 5B: APSI Comparison (Asgard vs. GV) (Violin Plot) ---\n",
    "            print(\"\\n--- Figure 5B: APSI Comparison (Asgard vs. GV) (Violin Plot) ---\")\n",
    "\n",
    "            fig_5b = px.violin(\n",
    "                df_apsi_data,\n",
    "                x=group_col,\n",
    "                y=apsi_col,\n",
    "                color=group_col,\n",
    "                # title='APSI Comparison: Asgard vs. GV', # No title\n",
    "                labels={group_col: 'Group', apsi_col: 'Average Pairwise Sequence Identity (APSI)'},\n",
    "                color_discrete_map=group_colors,\n",
    "                category_orders={group_col: ['Asgard', 'GV']}, # Ensure order\n",
    "                box=True, # Show box plot inside violin\n",
    "                points=\"all\", # Show all points (can be slow for large datasets) or False/None\n",
    "                template='plotly_white'\n",
    "            )\n",
    "            # Apply default layout and remove gridlines\n",
    "            fig_5b.update_layout(plotly_layout_defaults)\n",
    "            fig_5b.update_xaxes(title_text='Group', showgrid=False)\n",
    "            fig_5b.update_yaxes(title_text='Average Pairwise Sequence Identity (APSI)', showgrid=False)\n",
    "            fig_5b.update_layout(showlegend=False) # Hide legend if colors are obvious from x-axis\n",
    "\n",
    "\n",
    "            # Export Figure 5B (HTML, PDF, SVG)\n",
    "            fig_5b_path_html = Path(output_figure_dir) / \"figure5b_apsi_comparison_group_violinplot.html\"\n",
    "            fig_5b.write_html(str(fig_5b_path_html))\n",
    "            print(f\"Figure 5B HTML saved to: {fig_5b_path_html}\")\n",
    "            try:\n",
    "                fig_5b_path_pdf = Path(output_figure_dir) / \"figure5b_apsi_comparison_group_violinplot.pdf\"\n",
    "                fig_5b.write_image(str(fig_5b_path_pdf))\n",
    "                print(f\"Figure 5B PDF saved to: {fig_5b_path_pdf}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not export Figure 5B to PDF. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "            try:\n",
    "                fig_5b_path_svg = Path(output_figure_dir) / \"figure5b_apsi_comparison_group_violinplot.svg\"\n",
    "                fig_5b.write_image(str(fig_5b_path_svg))\n",
    "                print(f\"Figure 5B SVG saved to: {fig_5b_path_svg}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not export Figure 5B to SVG. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "\n",
    "\n",
    "            # --- Figure 5C: APSI vs. Orthogroup Size ---\n",
    "            print(\"\\n--- Figure 5C: APSI vs. Orthogroup Size ---\")\n",
    "\n",
    "            if num_og_sequences_col in df_apsi_data.columns:\n",
    "                fig_5c = px.scatter(\n",
    "                    df_apsi_data,\n",
    "                    x=num_og_sequences_col,\n",
    "                    y=apsi_col,\n",
    "                    color=group_col, # Color by group\n",
    "                    # title='APSI vs. Orthogroup Size', # No title\n",
    "                    labels={num_og_sequences_col: 'Orthogroup Size (Number of Proteins)', apsi_col: 'Average Pairwise Sequence Identity (APSI)', group_col: 'Group'},\n",
    "                    color_discrete_map=group_colors,\n",
    "                    template='plotly_white'\n",
    "                )\n",
    "                # Apply default layout and remove gridlines\n",
    "                fig_5c.update_layout(plotly_layout_defaults)\n",
    "                fig_5c.update_xaxes(title_text='Orthogroup Size (Number of Proteins)', showgrid=False)\n",
    "                fig_5c.update_yaxes(title_text='Average Pairwise Sequence Identity (APSI)', showgrid=False)\n",
    "                fig_5c.update_layout(legend_title_text='Group') # Add legend title\n",
    "\n",
    "\n",
    "                # Export Figure 5C (HTML, PDF, SVG)\n",
    "                fig_5c_path_html = Path(output_figure_dir) / \"figure5c_apsi_vs_og_size_scatter.html\"\n",
    "                fig_5c.write_html(str(fig_5c_path_html))\n",
    "                print(f\"Figure 5C HTML saved to: {fig_5c_path_html}\")\n",
    "                try:\n",
    "                    fig_5c_path_pdf = Path(output_figure_dir) / \"figure5c_apsi_vs_og_size_scatter.pdf\"\n",
    "                    fig_5c.write_image(str(fig_5c_path_pdf))\n",
    "                    print(f\"Figure 5C PDF saved to: {fig_5c_path_pdf}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not export Figure 5C to PDF. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "                try:\n",
    "                    fig_5c_path_svg = Path(output_figure_dir) / \"figure5c_apsi_vs_og_size_scatter.svg\"\n",
    "                    fig_5c.write_image(str(fig_5c_path_svg))\n",
    "                    print(f\"Figure 5C SVG saved to: {fig_5c_path_svg}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not export Figure 5C to SVG. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "\n",
    "            else:\n",
    "                print(f\"Skipping Figure 5C due to missing column: '{num_og_sequences_col}'.\")\n",
    "\n",
    "\n",
    "            # --- Figure 5D: APSI by Broad Functional Category ---\n",
    "            print(\"\\n--- Figure 5D: APSI by Broad Functional Category ---\")\n",
    "\n",
    "            # Check if broad_func_cat_col exists in df_full before merging\n",
    "            if broad_func_cat_col in df_full.columns:\n",
    "                # Merge APSI data with functional categories (need to ensure one category per OG for plotting)\n",
    "                # Assuming broad_func_cat_col is representative at the OG level or we take the most common\n",
    "                # For simplicity here, let's merge APSI with the first protein's broad category in each OG\n",
    "                df_og_categories = df_full.drop_duplicates(subset=[orthogroup_col], keep='first')[[orthogroup_col, broad_func_cat_col]].copy()\n",
    "\n",
    "                # Add a check for broad_func_cat_col in df_og_categories before merging\n",
    "                if broad_func_cat_col in df_og_categories.columns:\n",
    "                    df_apsi_with_categories = pd.merge(df_apsi_data, df_og_categories, on=orthogroup_col, how='left')\n",
    "\n",
    "                    # --- Debugging Print Statement ---\n",
    "                    print(f\"\\nDEBUG: Columns in df_apsi_with_categories before filtering categories: {df_apsi_with_categories.columns.tolist()}\")\n",
    "                    # --- End Debugging Print Statement ---\n",
    "\n",
    "                    # Filter out unwanted categories for plotting (same as Figure 3B/2D)\n",
    "                    categories_to_exclude = ['Unknown/Unclassified', 'Other Specific Annotation']\n",
    "                    # CORRECTED: Use the correct column name after merge ('Broad_Functional_Category_y')\n",
    "                    functional_category_col_merged = broad_func_cat_col + '_y' # Assuming '_y' is the correct suffix\n",
    "\n",
    "                    # Add a check for the expected merged column name\n",
    "                    if functional_category_col_merged in df_apsi_with_categories.columns:\n",
    "                        df_apsi_with_categories_filtered = df_apsi_with_categories[\n",
    "                            (~df_apsi_with_categories[functional_category_col_merged].isin(categories_to_exclude))\n",
    "                        ].copy()\n",
    "\n",
    "\n",
    "                        if not df_apsi_with_categories_filtered.empty:\n",
    "                            # Define category order for plotting (based on frequency or a predefined list)\n",
    "                            # Using frequency in the filtered data for consistent order\n",
    "                            category_order_5d = df_apsi_with_categories_filtered[functional_category_col_merged].value_counts().index.tolist()\n",
    "\n",
    "                            fig_5d = px.box( # Using box plot as it's standard for comparing distributions across categories\n",
    "                                df_apsi_with_categories_filtered,\n",
    "                                x=functional_category_col_merged, # Use the corrected column name\n",
    "                                y=apsi_col,\n",
    "                                color=group_col, # Color by group\n",
    "                                # title='APSI by Broad Functional Category', # No title\n",
    "                                labels={functional_category_col_merged: 'Broad Functional Category', apsi_col: 'Average Pairwise Sequence Identity (APSI)', group_col: 'Group'},\n",
    "                                color_discrete_map=group_colors,\n",
    "                                category_orders={functional_category_col_merged: category_order_5d, group_col: ['Asgard', 'GV']}, # Apply consistent order\n",
    "                                template='plotly_white'\n",
    "                            )\n",
    "                            # Apply default layout and remove gridlines\n",
    "                            fig_5d.update_layout(plotly_layout_defaults)\n",
    "                            fig_5d.update_xaxes(title_text='Broad Functional Category', showgrid=False)\n",
    "                            fig_5d.update_yaxes(title_text='Average Pairwise Sequence Identity (APSI)', showgrid=False)\n",
    "                            fig_5d.update_layout(legend_title_text='Group') # Add legend title\n",
    "                            fig_5d.update_xaxes(tickangle=45) # Angle labels if needed\n",
    "\n",
    "\n",
    "                            # Export Figure 5D (HTML, PDF, SVG)\n",
    "                            fig_5d_path_html = Path(output_figure_dir) / \"figure5d_apsi_by_functional_category_boxplot.html\"\n",
    "                            fig_5d.write_html(str(fig_5d_path_html))\n",
    "                            print(f\"Figure 5D HTML saved to: {fig_5d_path_html}\")\n",
    "                            try:\n",
    "                                fig_5d_path_pdf = Path(output_figure_dir) / \"figure5d_apsi_by_functional_category_boxplot.pdf\"\n",
    "                                fig_5d.write_image(str(fig_5d_path_pdf))\n",
    "                                print(f\"Figure 5D PDF saved to: {fig_5d_path_pdf}\")\n",
    "                            except Exception as e:\n",
    "                                print(f\"Warning: Could not export Figure 5D to PDF. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "                            try:\n",
    "                                fig_5d_path_svg = Path(output_figure_dir) / \"figure5d_apsi_by_functional_category_boxplot.svg\"\n",
    "                                fig_5d.write_image(str(fig_5d_path_svg))\n",
    "                                print(f\"Figure 5D SVG saved to: {fig_5d_path_svg}\")\n",
    "                            except Exception as e:\n",
    "                                print(f\"Warning: Could not export Figure 5D to SVG. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "\n",
    "                        else:\n",
    "                            print(\"No APSI data with filtered functional categories available for plotting Figure 5D.\")\n",
    "                    else:\n",
    "                         print(f\"Skipping Figure 5D plot: Expected merged column '{functional_category_col_merged}' not found in df_apsi_with_categories.\")\n",
    "\n",
    "                else:\n",
    "                    print(f\"Skipping Figure 5D merge and plot: '{broad_func_cat_col}' column not found in merged OG categories.\")\n",
    "            else:\n",
    "                print(f\"Skipping Figure 5D due to missing column in df_full: '{broad_func_cat_col}'.\")\n",
    "\n",
    "\n",
    "           # Cell 7: Figure 5 - Intra-Orthogroup Divergence (APSI) Landscape\n",
    "# ... (previous code in Cell 7 for Figures 5A, 5B, 5C, 5D) ...\n",
    "\n",
    "            # --- Figure 5E: APSI by Structural Status ---\n",
    "            print(\"\\n--- Figure 5E: APSI by Structural Status ---\")\n",
    "\n",
    "            # df_apsi_data already contains 'Is_Structurally_Dark' for each OG,\n",
    "            # so no need to merge it in again.\n",
    "            # We will use a copy of df_apsi_data to avoid modifying it if it's used elsewhere.\n",
    "            df_apsi_with_structural_status = df_apsi_data.copy()\n",
    "\n",
    "            # Check if structurally_dark_col (Is_Structurally_Dark) exists\n",
    "            if structurally_dark_col in df_apsi_with_structural_status.columns:\n",
    "                 # Ensure structural status column is boolean\n",
    "                 df_apsi_with_structural_status[structurally_dark_col] = df_apsi_with_structural_status[structurally_dark_col].astype(bool)\n",
    "\n",
    "                 # Define labels for True/False structural status\n",
    "                 structural_status_labels = {True: 'Structurally Dark', False: 'Has Known Structure'}\n",
    "\n",
    "                 # Add a column with string labels for plotting\n",
    "                 df_apsi_with_structural_status['Structural_Status_Label'] = df_apsi_with_structural_status[structurally_dark_col].map(structural_status_labels)\n",
    "\n",
    "                 # Filter out NaNs if any (e.g. if Is_Structurally_Dark was NaN for some OGs)\n",
    "                 df_apsi_with_structural_status_filtered = df_apsi_with_structural_status.dropna(subset=['Structural_Status_Label', apsi_col]).copy()\n",
    "\n",
    "\n",
    "                 if not df_apsi_with_structural_status_filtered.empty:\n",
    "                     fig_5e = px.box( # Using box plot as it's standard for comparing distributions across categories\n",
    "                         df_apsi_with_structural_status_filtered,\n",
    "                         x='Structural_Status_Label',\n",
    "                         y=apsi_col,\n",
    "                         color=group_col, # Color by group (Asgard/GV)\n",
    "                         # title='APSI by Structural Status', # No title\n",
    "                         labels={'Structural_Status_Label': 'Structural Status', apsi_col: 'Average Pairwise Sequence Identity (APSI)', group_col: 'Group'},\n",
    "                         color_discrete_map=group_colors, # Defined in Cell 1\n",
    "                         category_orders={'Structural_Status_Label': ['Structurally Dark', 'Has Known Structure'], group_col: ['Asgard', 'GV']}, # Apply consistent order\n",
    "                         template='plotly_white'\n",
    "                     )\n",
    "                     # Apply default layout and remove gridlines\n",
    "                     fig_5e.update_layout(plotly_layout_defaults) # Defined in Cell 1\n",
    "                     fig_5e.update_xaxes(title_text='Structural Status', showgrid=False)\n",
    "                     fig_5e.update_yaxes(title_text='Average Pairwise Sequence Identity (APSI)', showgrid=False)\n",
    "                     fig_5e.update_layout(legend_title_text='Group') # Add legend title\n",
    "\n",
    "\n",
    "                     # Export Figure 5E (HTML, PDF, SVG)\n",
    "                     fig_5e_path_html = Path(output_figure_dir) / \"figure5e_apsi_by_structural_status_boxplot.html\" # Defined in Cell 1\n",
    "                     fig_5e.write_html(str(fig_5e_path_html))\n",
    "                     print(f\"Figure 5E HTML saved to: {fig_5e_path_html}\")\n",
    "                     try:\n",
    "                         fig_5e_path_pdf = Path(output_figure_dir) / \"figure5e_apsi_by_structural_status_boxplot.pdf\"\n",
    "                         fig_5e.write_image(str(fig_5e_path_pdf))\n",
    "                         print(f\"Figure 5E PDF saved to: {fig_5e_path_pdf}\")\n",
    "                     except Exception as e:\n",
    "                         print(f\"Warning: Could not export Figure 5E to PDF. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "                     try:\n",
    "                         fig_5e_path_svg = Path(output_figure_dir) / \"figure5e_apsi_by_structural_status_boxplot.svg\"\n",
    "                         fig_5e.write_image(str(fig_5e_path_svg))\n",
    "                         print(f\"Figure 5E SVG saved to: {fig_5e_path_svg}\")\n",
    "                     except Exception as e:\n",
    "                         print(f\"Warning: Could not export Figure 5E to SVG. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "\n",
    "                 else:\n",
    "                     print(\"No APSI data with structural status available for plotting Figure 5E after filtering NaNs.\")\n",
    "            else:\n",
    "                print(f\"Skipping Figure 5E: Column '{structurally_dark_col}' not found in df_apsi_data.\")\n",
    "\n",
    "            # --- Figure 5F: APSI vs. Protein Length/Disorder ---\n",
    "            print(\"\\n--- Figure 5F: APSI vs. Protein Length/Disorder ---\")\n",
    "\n",
    "            # Check if original_seq_length_col and percent_disorder_col exist in df_filtered_groups before calculating averages\n",
    "            if original_seq_length_col in df_filtered_groups.columns and percent_disorder_col in df_filtered_groups.columns:\n",
    "                 # Need average protein length and disorder per OG\n",
    "                 # Calculate average length and disorder per OG from df_filtered_groups\n",
    "                 df_og_avg_features = df_filtered_groups.groupby(orthogroup_col).agg(\n",
    "                     Avg_Length=(original_seq_length_col, 'mean'),\n",
    "                     Avg_Disorder=(percent_disorder_col, 'mean')\n",
    "                 ).reset_index()\n",
    "\n",
    "                 # Merge APSI data with average features\n",
    "                 df_apsi_with_avg_features = pd.merge(df_apsi_data, df_og_avg_features, on=orthogroup_col, how='left')\n",
    "\n",
    "                 # Scatter plot: APSI vs. Average Length\n",
    "                 if 'Avg_Length' in df_apsi_with_avg_features.columns:\n",
    "                      fig_5f_length = px.scatter(\n",
    "                          df_apsi_with_avg_features,\n",
    "                          x='Avg_Length',\n",
    "                          y=apsi_col,\n",
    "                          color=group_col, # Color by group\n",
    "                          # title='APSI vs. Average Orthogroup Protein Length', # No title\n",
    "                          labels={'Avg_Length': 'Average Protein Length per Orthogroup', apsi_col: 'Average Pairwise Sequence Identity (APSI)', group_col: 'Group'},\n",
    "                          color_discrete_map=group_colors,\n",
    "                          template='plotly_white'\n",
    "                      )\n",
    "                      # Apply default layout and remove gridlines\n",
    "                      fig_5f_length.update_layout(plotly_layout_defaults)\n",
    "                      fig_5f_length.update_xaxes(title_text='Average Protein Length per Orthogroup', showgrid=False)\n",
    "                      fig_5f_length.update_yaxes(title_text='Average Pairwise Sequence Identity (APSI)', showgrid=False)\n",
    "                      fig_5f_length.update_layout(legend_title_text='Group') # Add legend title\n",
    "\n",
    "\n",
    "                      # Export Figure 5F (Length) (HTML, PDF, SVG)\n",
    "                      fig_5f_length_path_html = Path(output_figure_dir) / \"figure5f_apsi_vs_avg_length_scatter.html\"\n",
    "                      fig_5f_length.write_html(str(fig_5f_length_path_html))\n",
    "                      print(f\"Figure 5F (Length) HTML saved to: {fig_5f_length_path_html}\")\n",
    "                      try:\n",
    "                          fig_5f_length_path_pdf = Path(output_figure_dir) / \"figure5f_apsi_vs_avg_length_scatter.pdf\"\n",
    "                          fig_5f_length.write_image(str(fig_5f_length_path_pdf))\n",
    "                          print(f\"Figure 5F (Length) PDF saved to: {fig_5f_length_path_pdf}\")\n",
    "                      except Exception as e:\n",
    "                          print(f\"Warning: Could not export Figure 5F (Length) to PDF. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "                      try:\n",
    "                          fig_5f_length_path_svg = Path(output_figure_dir) / \"figure5f_apsi_vs_avg_length_scatter.svg\"\n",
    "                          fig_5f_length.write_image(str(fig_5f_length_path_svg))\n",
    "                          print(f\"Figure 5F (Length) SVG saved to: {fig_5f_length_path_svg}\")\n",
    "                      except Exception as e:\n",
    "                          print(f\"Warning: Could not export Figure 5F (Length) to SVG. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "\n",
    "                 else:\n",
    "                     print(f\"Skipping Figure 5F (Length) due to missing 'Avg_Length' column after merge.\")\n",
    "\n",
    "\n",
    "                 # Scatter plot: APSI vs. Average Disorder\n",
    "                 if 'Avg_Disorder' in df_apsi_with_avg_features.columns:\n",
    "                      fig_5f_disorder = px.scatter(\n",
    "                          df_apsi_with_avg_features,\n",
    "                          x='Avg_Disorder',\n",
    "                          y=apsi_col,\n",
    "                          color=group_col, # Color by group\n",
    "                          # title='APSI vs. Average Orthogroup Protein Disorder', # No title\n",
    "                          labels={'Avg_Disorder': 'Average Protein Disorder (%) per Orthogroup', apsi_col: 'Average Pairwise Sequence Identity (APSI)', group_col: 'Group'},\n",
    "                          color_discrete_map=group_colors,\n",
    "                          template='plotly_white'\n",
    "                      )\n",
    "                      # Apply default layout and remove gridlines\n",
    "                      fig_5f_disorder.update_layout(plotly_layout_defaults)\n",
    "                      fig_5f_disorder.update_xaxes(title_text='Average Protein Disorder (%) per Orthogroup', showgrid=False)\n",
    "                      fig_5f_disorder.update_yaxes(title_text='Average Pairwise Sequence Identity (APSI)', showgrid=False)\n",
    "                      fig_5f_disorder.update_layout(legend_title_text='Group') # Add legend title\n",
    "\n",
    "\n",
    "                      # Export Figure 5F (Disorder) (HTML, PDF, SVG)\n",
    "                      fig_5f_disorder_path_html = Path(output_figure_dir) / \"figure5f_apsi_vs_avg_disorder_scatter.html\"\n",
    "                      fig_5f_disorder.write_html(str(fig_5f_disorder_path_html))\n",
    "                      print(f\"Figure 5F (Disorder) HTML saved to: {fig_5f_disorder_path_html}\")\n",
    "                      try:\n",
    "                          fig_5f_disorder_path_pdf = Path(output_figure_dir) / \"figure5f_apsi_vs_avg_disorder_scatter.pdf\"\n",
    "                          fig_5f_disorder.write_image(str(fig_5f_disorder_path_pdf))\n",
    "                          print(f\"Figure 5F (Disorder) PDF saved to: {fig_5f_disorder_path_pdf}\")\n",
    "                      except Exception as e:\n",
    "                          print(f\"Warning: Could not export Figure 5F (Disorder) to PDF. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "                      try:\n",
    "                          fig_5f_disorder_path_svg = Path(output_figure_dir) / \"figure5f_apsi_vs_avg_disorder_scatter.svg\"\n",
    "                          fig_5f_disorder.write_image(str(fig_5f_disorder_path_svg))\n",
    "                          print(f\"Figure 5F (Disorder) SVG saved to: {fig_5f_disorder_path_svg}\")\n",
    "                      except Exception as e:\n",
    "                          print(f\"Warning: Could not export Figure 5F (Disorder) to SVG. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "\n",
    "                 else:\n",
    "                     print(f\"Skipping Figure 5F (Disorder) due to missing 'Avg_Disorder' column after merge.\")\n",
    "\n",
    "            else:\n",
    "                 print(f\"Skipping Figure 5F due to missing columns in filtered data: '{original_seq_length_col}' or '{percent_disorder_col}'.\")\n",
    "\n",
    "\n",
    "            # --- Figure 5G: APSI and Conserved Motif Presence ---\n",
    "            print(\"\\n--- Figure 5G: APSI and Conserved Motif Presence ---\")\n",
    "\n",
    "            if 'Has_Conserved_Motif' in df_apsi_data.columns: # Check if the flag was merged\n",
    "                 # Ensure the flag column is boolean\n",
    "                 df_apsi_data['Has_Conserved_Motif'] = df_apsi_data['Has_Conserved_Motif'].astype(bool)\n",
    "\n",
    "                 # Define labels for True/False motif presence\n",
    "                 motif_presence_labels = {True: 'Has Conserved Motif', False: 'No Conserved Motif'}\n",
    "\n",
    "                 # Add a column with string labels for plotting\n",
    "                 df_apsi_data['Motif_Presence_Label'] = df_apsi_data['Has_Conserved_Motif'].map(motif_presence_labels)\n",
    "\n",
    "                 # Filter out NaNs if any (shouldn't be if flag was added correctly)\n",
    "                 df_apsi_with_motif_filtered = df_apsi_data.dropna(subset=['Motif_Presence_Label']).copy()\n",
    "\n",
    "\n",
    "                 if not df_apsi_with_motif_filtered.empty:\n",
    "                     fig_5g = px.box( # Using box plot as it's standard for comparing distributions across categories\n",
    "                         df_apsi_with_motif_filtered,\n",
    "                         x='Motif_Presence_Label',\n",
    "                         y=apsi_col,\n",
    "                         color=group_col, # Color by group\n",
    "                         # title='APSI by Conserved Motif Presence', # No title\n",
    "                         labels={'Motif_Presence_Label': 'Conserved Motif Presence', apsi_col: 'Average Pairwise Sequence Identity (APSI)', group_col: 'Group'},\n",
    "                         color_discrete_map=group_colors,\n",
    "                         category_orders={'Motif_Presence_Label': ['Has Conserved Motif', 'No Conserved Motif'], group_col: ['Asgard', 'GV']}, # Apply consistent order\n",
    "                         template='plotly_white'\n",
    "                     )\n",
    "                     # Apply default layout and remove gridlines\n",
    "                     fig_5g.update_layout(plotly_layout_defaults)\n",
    "                     fig_5g.update_xaxes(title_text='Conserved Motif Presence', showgrid=False)\n",
    "                     fig_5g.update_yaxes(title_text='Average Pairwise Sequence Identity (APSI)', showgrid=False)\n",
    "                     fig_5g.update_layout(legend_title_text='Group') # Add legend title\n",
    "\n",
    "\n",
    "                     # Export Figure 5G (HTML, PDF, SVG)\n",
    "                     fig_5g_path_html = Path(output_figure_dir) / \"figure5g_apsi_by_motif_presence_boxplot.html\"\n",
    "                     fig_5g.write_html(str(fig_5g_path_html))\n",
    "                     print(f\"Figure 5G HTML saved to: {fig_5g_path_html}\")\n",
    "                     try:\n",
    "                         fig_5g_path_pdf = Path(output_figure_dir) / \"figure5g_apsi_by_motif_presence_boxplot.pdf\"\n",
    "                         fig_5g.write_image(str(fig_5g_path_pdf))\n",
    "                         print(f\"Figure 5G PDF saved to: {fig_5g_path_pdf}\")\n",
    "                     except Exception as e:\n",
    "                         print(f\"Warning: Could not export Figure 5G to PDF. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "                     try:\n",
    "                         fig_5g_path_svg = Path(output_figure_dir) / \"figure5g_apsi_by_motif_presence_boxplot.svg\"\n",
    "                         fig_5g.write_image(str(fig_5g_path_svg))\n",
    "                         print(f\"Figure 5G SVG saved to: {fig_5g_path_svg}\")\n",
    "                     except Exception as e:\n",
    "                         print(f\"Warning: Could not export Figure 5G to SVG. Ensure 'kaleido' is installed (`pip install kaleido`). Error: {e}\")\n",
    "\n",
    "                 else:\n",
    "                     print(\"No APSI data with motif presence information available for plotting Figure 5G.\")\n",
    "            else:\n",
    "                print(f\"Skipping Figure 5G due to missing column: 'Has_Conserved_Motif'.\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n--- Figure 5 Generation Complete ---\")\n",
    "print(f\"Figures saved to the '{output_figure_dir}' directory (HTML, PDF, SVG).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e46af4-0d1f-4ff0-af06-dbca9adbe203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Figure 4 (from Pub Ideas) - Conserved Motif Analysis\n",
    "# Corresponds to Figure 6 in the notebook's sequence if Cell 7 was Figure 5.\n",
    "\n",
    "# --- Imports & Setup Assumptions ---\n",
    "# This cell assumes that pandas (pd), numpy (np), plotly.express (px),\n",
    "# plotly.graph_objects (go), plotly.io (pio), Path from pathlib,\n",
    "# and all necessary variables/helper functions from Cell 1 (Setup) are available.\n",
    "# Key variables from Cell 1:\n",
    "# - df_motifs: DataFrame containing motif data.\n",
    "# - output_figure_dir: Directory to save plots.\n",
    "# - plotly_layout_defaults: Default layout for Plotly figures.\n",
    "# - arcadia_primary_palette: Color palette.\n",
    "# - group_colors: Color map for Asgard/GV.\n",
    "# - orthogroup_col, group_col: Column names.\n",
    "\n",
    "print(\"\\n\\n--- Generating Figure 6 (Pub Ideas): Conserved Motif Analysis (Cell 8) ---\")\n",
    "\n",
    "# --- Check if df_motifs is available and not empty ---\n",
    "if 'df_motifs' not in locals() or df_motifs.empty:\n",
    "    print(\"ERROR: DataFrame 'df_motifs' not found or is empty. Please run Cell 1 first.\")\n",
    "    # Create an empty DataFrame to prevent errors if subsequent code relies on it, though plots won't generate.\n",
    "    df_motifs = pd.DataFrame(columns=['Motif', 'Orthogroup']) \n",
    "else:\n",
    "    print(f\"Using 'df_motifs' (shape: {df_motifs.shape}) for motif analysis.\")\n",
    "\n",
    "    # --- Figure 6A (Pub Ideas): Frequency of Top Conserved Motifs ---\n",
    "    print(\"\\n--- Figure 6A (Pub Ideas): Frequency of Top Conserved Motifs ---\")\n",
    "    \n",
    "    if 'Motif' in df_motifs.columns and not df_motifs.empty:\n",
    "        top_n_motifs_display = 30 # Number of top motifs to display\n",
    "        motif_counts = df_motifs['Motif'].value_counts()\n",
    "        \n",
    "        df_top_motifs_plot = motif_counts.head(top_n_motifs_display).reset_index()\n",
    "        df_top_motifs_plot.columns = ['Motif', 'Frequency']\n",
    "        \n",
    "        print(f\"\\nTop {top_n_motifs_display} Conserved Motifs (Overall):\")\n",
    "        try:\n",
    "            print(df_top_motifs_plot.to_markdown(index=False))\n",
    "        except ImportError:\n",
    "            print(df_top_motifs_plot)\n",
    "\n",
    "        # Save the full list of motif counts\n",
    "        motif_counts_summary_path = Path(output_figure_summary_dir) / \"figure4a_motif_frequency_all.csv\"\n",
    "        try:\n",
    "            motif_counts.reset_index().rename(columns={'index': 'Motif', 'Motif': 'Frequency'}).to_csv(motif_counts_summary_path, index=False)\n",
    "            print(f\"Full motif frequency summary saved to: {motif_counts_summary_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving full motif frequency summary: {e}\")\n",
    "\n",
    "        # Plot\n",
    "        fig_6a_motifs = px.bar(\n",
    "            df_top_motifs_plot,\n",
    "            x='Motif',\n",
    "            y='Frequency',\n",
    "            # title=f'Top {top_n_motifs_display} Conserved Motifs Found Across Orthogroups', # No title by default\n",
    "            labels={'Motif': 'Conserved Motif', 'Frequency': 'Number of Orthogroups'},\n",
    "            color_discrete_sequence=arcadia_primary_palette # Use a palette\n",
    "        )\n",
    "        fig_6a_motifs.update_layout(plotly_layout_defaults) # Apply Arcadia style\n",
    "        fig_6a_motifs.update_xaxes(title_text='Conserved Motif', showgrid=False, categoryorder='total descending', tickangle=45)\n",
    "        fig_6a_motifs.update_yaxes(title_text='Number of Orthogroups', showgrid=False)\n",
    "        fig_6a_motifs.update_layout(showlegend=False)\n",
    "\n",
    "        # Export Figure 6A (Motif Frequency)\n",
    "        fig_6a_motif_freq_path_html = Path(output_figure_dir) / \"figure6a_pub_ideas_motif_frequency.html\"\n",
    "        fig_6a_motifs.write_html(str(fig_6a_motif_freq_path_html))\n",
    "        print(f\"Figure 6A (Motif Frequency) HTML saved to: {fig_4a_motif_freq_path_html}\")\n",
    "        try:\n",
    "            fig_6a_motif_freq_path_pdf = Path(output_figure_dir) / \"figure6a_pub_ideas_motif_frequency.pdf\"\n",
    "            fig_6a_motifs.write_image(str(fig_6a_motif_freq_path_pdf))\n",
    "            print(f\"Figure 6A (Motif Frequency) PDF saved to: {fig_4a_motif_freq_path_pdf}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not export Figure 6A (Motif Frequency) to PDF. Ensure 'kaleido' is installed. Error: {e}\")\n",
    "        try:\n",
    "            fig_6a_motif_freq_path_svg = Path(output_figure_dir) / \"figure6a_pub_ideas_motif_frequency.svg\"\n",
    "            fig_6a_motifs.write_image(str(fig_4a_motif_freq_path_svg))\n",
    "            print(f\"Figure 6A (Motif Frequency) SVG saved to: {fig_4a_motif_freq_path_svg}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not export Figure 6A (Motif Frequency) to SVG. Ensure 'kaleido' is installed. Error: {e}\")\n",
    "            \n",
    "    else:\n",
    "        print(\"Skipping Figure 6A (Motif Frequency): 'Motif' column not found or DataFrame is empty.\")\n",
    "\n",
    "    # --- Figure 6B (Pub Ideas): Motif Length Distribution ---\n",
    "    print(\"\\n--- Figure 6B (Pub Ideas): Motif Length Distribution ---\")\n",
    "    \n",
    "    if 'Motif' in df_motifs.columns and not df_motifs.empty:\n",
    "        # Calculate motif lengths - ensure 'Motif' column is string type\n",
    "        df_motifs['Motif_Length'] = df_motifs['Motif'].astype(str).str.len()\n",
    "        \n",
    "        print(\"\\nMotif Length Statistics:\")\n",
    "        print(df_motifs['Motif_Length'].describe())\n",
    "\n",
    "        # Plot histogram of motif lengths\n",
    "        fig_6b_motif_len = px.histogram(\n",
    "            df_motifs.dropna(subset=['Motif_Length']), # Drop rows where motif length might be NaN (if Motif was NaN)\n",
    "            x='Motif_Length',\n",
    "            nbins=20, # Adjust number of bins as needed\n",
    "            # title='Distribution of Conserved Motif Lengths', # No title by default\n",
    "            labels={'Motif_Length': 'Motif Length (Amino Acids)', 'count': 'Number of Motifs'},\n",
    "            color_discrete_sequence=[arcadia_primary_palette[1]] # Use a color from the palette\n",
    "        )\n",
    "        fig_6b_motif_len.update_layout(plotly_layout_defaults) # Apply Arcadia style\n",
    "        fig_6b_motif_len.update_xaxes(title_text='Motif Length (Amino Acids)', showgrid=False)\n",
    "        fig_6b_motif_len.update_yaxes(title_text='Number of Motifs', showgrid=False)\n",
    "        fig_6b_motif_len.update_layout(bargap=0.1) # Add a small gap between bars\n",
    "\n",
    "        # Export Figure 6B (Motif Length)\n",
    "        fig_6b_motif_len_path_html = Path(output_figure_dir) / \"figure6b_pub_ideas_motif_length_distribution.html\"\n",
    "        fig_6b_motif_len.write_html(str(fig_6b_motif_len_path_html))\n",
    "        print(f\"Figure 6B (Motif Length) HTML saved to: {fig_6b_motif_len_path_html}\")\n",
    "        try:\n",
    "            fig_6b_motif_len_path_pdf = Path(output_figure_dir) / \"figure6b_pub_ideas_motif_length_distribution.pdf\"\n",
    "            fig_6b_motif_len.write_image(str(fig_4b_motif_len_path_pdf))\n",
    "            print(f\"Figure 6B (Motif Length) PDF saved to: {fig_6b_motif_len_path_pdf}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not export Figure 6B (Motif Length) to PDF. Error: {e}\")\n",
    "        try:\n",
    "            fig_6b_motif_len_path_svg = Path(output_figure_dir) / \"figure6b_pub_ideas_motif_length_distribution.svg\"\n",
    "            fig_6b_motif_len.write_image(str(fig_6b_motif_len_path_svg))\n",
    "            print(f\"Figure 6B (Motif Length) SVG saved to: {fig_6b_motif_len_path_svg}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not export Figure 6B (Motif Length) to SVG. Error: {e}\")\n",
    "            \n",
    "    else:\n",
    "        print(\"Skipping Figure 6B (Motif Length): 'Motif' column not found or DataFrame is empty.\")\n",
    "\n",
    "    # --- Note on Figure 6C and 6D (Pub Ideas) ---\n",
    "    print(\"\\n--- Regarding Figure 4C (Motif Conservation Level) & 4D (Example Motif Visualization) ---\")\n",
    "    print(\"Figure 6C (Motif Conservation Level): The current 'df_motifs' DataFrame does not explicitly store a per-motif conservation percentage.\")\n",
    "    print(\"  The motifs were identified as 'conserved' based on the parameters in the upstream script (e.g., >90% or 100% identity in columns).\")\n",
    "    print(\"  If a variable conservation level per motif is needed, the motif generation script or its output CSV would need to be adapted.\")\n",
    "    print(\"Figure 6D (Example Motif Visualization): Programmatically generating a publication-quality alignment snippet with highlighted motifs is complex.\")\n",
    "    print(\"  This is often best done manually or with specialized alignment visualization tools for a few key examples.\")\n",
    "\n",
    "print(\"\\n\\n--- Figure 6 (Pub Ideas) - Conserved Motif Analysis (Cell 8) Complete ---\")\n",
    "print(f\"Figures and summaries saved to '{output_figure_dir}' and '{output_figure_summary_dir}'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e6a36d-b512-4ebd-bf15-3e7e95519220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Figure 7 (User's numbering) / Figure 5 (Pub Ideas) - Integrating Divergence, Motifs, and Protein Features\n",
    "\n",
    "# --- Imports & Setup Assumptions ---\n",
    "# This cell assumes that pandas (pd), numpy (np), plotly.express (px),\n",
    "# plotly.graph_objects (go), plotly.io (pio), Path from pathlib, Counter from collections,\n",
    "# and all necessary variables/helper functions from Cell 1 (Setup) are available.\n",
    "# Key variables from Cell 1:\n",
    "# - df_full: Main DataFrame with all protein annotations, including Intra_OG_APSI and Has_Conserved_Motif.\n",
    "# - df_motifs: DataFrame with detailed motif data (Orthogroup, Motif, etc.).\n",
    "# - output_figure_dir, output_summary_dir_phase1: Directories to save plots/data.\n",
    "# - plotly_layout_defaults: Default layout for Plotly figures.\n",
    "# - group_colors, broad_category_color_map, arcadia_primary_palette: Color maps.\n",
    "# - orthogroup_col, group_col, esp_col, structurally_dark_col, \n",
    "#   apsi_col ('Intra_OG_APSI'), 'Has_Conserved_Motif' (flag in df_full)\n",
    "\n",
    "print(\"\\n\\n--- Generating Figure 7 (User) / Figure 5 (Pub Ideas): Integrating Divergence, Motifs, and Features (Cell 9) ---\")\n",
    "\n",
    "# --- Check if df_full is available and has necessary columns ---\n",
    "required_cols_fig5 = [\n",
    "    orthogroup_col, group_col, esp_col, structurally_dark_col, \n",
    "    apsi_col, 'Has_Conserved_Motif' # Assuming 'Has_Conserved_Motif' is the flag in df_full\n",
    "]\n",
    "if 'df_full' not in locals() or df_full.empty:\n",
    "    print(\"ERROR: DataFrame 'df_full' not found or is empty. Please run Cell 1 first.\")\n",
    "    # Create an empty DataFrame to prevent errors if subsequent code relies on it\n",
    "    df_full = pd.DataFrame(columns=required_cols_fig5) \n",
    "elif not all(col in df_full.columns for col in required_cols_fig5):\n",
    "    missing = [col for col in required_cols_fig5 if col not in df_full.columns]\n",
    "    print(f\"ERROR: 'df_full' is missing required columns for Figure 5: {missing}. Please ensure Cell 1 ran correctly and merged APSI/Motif data.\")\n",
    "    # Potentially create empty df_full to avoid further errors, or exit\n",
    "\n",
    "# --- Prepare data: One row per Orthogroup with relevant flags and APSI ---\n",
    "# This df_og_summary will be used for most plots in this figure.\n",
    "df_og_summary = pd.DataFrame()\n",
    "if orthogroup_col in df_full.columns and not df_full.empty:\n",
    "    # Aggregate Is_ESP: True if any protein in the OG is ESP (for Asgard)\n",
    "    # Aggregate Is_Structurally_Dark: True if >50% proteins in OG are dark (can be adjusted)\n",
    "    # Aggregate Has_Conserved_Motif: True if the OG has any motif (already in df_full)\n",
    "    \n",
    "    agg_dict = {\n",
    "        apsi_col: 'first', # APSI is per OG\n",
    "        'Has_Conserved_Motif': 'first', # Motif flag is per OG\n",
    "        group_col: 'first' # Assuming all proteins in an OG belong to the same group\n",
    "    }\n",
    "    if esp_col in df_full.columns:\n",
    "        # For ESP, we only care about Asgard. So, first filter df_full for Asgard, then aggregate.\n",
    "        # This ensures 'any' ESP flag is specific to Asgard OGs.\n",
    "        # For GV OGs, ESP will effectively be False or NaN depending on merge.\n",
    "        asgard_esp_agg = df_full[df_full[group_col] == 'Asgard'].groupby(orthogroup_col)[esp_col].any().rename(esp_col)\n",
    "    else:\n",
    "        asgard_esp_agg = pd.Series(name=esp_col, dtype=bool) # Empty series if esp_col is missing\n",
    "\n",
    "    if structurally_dark_col in df_full.columns:\n",
    "        # Calculate proportion of structurally dark proteins per OG\n",
    "        df_full['Is_Dark_Numeric'] = df_full[structurally_dark_col].astype(int) # Convert boolean to int for mean\n",
    "        og_dark_prop = df_full.groupby(orthogroup_col)['Is_Dark_Numeric'].mean().rename('Prop_Structurally_Dark')\n",
    "    else:\n",
    "        og_dark_prop = pd.Series(name='Prop_Structurally_Dark', dtype=float) # Empty series\n",
    "\n",
    "    # Base aggregation for APSI, Motif Flag, and Group\n",
    "    df_og_summary_base = df_full.groupby(orthogroup_col).agg(agg_dict).reset_index()\n",
    "\n",
    "    # Merge ESP aggregation (only for Asgard OGs)\n",
    "    df_og_summary = pd.merge(df_og_summary_base, asgard_esp_agg, on=orthogroup_col, how='left')\n",
    "    # Fill NaN for ESP (e.g., for GV OGs or if OG wasn't in Asgard) with False\n",
    "    if esp_col in df_og_summary.columns:\n",
    "        df_og_summary[esp_col] = df_og_summary[esp_col].fillna(False)\n",
    "    else: # If esp_col wasn't even in df_full\n",
    "        df_og_summary[esp_col] = False\n",
    "\n",
    "\n",
    "    # Merge proportion of structurally dark proteins\n",
    "    if not og_dark_prop.empty:\n",
    "        df_og_summary = pd.merge(df_og_summary, og_dark_prop, on=orthogroup_col, how='left')\n",
    "        if 'Prop_Structurally_Dark' in df_og_summary.columns:\n",
    "             df_og_summary['Is_Mostly_Dark_OG'] = df_og_summary['Prop_Structurally_Dark'] > 0.5 # Example threshold\n",
    "        else:\n",
    "             df_og_summary['Is_Mostly_Dark_OG'] = False \n",
    "    else:\n",
    "        # Fallback if structurally_dark_col was directly aggregated or missing\n",
    "        if structurally_dark_col in df_og_summary.columns: # if it was aggregated with 'first'\n",
    "            df_og_summary['Is_Mostly_Dark_OG'] = df_og_summary[structurally_dark_col].astype(bool)\n",
    "        else: \n",
    "            df_og_summary['Is_Mostly_Dark_OG'] = False\n",
    "\n",
    "    # Filter for OGs with valid APSI values for relevant plots\n",
    "    if apsi_col in df_og_summary.columns:\n",
    "        df_og_summary = df_og_summary.dropna(subset=[apsi_col])\n",
    "    else:\n",
    "        print(f\"WARNING: APSI column '{apsi_col}' not found in df_og_summary. APSI-related plots will fail.\")\n",
    "        df_og_summary = pd.DataFrame() # Make it empty if APSI is crucial and missing\n",
    "    \n",
    "    print(f\"Created 'df_og_summary' with {len(df_og_summary)} orthogroups for analysis.\")\n",
    "    if df_og_summary.empty and apsi_col in required_cols_fig5 : # Check if it became empty after APSI dropna\n",
    "        print(\"WARNING: 'df_og_summary' is empty after filtering for valid APSI. Plots for Figure 5 might not generate.\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping Figure 5 generation as 'df_full' is not suitable or 'Orthogroup' column is missing.\")\n",
    "\n",
    "# --- Figure 7A (Pub Ideas): APSI Comparison (ESPs vs. Non-ESPs) ---\n",
    "print(\"\\n--- Figure 7A (Pub Ideas): APSI Comparison (ESPs vs. Non-ESPs) ---\")\n",
    "if not df_og_summary.empty and esp_col in df_og_summary.columns and apsi_col in df_og_summary.columns and group_col in df_og_summary.columns:\n",
    "    # Consider only Asgard OGs for ESP comparison\n",
    "    df_asgard_og_summary = df_og_summary[df_og_summary[group_col] == 'Asgard'].copy()\n",
    "    \n",
    "    if not df_asgard_og_summary.empty:\n",
    "        # Create a more descriptive label for ESP status\n",
    "        df_asgard_og_summary['ESP_Status'] = df_asgard_og_summary[esp_col].apply(lambda x: 'ESP OG' if x else 'Non-ESP OG')\n",
    "\n",
    "        fig_7a_apsi_esp = px.violin(\n",
    "            df_asgard_og_summary.dropna(subset=[apsi_col, 'ESP_Status']),\n",
    "            x='ESP_Status',\n",
    "            y=apsi_col,\n",
    "            color='ESP_Status',\n",
    "            box=True,\n",
    "            points=\"all\", # \"all\" or False or \"outliers\"\n",
    "            # title='APSI Distribution: Asgard ESP OGs vs. Non-ESP OGs', # No title\n",
    "            labels={apsi_col: 'Average Pairwise Sequence Identity (APSI)', 'ESP_Status': 'ESP Orthogroup Status (Asgard)'},\n",
    "            color_discrete_map={'ESP OG': arcadia_primary_palette[0], 'Non-ESP OG': arcadia_primary_palette[1]},\n",
    "            category_orders={'ESP_Status': ['ESP OG', 'Non-ESP OG']}\n",
    "        )\n",
    "        fig_7a_apsi_esp.update_layout(plotly_layout_defaults)\n",
    "        fig_7a_apsi_esp.update_xaxes(title_text='ESP Orthogroup Status (Asgard)', showgrid=False)\n",
    "        fig_7a_apsi_esp.update_yaxes(title_text='Average Pairwise Sequence Identity (APSI)', showgrid=False)\n",
    "        fig_7a_apsi_esp.update_layout(showlegend=False)\n",
    "        \n",
    "        # Export Figure\n",
    "        fig_7a_path_html = Path(output_figure_dir) / \"figure5a_pub_ideas_apsi_esp_vs_non_esp.html\"\n",
    "        fig_7a_apsi_esp.write_html(str(fig_7a_path_html))\n",
    "        print(f\"Figure 7A (APSI ESP vs Non-ESP) HTML saved to: {fig_7a_path_html}\")\n",
    "        try:\n",
    "            fig_7a_path_pdf = Path(output_figure_dir) / \"figure7a_pub_ideas_apsi_esp_vs_non_esp.pdf\"\n",
    "            fig_7a_apsi_esp.write_image(str(fig_7a_path_pdf))\n",
    "            print(f\"Figure 7A (APSI ESP vs Non-ESP) PDF saved to: {fig_7a_path_pdf}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not export Figure 7A to PDF. Error: {e}\")\n",
    "        try:\n",
    "            fig_7a_path_svg = Path(output_figure_dir) / \"figure7a_pub_ideas_apsi_esp_vs_non_esp.svg\"\n",
    "            fig_7a_apsi_esp.write_image(str(fig_5a_path_svg))\n",
    "            print(f\"Figure 7A (APSI ESP vs Non-ESP) SVG saved to: {fig_7a_path_svg}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not export Figure 7A to SVG. Error: {e}\")\n",
    "            \n",
    "    else:\n",
    "        print(\"No Asgard OGs found in df_og_summary to compare ESP vs Non-ESP APSI.\")\n",
    "else:\n",
    "    print(\"Skipping Figure 7A (APSI ESP vs Non-ESP): df_og_summary is empty or missing required columns.\")\n",
    "\n",
    "# --- Figure 7C (Pub Ideas): Divergence/Motifs in Structurally Dark Proteins ---\n",
    "print(\"\\n--- Figure 7C (Pub Ideas): Divergence & Motif Presence in Structurally Dark OGs ---\")\n",
    "\n",
    "# Part 1: APSI vs. Structural Darkness\n",
    "print(\"\\n--- Part 1: APSI vs. Structural Darkness ---\")\n",
    "if not df_og_summary.empty and 'Is_Mostly_Dark_OG' in df_og_summary.columns and apsi_col in df_og_summary.columns:\n",
    "    \n",
    "    # Create a descriptive label for darkness status\n",
    "    df_og_summary['Structural_Darkness_Status_OG'] = df_og_summary['Is_Mostly_Dark_OG'].apply(lambda x: 'Mostly Dark OG' if x else 'Not Mostly Dark OG')\n",
    "\n",
    "    fig_7c_apsi_dark = px.violin(\n",
    "        df_og_summary.dropna(subset=[apsi_col, 'Structural_Darkness_Status_OG', group_col]),\n",
    "        x='Structural_Darkness_Status_OG',\n",
    "        y=apsi_col,\n",
    "        color=group_col, # Compare Asgard vs GV within each darkness category\n",
    "        box=True,\n",
    "        points=False, \n",
    "        # title='APSI Distribution by Orthogroup Structural Darkness Status', # No title\n",
    "        labels={apsi_col: 'Average Pairwise Sequence Identity (APSI)', \n",
    "                'Structural_Darkness_Status_OG': 'OG Structural Darkness Status',\n",
    "                group_col: 'Group (Asgard/GV)'},\n",
    "        color_discrete_map=group_colors, # group_colors defined in Cell 1\n",
    "        category_orders={'Structural_Darkness_Status_OG': ['Mostly Dark OG', 'Not Mostly Dark OG'],\n",
    "                         group_col: ['Asgard', 'GV']} \n",
    "    )\n",
    "    fig_7c_apsi_dark.update_layout(plotly_layout_defaults)\n",
    "    fig_7c_apsi_dark.update_xaxes(title_text='OG Structural Darkness Status', showgrid=False)\n",
    "    fig_7c_apsi_dark.update_yaxes(title_text='Average Pairwise Sequence Identity (APSI)', showgrid=False)\n",
    "    \n",
    "    # Export Figure\n",
    "    fig_7c_part1_path_html = Path(output_figure_dir) / \"figure7c_pub_ideas_apsi_vs_darkness.html\"\n",
    "    fig_7c_apsi_dark.write_html(str(fig_7c_part1_path_html))\n",
    "    print(f\"Figure 7C Part 1 (APSI vs Darkness) HTML saved to: {fig_7c_part1_path_html}\")\n",
    "    try:\n",
    "        fig_7c_part1_path_pdf = Path(output_figure_dir) / \"figure7c_pub_ideas_apsi_vs_darkness.pdf\"\n",
    "        fig_7c_apsi_dark.write_image(str(fig_7c_part1_path_pdf))\n",
    "        print(f\"Figure 7C Part 1 (APSI vs Darkness) PDF saved to: {fig_7c_part1_path_pdf}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not export Figure 7C Part 1 to PDF. Error: {e}\")\n",
    "    try:\n",
    "        fig_7c_part1_path_svg = Path(output_figure_dir) / \"figure7c_pub_ideas_apsi_vs_darkness.svg\"\n",
    "        fig_7c_apsi_dark.write_image(str(fig_7c_part1_path_svg))\n",
    "        print(f\"Figure 7C Part 1 (APSI vs Darkness) SVG saved to: {fig_7c_part1_path_svg}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not export Figure 7C Part 1 to SVG. Error: {e}\")\n",
    "else:\n",
    "    print(\"Skipping Figure 7C Part 1 (APSI vs Darkness): df_og_summary is empty or missing 'Is_Mostly_Dark_OG' or apsi_col.\")\n",
    "\n",
    "# Part 2: Motif Presence vs. Structural Darkness\n",
    "print(\"\\n--- Part 2: Motif Presence vs. Structural Darkness ---\")\n",
    "if not df_og_summary.empty and 'Is_Mostly_Dark_OG' in df_og_summary.columns and 'Has_Conserved_Motif' in df_og_summary.columns:\n",
    "    \n",
    "    # Calculate percentage of OGs with motifs within each darkness category, per group (Asgard/GV)\n",
    "    df_motif_dark_summary = df_og_summary.groupby([group_col, 'Structural_Darkness_Status_OG'])['Has_Conserved_Motif'].agg(\n",
    "        Total_OGs='count',\n",
    "        OGs_With_Motif='sum'\n",
    "    ).reset_index()\n",
    "    df_motif_dark_summary['Percent_OGs_With_Motif'] = (df_motif_dark_summary['OGs_With_Motif'] / df_motif_dark_summary['Total_OGs'] * 100).fillna(0)\n",
    "\n",
    "    print(\"\\nMotif Presence by OG Structural Darkness Status and Group:\")\n",
    "    try:\n",
    "        print(df_motif_dark_summary.to_markdown(index=False))\n",
    "    except ImportError:\n",
    "        print(df_motif_dark_summary)\n",
    "\n",
    "    fig_7c_motif_dark = px.bar(\n",
    "        df_motif_dark_summary,\n",
    "        x='Structural_Darkness_Status_OG',\n",
    "        y='Percent_OGs_With_Motif',\n",
    "        color=group_col,\n",
    "        barmode='group',\n",
    "        # title='Motif Presence in Structurally Dark vs. Non-Dark OGs', # No title\n",
    "        labels={'Percent_OGs_With_Motif': '% of OGs with Conserved Motif(s)', \n",
    "                'Structural_Darkness_Status_OG': 'OG Structural Darkness Status',\n",
    "                group_col: 'Group (Asgard/GV)'},\n",
    "        color_discrete_map=group_colors,\n",
    "        category_orders={'Structural_Darkness_Status_OG': ['Mostly Dark OG', 'Not Mostly Dark OG'],\n",
    "                         group_col: ['Asgard', 'GV']}\n",
    "    )\n",
    "    fig_7c_motif_dark.update_layout(plotly_layout_defaults)\n",
    "    fig_7c_motif_dark.update_xaxes(title_text='OG Structural Darkness Status', showgrid=False)\n",
    "    fig_7c_motif_dark.update_yaxes(title_text='% of OGs with Conserved Motif(s)', showgrid=False)\n",
    "    \n",
    "    # Export Figure\n",
    "    fig_7c_part2_path_html = Path(output_figure_dir) / \"figure7c_pub_ideas_motif_vs_darkness.html\"\n",
    "    fig_7c_motif_dark.write_html(str(fig_7c_part2_path_html))\n",
    "    print(f\"Figure 57 Part 2 (Motif vs Darkness) HTML saved to: {fig_7c_part2_path_html}\")\n",
    "    try:\n",
    "        fig_7c_part2_path_pdf = Path(output_figure_dir) / \"figure7c_pub_ideas_motif_vs_darkness.pdf\"\n",
    "        fig_7c_motif_dark.write_image(str(fig_7c_part2_path_pdf))\n",
    "        print(f\"Figure 7C Part 2 (Motif vs Darkness) PDF saved to: {fig_7c_part2_path_pdf}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not export Figure 7C Part 2 to PDF. Error: {e}\")\n",
    "    try:\n",
    "        fig_7c_part2_path_svg = Path(output_figure_dir) / \"figure7c_pub_ideas_motif_vs_darkness.svg\"\n",
    "        fig_7c_motif_dark.write_image(str(fig_7c_part2_path_svg))\n",
    "        print(f\"Figure 57 Part 2 (Motif vs Darkness) SVG saved to: {fig_7c_part2_path_svg}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not export Figure 5C Part 2 to SVG. Error: {e}\")\n",
    "else:\n",
    "    print(\"Skipping Figure 7C Part 2 (Motif vs Darkness): df_og_summary is empty or missing required columns.\")\n",
    "\n",
    "print(\"\\n\\n--- Figure 7 (User) / Figure 5 (Pub Ideas) - Cell 9 Complete ---\")\n",
    "print(f\"Figures and summaries saved to '{output_figure_dir}' and '{output_summary_dir_phase1}'.\")\n",
    "print(\"Next, we can consider the Lokiactin case study or other specific analyses.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed746979-2db0-4296-a9cd-83b10d79bf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Case Study for Figure 8 - Lokiactin (OG0000203.ASG) - APSI and Structural Darkness\n",
    "\n",
    "# --- Imports & Setup Assumptions ---\n",
    "# This cell assumes that pandas (pd), numpy (np), plotly.express (px),\n",
    "# Path from pathlib, and all necessary variables/helper functions from Cell 1 (Setup) are available.\n",
    "# Key variables from Cell 1:\n",
    "# - df_full: Main DataFrame with all protein annotations.\n",
    "# - df_og_summary: DataFrame created in Cell 9, containing OG-level summaries including Intra_OG_APSI.\n",
    "# - output_figure_dir, output_summary_dir_phase1: Directories to save plots/data.\n",
    "# - plotly_layout_defaults: Default layout for Plotly figures.\n",
    "# - arcadia_primary_palette: Color palette.\n",
    "# - orthogroup_col, protein_id_col, structurally_dark_col, apsi_col ('Intra_OG_APSI')\n",
    "\n",
    "print(\"\\n\\n--- Generating Lokiactin (OG0000203.ASG) Case Study - APSI and Structural Darkness (Cell 10) ---\")\n",
    "\n",
    "# --- Configuration for this Case Study ---\n",
    "target_og_id = \"OG0000203.ASG\"\n",
    "reference_protein_id = \"UYP47028.1\" # The experimentally characterized Lokiactin\n",
    "\n",
    "# --- Check if necessary DataFrames are available ---\n",
    "if 'df_full' not in locals() or df_full.empty:\n",
    "    print(f\"ERROR: DataFrame 'df_full' not found or is empty. Please run Cell 1 first.\")\n",
    "    df_lokiactin_og_members = pd.DataFrame() # Ensure it's defined\n",
    "elif 'df_og_summary' not in locals() or df_og_summary.empty:\n",
    "    print(f\"ERROR: DataFrame 'df_og_summary' (with OG-level APSI) not found or is empty. Please run Cell 9 first.\")\n",
    "    df_lokiactin_og_members = pd.DataFrame() # Ensure it's defined\n",
    "elif orthogroup_col not in df_full.columns or protein_id_col not in df_full.columns:\n",
    "    print(f\"ERROR: 'df_full' is missing required columns: '{orthogroup_col}' or '{protein_id_col}'.\")\n",
    "    df_lokiactin_og_members = pd.DataFrame()\n",
    "elif orthogroup_col not in df_og_summary.columns or apsi_col not in df_og_summary.columns:\n",
    "    print(f\"ERROR: 'df_og_summary' is missing required columns: '{orthogroup_col}' or '{apsi_col}'.\")\n",
    "    df_lokiactin_og_members = pd.DataFrame()\n",
    "else:\n",
    "    # --- 1. Filter for the Lokiactin Orthogroup Members from df_full ---\n",
    "    print(f\"\\n--- Analyzing Orthogroup: {target_og_id} ---\")\n",
    "    df_lokiactin_og_members = df_full[df_full[orthogroup_col] == target_og_id].copy()\n",
    "    \n",
    "    if df_lokiactin_og_members.empty:\n",
    "        print(f\"No proteins found for Orthogroup {target_og_id} in df_full.\")\n",
    "    else:\n",
    "        num_members = len(df_lokiactin_og_members)\n",
    "        print(f\"Found {num_members} protein members in {target_og_id}.\")\n",
    "\n",
    "        # --- 2. Verify Presence of Reference Lokiactin ---\n",
    "        reference_present = reference_protein_id in df_lokiactin_og_members[protein_id_col].values\n",
    "        print(f\"Reference Lokiactin ({reference_protein_id}) present in this OG: {reference_present}\")\n",
    "        if not reference_present:\n",
    "            print(f\"WARNING: Reference protein {reference_protein_id} was NOT found among the members of {target_og_id} in your dataset.\")\n",
    "\n",
    "        # --- 3. Get Pre-calculated APSI for this Orthogroup ---\n",
    "        og_apsi_data = df_og_summary[df_og_summary[orthogroup_col] == target_og_id]\n",
    "        \n",
    "        if og_apsi_data.empty:\n",
    "            lokiactin_og_apsi = np.nan\n",
    "            print(f\"WARNING: APSI data not found for {target_og_id} in df_og_summary.\")\n",
    "        else:\n",
    "            lokiactin_og_apsi = og_apsi_data[apsi_col].iloc[0]\n",
    "            print(f\"Intra-OG Average Pairwise Sequence Identity (APSI) for {target_og_id}: {lokiactin_og_apsi:.4f}\")\n",
    "\n",
    "        # --- 4. Analyze Structural Darkness within the Lokiactin OG ---\n",
    "        if structurally_dark_col not in df_lokiactin_og_members.columns:\n",
    "            print(f\"WARNING: Column '{structurally_dark_col}' not found. Cannot analyze structural darkness for this OG.\")\n",
    "            num_dark_members = 0\n",
    "            percent_dark = 0.0\n",
    "            df_darkness_counts = pd.DataFrame()\n",
    "        else:\n",
    "            df_lokiactin_og_members[structurally_dark_col] = df_lokiactin_og_members[structurally_dark_col].fillna(False).astype(bool)\n",
    "            num_dark_members = df_lokiactin_og_members[structurally_dark_col].sum()\n",
    "            percent_dark = (num_dark_members / num_members) * 100 if num_members > 0 else 0.0\n",
    "            \n",
    "            print(f\"Number of structurally dark members in {target_og_id}: {num_dark_members} out of {num_members} ({percent_dark:.1f}%)\")\n",
    "\n",
    "            # Prepare data for plotting darkness breakdown\n",
    "            darkness_counts = df_lokiactin_og_members[structurally_dark_col].value_counts().reset_index()\n",
    "            darkness_counts.columns = ['Is_Structurally_Dark', 'Count']\n",
    "            darkness_counts['Label'] = darkness_counts['Is_Structurally_Dark'].apply(lambda x: 'Structurally Dark' if x else 'Has Known/Predicted Structure')\n",
    "            df_darkness_counts = darkness_counts\n",
    "\n",
    "\n",
    "            # --- 5. Visualize Structural Darkness Breakdown for Lokiactin OG ---\n",
    "            if not df_darkness_counts.empty:\n",
    "                fig_lokiactin_darkness = px.bar(\n",
    "                    df_darkness_counts,\n",
    "                    x='Label',\n",
    "                    y='Count',\n",
    "                    color='Label',\n",
    "                    # title=f'Structural Darkness of Proteins in Lokiactin OG ({target_og_id})', # No title\n",
    "                    labels={'Count': 'Number of Proteins', 'Label': 'Structural Annotation Status'},\n",
    "                    color_discrete_map={'Structurally Dark': arcadia_primary_palette[3], 'Has Known/Predicted Structure': arcadia_primary_palette[4]}\n",
    "                )\n",
    "                fig_lokiactin_darkness.update_layout(plotly_layout_defaults)\n",
    "                fig_lokiactin_darkness.update_xaxes(title_text='Structural Annotation Status', showgrid=False)\n",
    "                fig_lokiactin_darkness.update_yaxes(title_text='Number of Proteins', showgrid=False)\n",
    "                fig_lokiactin_darkness.update_layout(showlegend=False)\n",
    "                fig_lokiactin_darkness.show()\n",
    "\n",
    "                # Save plot\n",
    "                fig_lokiactin_dark_path_html = Path(output_figure_dir) / f\"lokiactin_{target_og_id}_darkness_breakdown.html\"\n",
    "                fig_lokiactin_darkness.write_html(str(fig_lokiactin_dark_path_html))\n",
    "                print(f\"Lokiactin OG darkness breakdown plot HTML saved to: {fig_lokiactin_dark_path_html}\")\n",
    "                try:\n",
    "                    fig_lokiactin_dark_path_pdf = Path(output_figure_dir) / f\"lokiactin_{target_og_id}_darkness_breakdown.pdf\"\n",
    "                    fig_lokiactin_darkness.write_image(str(fig_lokiactin_dark_path_pdf))\n",
    "                    print(f\"Lokiactin OG darkness breakdown plot PDF saved to: {fig_lokiactin_dark_path_pdf}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not export Lokiactin darkness plot to PDF. Error: {e}\")\n",
    "                try:\n",
    "                    fig_lokiactin_dark_path_svg = Path(output_figure_dir) / f\"lokiactin_{target_og_id}_darkness_breakdown.svg\"\n",
    "                    fig_lokiactin_darkness.write_image(str(fig_lokiactin_dark_path_svg))\n",
    "                    print(f\"Lokiactin OG darkness breakdown plot SVG saved to: {fig_lokiactin_dark_path_svg}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not export Lokiactin darkness plot to SVG. Error: {e}\")\n",
    "            else:\n",
    "                print(\"No data to plot for structural darkness breakdown of Lokiactin OG.\")\n",
    "\n",
    "        # --- Summary Output for Lokiactin OG ---\n",
    "        print(\"\\n--- Lokiactin Orthogroup Summary ---\")\n",
    "        print(f\"Orthogroup ID: {target_og_id}\")\n",
    "        print(f\"Contains Reference Lokiactin ({reference_protein_id}): {reference_present}\")\n",
    "        print(f\"Number of Protein Members: {num_members}\")\n",
    "        print(f\"Intra-OG APSI: {lokiactin_og_apsi:.4f}\" if not pd.isna(lokiactin_og_apsi) else \"APSI: Not Available\")\n",
    "        if structurally_dark_col in df_lokiactin_og_members.columns:\n",
    "            print(f\"Structurally Dark Members: {num_dark_members} ({percent_dark:.1f}%)\")\n",
    "        \n",
    "        # Save summary data for this OG\n",
    "        lokiactin_og_details_path = Path(output_summary_dir_phase1) / f\"lokiactin_{target_og_id}_detailed_summary.csv\"\n",
    "        try:\n",
    "            # Save all members of the OG with their relevant info\n",
    "            cols_to_save = [protein_id_col, sequence_col, 'Length', structurally_dark_col, group_col, asgard_phylum_col, esp_col, 'Has_Conserved_Motif']\n",
    "            cols_present_in_df = [col for col in cols_to_save if col in df_lokiactin_og_members.columns]\n",
    "            df_lokiactin_og_members[cols_present_in_df].to_csv(lokiactin_og_details_path, index=False)\n",
    "            print(f\"Detailed member data for {target_og_id} saved to: {lokiactin_og_details_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving Lokiactin OG detailed member data: {e}\")\n",
    "\n",
    "print(\"\\n\\n--- Cell 10 (Lokiactin Case Study - APSI & Darkness) Complete ---\")\n",
    "print(f\"Analysis for OG {target_og_id} focusing on its APSI and structural darkness of members is done.\")\n",
    "print(f\"Figures and summaries saved to '{output_figure_dir}' and '{output_summary_dir_phase1}'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba1399b-2fdb-4463-98e6-ad0b3a8057f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Figure 8 (User's numbering) - Case Studies: Divergence, Diversity, and Structural Darkness\n",
    "\n",
    "# --- Imports & Setup Assumptions ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots # ****** ENSURED THIS IMPORT IS PRESENT ******\n",
    "from pathlib import Path\n",
    "# Other necessary imports like Counter might be needed if used, but make_subplots was the direct error.\n",
    "\n",
    "# This cell assumes that pandas (pd), numpy (np), plotly.express (px),\n",
    "# plotly.graph_objects (go), Path from pathlib,\n",
    "# and all necessary variables/helper functions from Cell 1 (Setup) are available.\n",
    "# Key variables from Cell 1 & subsequent cells:\n",
    "# - df_full: Main DataFrame with all protein annotations.\n",
    "# - df_og_summary: DataFrame created in Cell 9, containing OG-level summaries including Intra_OG_APSI.\n",
    "# - output_figure_dir, output_summary_dir_phase1: Directories to save plots/data.\n",
    "# - plotly_layout_defaults: Default layout for Plotly figures. (This is a go.Layout object)\n",
    "# - arcadia_primary_palette: Color palette.\n",
    "# - orthogroup_col, protein_id_col, structurally_dark_col, apsi_col ('Intra_OG_APSI'), group_col, sequence_col\n",
    "\n",
    "print(\"\\n\\n--- Generating Figure 8: Case Studies - Divergence, Diversity, and Structural Darkness (Cell 11) ---\")\n",
    "\n",
    "# --- Configuration for Case Studies ---\n",
    "case_study_protein_ids_str = \"UYP47028.1;KKK44605.1;KKK42122.1;OLS22855.1;TFG12995.1;OLS30618.1;TET76256.1;OLS23013.1;NHJ47629.1\"\n",
    "case_study_protein_ids = [pid.strip() for pid in case_study_protein_ids_str.split(';')]\n",
    "\n",
    "diversity_metrics_path = \"orthogroup_diversity_metrics.csv\" # Path to the uploaded diversity file\n",
    "\n",
    "# --- Check if necessary DataFrames are available ---\n",
    "error_found = False\n",
    "# Ensure df_full and df_og_summary are defined and not empty (from previous cells)\n",
    "if 'df_full' not in locals() or df_full.empty:\n",
    "    print(f\"ERROR: DataFrame 'df_full' not found or is empty. Please run Cell 1 first.\")\n",
    "    error_found = True\n",
    "elif 'df_og_summary' not in locals() or df_og_summary.empty:\n",
    "    print(f\"ERROR: DataFrame 'df_og_summary' (with OG-level APSI) not found or is empty. Please run Cell 9 first.\")\n",
    "    error_found = True\n",
    "# Check for essential columns within df_full\n",
    "elif not all(col in df_full.columns for col in [orthogroup_col, protein_id_col, structurally_dark_col, group_col, sequence_col]):\n",
    "    missing_cols = [col for col in [orthogroup_col, protein_id_col, structurally_dark_col, group_col, sequence_col] if col not in df_full.columns]\n",
    "    print(f\"ERROR: 'df_full' is missing required columns: {missing_cols}.\")\n",
    "    error_found = True\n",
    "# Check for essential columns within df_og_summary\n",
    "elif not all(col in df_og_summary.columns for col in [orthogroup_col, apsi_col]):\n",
    "    missing_cols = [col for col in [orthogroup_col, apsi_col] if col not in df_og_summary.columns]\n",
    "    print(f\"ERROR: 'df_og_summary' is missing required columns: {missing_cols}.\")\n",
    "    error_found = True\n",
    "\n",
    "if error_found:\n",
    "    print(\"Cannot proceed with case study generation due to missing data.\")\n",
    "    # For this environment, we'll just print the error and not run the rest.\n",
    "else:\n",
    "    # --- 1. Load Diversity Metrics ---\n",
    "    print(f\"\\n--- Loading Orthogroup Diversity Metrics from: {diversity_metrics_path} ---\")\n",
    "    try:\n",
    "        df_diversity = pd.read_csv(diversity_metrics_path)\n",
    "        # Rename OG_ID to orthogroup_col for consistent merging, if needed\n",
    "        if 'OG_ID' in df_diversity.columns and orthogroup_col not in df_diversity.columns:\n",
    "            df_diversity = df_diversity.rename(columns={'OG_ID': orthogroup_col})\n",
    "        \n",
    "        # Select and rename relevant diversity columns\n",
    "        diversity_cols_to_use = {\n",
    "            orthogroup_col: orthogroup_col,\n",
    "            'Tree_n_tips': 'Observed_Richness', # Number of tips in the tree\n",
    "            'Tree_entropy': 'Shannon_Entropy'   # Shannon entropy from tree\n",
    "        }\n",
    "        # Filter for columns that actually exist in the loaded df_diversity\n",
    "        actual_diversity_cols = {k: v for k, v in diversity_cols_to_use.items() if k in df_diversity.columns}\n",
    "        \n",
    "        if len(actual_diversity_cols) < 3: # Expecting Orthogroup + 2 metrics\n",
    "            print(f\"WARNING: Not all expected diversity columns found in '{diversity_metrics_path}'. Found: {df_diversity.columns.tolist()}\")\n",
    "            print(f\"         Will use available columns for diversity metrics: {list(actual_diversity_cols.keys())}\")\n",
    "        \n",
    "        # Create the subset using only the columns that were actually found\n",
    "        df_diversity_subset = df_diversity[list(actual_diversity_cols.keys())].rename(columns=actual_diversity_cols).copy()\n",
    "        print(f\"Loaded and processed diversity metrics for {len(df_diversity_subset)} orthogroups.\")\n",
    "        if df_diversity_subset.empty:\n",
    "            print(\"WARNING: Diversity metrics DataFrame is empty after processing.\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Diversity metrics file not found at '{diversity_metrics_path}'. Diversity metrics will be missing.\")\n",
    "        df_diversity_subset = pd.DataFrame(columns=[orthogroup_col, 'Observed_Richness', 'Shannon_Entropy']) # Empty df for graceful failure\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or processing diversity metrics file '{diversity_metrics_path}': {e}\")\n",
    "        df_diversity_subset = pd.DataFrame(columns=[orthogroup_col, 'Observed_Richness', 'Shannon_Entropy'])\n",
    "\n",
    "\n",
    "    # --- 2. Process Input Protein IDs to Get Unique Orthogroups ---\n",
    "    print(\"\\n--- Identifying Unique Orthogroups for Case Studies ---\")\n",
    "    case_study_ogs_info = []\n",
    "    seen_ogs = set()\n",
    "\n",
    "    for ref_pid in case_study_protein_ids:\n",
    "        # Ensure protein_id_col is correctly defined and exists in df_full\n",
    "        if protein_id_col not in df_full.columns:\n",
    "            print(f\"CRITICAL ERROR: Protein ID column '{protein_id_col}' not found in df_full. Cannot map Protein IDs to OGs.\")\n",
    "            break \n",
    "        og_entry = df_full[df_full[protein_id_col] == ref_pid]\n",
    "        if not og_entry.empty:\n",
    "            # Ensure orthogroup_col exists\n",
    "            if orthogroup_col not in og_entry.columns:\n",
    "                print(f\"CRITICAL ERROR: Orthogroup column '{orthogroup_col}' not found in df_full. Cannot map Protein IDs to OGs.\")\n",
    "                break \n",
    "            og_id = og_entry[orthogroup_col].iloc[0]\n",
    "            if og_id not in seen_ogs:\n",
    "                case_study_ogs_info.append({'Ref_ProteinID': ref_pid, orthogroup_col: og_id})\n",
    "                seen_ogs.add(og_id)\n",
    "        else:\n",
    "            print(f\"Warning: Reference Protein ID {ref_pid} not found in df_full.\")\n",
    "    \n",
    "    if not case_study_ogs_info:\n",
    "        print(\"ERROR: No valid orthogroups found for the provided Protein IDs. Cannot create case studies.\")\n",
    "    else:\n",
    "        print(f\"Identified {len(case_study_ogs_info)} unique orthogroups for case studies.\")\n",
    "\n",
    "        # --- 3. Gather Data for Each Case Study OG ---\n",
    "        plot_data_list = []\n",
    "        for og_info in case_study_ogs_info:\n",
    "            og_id = og_info[orthogroup_col]\n",
    "            ref_pid = og_info['Ref_ProteinID']\n",
    "            \n",
    "            df_og_members = df_full[df_full[orthogroup_col] == og_id]\n",
    "            if df_og_members.empty:\n",
    "                print(f\"Warning: No members found for {og_id} in df_full during data gathering. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            num_members = len(df_og_members)\n",
    "            group_name = df_og_members[group_col].iloc[0] if group_col in df_og_members.columns and not df_og_members[group_col].empty else \"N/A\"\n",
    "            \n",
    "            apsi_value_series = df_og_summary[df_og_summary[orthogroup_col] == og_id][apsi_col]\n",
    "            apsi_value = apsi_value_series.iloc[0] if not apsi_value_series.empty else np.nan\n",
    "            \n",
    "            if structurally_dark_col in df_og_members.columns:\n",
    "                percent_dark = (df_og_members[structurally_dark_col].sum() / num_members) * 100 if num_members > 0 else 0.0\n",
    "            else:\n",
    "                percent_dark = np.nan \n",
    "                print(f\"Warning: Structurally dark column '{structurally_dark_col}' not found for OG {og_id}\")\n",
    "\n",
    "            diversity_entry = df_diversity_subset[df_diversity_subset[orthogroup_col] == og_id]\n",
    "            shannon_entropy = diversity_entry['Shannon_Entropy'].iloc[0] if not diversity_entry.empty and 'Shannon_Entropy' in diversity_entry.columns else np.nan\n",
    "            observed_richness = diversity_entry['Observed_Richness'].iloc[0] if not diversity_entry.empty and 'Observed_Richness' in diversity_entry.columns else np.nan\n",
    "            \n",
    "            plot_data_list.append({\n",
    "                'Orthogroup': og_id,\n",
    "                'Ref_ProteinID': ref_pid,\n",
    "                'APSI': apsi_value * 100 if not pd.isna(apsi_value) else np.nan, \n",
    "                'Percent_Dark': percent_dark,\n",
    "                'Shannon_Entropy': shannon_entropy,\n",
    "                'Observed_Richness': observed_richness,\n",
    "                'Total_Members': num_members,\n",
    "                'Group': group_name\n",
    "            })\n",
    "\n",
    "        df_plot_data = pd.DataFrame(plot_data_list)\n",
    "\n",
    "        # --- 4. Create the Figure with Subplots ---\n",
    "        if not df_plot_data.empty:\n",
    "            num_case_studies = len(df_plot_data)\n",
    "            subplot_height = 150 \n",
    "            annotation_lines = 4 \n",
    "            annotation_line_height = 20 \n",
    "            total_subplot_height = subplot_height + (annotation_lines * annotation_line_height)\n",
    "            \n",
    "            fig = make_subplots(\n",
    "                rows=num_case_studies, \n",
    "                cols=1,\n",
    "                subplot_titles=[f\"<b>OG: {row['Orthogroup']}</b> (Ref: {row['Ref_ProteinID']})\" for index, row in df_plot_data.iterrows()],\n",
    "                vertical_spacing=0.08, \n",
    "                row_heights=[total_subplot_height]*num_case_studies \n",
    "            )\n",
    "\n",
    "            for i, (index, row) in enumerate(df_plot_data.iterrows()):\n",
    "                row_num = i + 1 \n",
    "                \n",
    "                fig.add_trace(go.Bar(\n",
    "                    y=['APSI (%)'], \n",
    "                    x=[row['APSI'] if not pd.isna(row['APSI']) else 0], \n",
    "                    orientation='h', \n",
    "                    name='APSI',\n",
    "                    marker_color=arcadia_primary_palette[0 % len(arcadia_primary_palette)],\n",
    "                    text=[f\"{row['APSI']:.1f}%\" if not pd.isna(row['APSI']) else \"N/A\"],\n",
    "                    textposition='outside',\n",
    "                    width=0.3 \n",
    "                ), row=row_num, col=1)\n",
    "                \n",
    "                fig.add_trace(go.Bar(\n",
    "                    y=['% Structurally Dark'], \n",
    "                    x=[row['Percent_Dark'] if not pd.isna(row['Percent_Dark']) else 0], \n",
    "                    orientation='h', \n",
    "                    name='% Dark',\n",
    "                    marker_color=arcadia_primary_palette[1 % len(arcadia_primary_palette)],\n",
    "                    text=[f\"{row['Percent_Dark']:.1f}%\" if not pd.isna(row['Percent_Dark']) else \"N/A\"],\n",
    "                    textposition='outside',\n",
    "                    width=0.3 \n",
    "                ), row=row_num, col=1)\n",
    "\n",
    "                fig.update_xaxes(range=[0, 105], row=row_num, col=1, showgrid=False, zeroline=False, linecolor='black', linewidth=1.5, ticks=\"outside\", ticklen=5, tickwidth=1.5, tickcolor='black', title_text=\"Percentage\")\n",
    "                fig.update_yaxes(showgrid=False, zeroline=False, linecolor='black', linewidth=1.5, ticks=\"outside\", ticklen=5, tickwidth=1.5, tickcolor='black', row=row_num, col=1, autorange=\"reversed\", title_text=\"\") \n",
    "\n",
    "                annotations_texts = [\n",
    "                    f\"<b>Group:</b> {row['Group']}\",\n",
    "                    f\"<b>Total Members:</b> {int(row['Total_Members']) if not pd.isna(row['Total_Members']) else 'N/A'}\",\n",
    "                    f\"<b>Shannon Entropy (Tree):</b> {row['Shannon_Entropy']:.2f}\" if not pd.isna(row['Shannon_Entropy']) else \"Shannon Entropy: N/A\",\n",
    "                    f\"<b>Observed Richness (Tree Tips):</b> {int(row['Observed_Richness']) if not pd.isna(row['Observed_Richness']) else 'Observed Richness: N/A'}\"\n",
    "                ]\n",
    "                \n",
    "                current_subplot_yaxis = f\"y{row_num}\" \n",
    "                \n",
    "                for k, text in enumerate(annotations_texts):\n",
    "                    fig.add_annotation(\n",
    "                        text=text,\n",
    "                        align='left',\n",
    "                        showarrow=False,\n",
    "                        xref='paper', \n",
    "                        yref=current_subplot_yaxis, \n",
    "                        x=0.02, \n",
    "                        y=-0.6 - (k*0.25), \n",
    "                        xanchor='left',\n",
    "                        yanchor='top', \n",
    "                        font=dict(size=10, color=\"black\")\n",
    "                    )\n",
    "            \n",
    "            fig.update_layout(\n",
    "                height=total_subplot_height * num_case_studies + 100, \n",
    "                showlegend=False,\n",
    "                plot_bgcolor='rgba(0,0,0,0)', \n",
    "                paper_bgcolor='rgba(0,0,0,0)',\n",
    "                margin=dict(l=150, r=50, t=50 + (num_case_studies*25), b=50 + (num_case_studies * annotation_lines * 5)) \n",
    "            )\n",
    "            \n",
    "            # ****** CORRECTED SECTION FOR APPLYING FONT STYLES ******\n",
    "            if 'plotly_layout_defaults' in locals():\n",
    "                default_font_family = plotly_layout_defaults.font.family if plotly_layout_defaults.font else 'Arial'\n",
    "                default_font_size_axis_title = plotly_layout_defaults.xaxis.title.font.size if plotly_layout_defaults.xaxis and plotly_layout_defaults.xaxis.title and plotly_layout_defaults.xaxis.title.font else 12\n",
    "                \n",
    "                for i in range(1, num_case_studies + 1):\n",
    "                    # Subplot titles (annotations for subplots)\n",
    "                    # Plotly subplot titles are actually part of the layout.annotations\n",
    "                    # The first num_case_studies annotations are the subplot titles if subplot_titles was used.\n",
    "                    if len(fig.layout.annotations) >= i: \n",
    "                         fig.layout.annotations[i-1].font.size=14 \n",
    "                         fig.layout.annotations[i-1].font.family=default_font_family\n",
    "                    \n",
    "                    # X-axis titles for each subplot\n",
    "                    if hasattr(fig.layout, f'xaxis{i}') and fig.layout[f'xaxis{i}'].title:\n",
    "                        fig.layout[f'xaxis{i}'].title.font.size = default_font_size_axis_title\n",
    "                        fig.layout[f'xaxis{i}'].title.font.family = default_font_family\n",
    "                    \n",
    "                    # Y-axis tick labels for each subplot\n",
    "                    if hasattr(fig.layout, f'yaxis{i}'):\n",
    "                        fig.layout[f'yaxis{i}'].tickfont.size = 11 # Adjust y-axis category label size\n",
    "                        fig.layout[f'yaxis{i}'].tickfont.family = default_font_family\n",
    "            # ****** END OF CORRECTED SECTION ******\n",
    "\n",
    "            fig.show()\n",
    "\n",
    "            fig_case_study_path_html = Path(output_figure_dir) / \"figure8_case_studies_summary.html\"\n",
    "            fig.write_html(str(fig_case_study_path_html))\n",
    "            print(f\"Figure 8 (Case Studies) HTML saved to: {fig_case_study_path_html}\")\n",
    "            try:\n",
    "                fig_case_study_path_pdf = Path(output_figure_dir) / \"figure8_case_studies_summary.pdf\"\n",
    "                fig.write_image(str(fig_case_study_path_pdf), width=800, height=total_subplot_height * num_case_studies + 100) \n",
    "                print(f\"Figure 8 (Case Studies) PDF saved to: {fig_case_study_path_pdf}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not export Figure 8 (Case Studies) to PDF. Error: {e}\")\n",
    "            try:\n",
    "                fig_case_study_path_svg = Path(output_figure_dir) / \"figure8_case_studies_summary.svg\"\n",
    "                fig.write_image(str(fig_case_study_path_svg), width=800, height=total_subplot_height * num_case_studies + 100) \n",
    "                print(f\"Figure 8 (Case Studies) SVG saved to: {fig_case_study_path_svg}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not export Figure 8 (Case Studies) to SVG. Error: {e}\")\n",
    "\n",
    "            case_study_summary_path = Path(output_summary_dir_phase1) / \"figure8_case_studies_data.csv\"\n",
    "            try:\n",
    "                df_plot_data.to_csv(case_study_summary_path, index=False)\n",
    "                print(f\"Case study summary data saved to: {case_study_summary_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving case study summary data: {e}\")\n",
    "        else:\n",
    "            print(\"No data processed for plotting case studies after gathering all metrics.\")\n",
    "\n",
    "print(\"\\n\\n--- Cell 11 (Figure 8 Case Studies) Complete ---\")\n",
    "print(f\"Figures and summaries saved to '{output_figure_dir}' and '{output_summary_dir_phase1}'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d924eef-675f-48d0-af05-9e5ba4ecea02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Figure 8 (User's numbering) - Case Studies: Faceted by Metric\n",
    "\n",
    "# --- Imports & Setup Assumptions ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from pathlib import Path\n",
    "\n",
    "# This cell assumes that pandas (pd), numpy (np), plotly.express (px),\n",
    "# plotly.graph_objects (go), Path from pathlib, make_subplots,\n",
    "# and all necessary variables/helper functions from Cell 1 (Setup) are available.\n",
    "# Key variables from Cell 1 & subsequent cells:\n",
    "# - df_full: Main DataFrame with all protein annotations.\n",
    "# - df_og_summary: DataFrame created in Cell 9, containing OG-level summaries including Intra_OG_APSI.\n",
    "# - output_figure_dir, output_summary_dir_phase1: Directories to save plots/data.\n",
    "# - plotly_layout_defaults: Default layout for Plotly figures.\n",
    "# - arcadia_primary_palette, arcadia_secondary_palette: Color palettes.\n",
    "# - orthogroup_col, protein_id_col, structurally_dark_col, apsi_col ('Intra_OG_APSI'), \n",
    "#   group_col, broad_func_cat_col\n",
    "\n",
    "print(\"\\n\\n--- Generating Figure 8: Case Studies (Faceted by Metric) (Cell 12) ---\")\n",
    "\n",
    "# --- Configuration for Case Studies ---\n",
    "case_study_protein_ids_str = \"UYP47028.1;KKK44605.1;KKK42122.1;OLS22855.1;TFG12995.1;OLS30618.1;TET76256.1;OLS23013.1;NHJ47629.1\"\n",
    "case_study_protein_ids = [pid.strip() for pid in case_study_protein_ids_str.split(';')]\n",
    "\n",
    "diversity_metrics_path = \"orthogroup_diversity_metrics.csv\" # Path to the uploaded diversity file\n",
    "\n",
    "# --- Check if necessary DataFrames are available ---\n",
    "error_found = False\n",
    "required_dataframes = ['df_full', 'df_og_summary']\n",
    "for df_name in required_dataframes:\n",
    "    if df_name not in locals() or locals()[df_name].empty:\n",
    "        print(f\"ERROR: DataFrame '{df_name}' not found or is empty. Please run prerequisite cells.\")\n",
    "        error_found = True\n",
    "\n",
    "required_cols_df_full = [orthogroup_col, protein_id_col, structurally_dark_col, group_col, broad_func_cat_col]\n",
    "required_cols_df_og_summary = [orthogroup_col, apsi_col]\n",
    "\n",
    "if not error_found:\n",
    "    if not all(col in df_full.columns for col in required_cols_df_full):\n",
    "        missing = [col for col in required_cols_df_full if col not in df_full.columns]\n",
    "        print(f\"ERROR: 'df_full' is missing required columns: {missing}.\")\n",
    "        error_found = True\n",
    "    if not all(col in df_og_summary.columns for col in required_cols_df_og_summary):\n",
    "        missing = [col for col in required_cols_df_og_summary if col not in df_og_summary.columns]\n",
    "        print(f\"ERROR: 'df_og_summary' is missing required columns: {missing}.\")\n",
    "        error_found = True\n",
    "\n",
    "if error_found:\n",
    "    print(\"Cannot proceed with case study generation due to missing data or columns.\")\n",
    "else:\n",
    "    # --- 1. Load Diversity Metrics ---\n",
    "    print(f\"\\n--- Loading Orthogroup Diversity Metrics from: {diversity_metrics_path} ---\")\n",
    "    try:\n",
    "        df_diversity = pd.read_csv(diversity_metrics_path)\n",
    "        if 'OG_ID' in df_diversity.columns and orthogroup_col not in df_diversity.columns:\n",
    "            df_diversity = df_diversity.rename(columns={'OG_ID': orthogroup_col})\n",
    "        \n",
    "        diversity_cols_to_use = {\n",
    "            orthogroup_col: orthogroup_col,\n",
    "            'Tree_n_tips': 'Observed_Richness',\n",
    "            'Tree_entropy': 'Shannon_Entropy'\n",
    "        }\n",
    "        actual_diversity_cols = {k: v for k, v in diversity_cols_to_use.items() if k in df_diversity.columns}\n",
    "        df_diversity_subset = df_diversity[list(actual_diversity_cols.keys())].rename(columns=actual_diversity_cols).copy()\n",
    "        print(f\"Loaded and processed diversity metrics for {len(df_diversity_subset)} orthogroups.\")\n",
    "        if df_diversity_subset.empty: print(\"WARNING: Diversity metrics DataFrame is empty.\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Diversity metrics file '{diversity_metrics_path}' not found. Metrics will be missing.\")\n",
    "        df_diversity_subset = pd.DataFrame(columns=[orthogroup_col, 'Observed_Richness', 'Shannon_Entropy'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading diversity metrics: {e}\")\n",
    "        df_diversity_subset = pd.DataFrame(columns=[orthogroup_col, 'Observed_Richness', 'Shannon_Entropy'])\n",
    "\n",
    "    # --- 2. Process Input Protein IDs to Get Unique Orthogroups & Ref Protein Info ---\n",
    "    print(\"\\n--- Identifying Unique Orthogroups and Reference Protein Info ---\")\n",
    "    case_study_ogs_info_list = []\n",
    "    seen_ogs = set()\n",
    "\n",
    "    for ref_pid in case_study_protein_ids:\n",
    "        og_entry = df_full[df_full[protein_id_col] == ref_pid]\n",
    "        if not og_entry.empty:\n",
    "            og_id = og_entry[orthogroup_col].iloc[0]\n",
    "            ref_func_cat = og_entry[broad_func_cat_col].iloc[0] if broad_func_cat_col in og_entry.columns else \"N/A\"\n",
    "            if og_id not in seen_ogs:\n",
    "                case_study_ogs_info_list.append({\n",
    "                    'Ref_ProteinID': ref_pid, \n",
    "                    orthogroup_col: og_id,\n",
    "                    'Ref_Function': ref_func_cat\n",
    "                })\n",
    "                seen_ogs.add(og_id)\n",
    "        else:\n",
    "            print(f\"Warning: Reference Protein ID {ref_pid} not found in df_full.\")\n",
    "    \n",
    "    if not case_study_ogs_info_list:\n",
    "        print(\"ERROR: No valid orthogroups found. Cannot create case studies.\")\n",
    "    else:\n",
    "        df_case_study_refs = pd.DataFrame(case_study_ogs_info_list)\n",
    "        print(f\"Identified {len(df_case_study_refs)} unique orthogroups for case studies.\")\n",
    "\n",
    "        # --- 3. Gather Data for Each Case Study OG ---\n",
    "        plot_data_list = []\n",
    "        for index, ref_row in df_case_study_refs.iterrows():\n",
    "            og_id = ref_row[orthogroup_col]\n",
    "            \n",
    "            df_og_members = df_full[df_full[orthogroup_col] == og_id]\n",
    "            if df_og_members.empty: continue\n",
    "\n",
    "            num_members = len(df_og_members)\n",
    "            group_name = df_og_members[group_col].iloc[0]\n",
    "            \n",
    "            apsi_value_series = df_og_summary[df_og_summary[orthogroup_col] == og_id][apsi_col]\n",
    "            apsi_value = apsi_value_series.iloc[0] * 100 if not apsi_value_series.empty else np.nan\n",
    "            \n",
    "            percent_dark = (df_og_members[structurally_dark_col].sum() / num_members) * 100 if num_members > 0 else 0.0\n",
    "            \n",
    "            diversity_entry = df_diversity_subset[df_diversity_subset[orthogroup_col] == og_id]\n",
    "            shannon_entropy = diversity_entry['Shannon_Entropy'].iloc[0] if not diversity_entry.empty and 'Shannon_Entropy' in diversity_entry.columns else np.nan\n",
    "            observed_richness = diversity_entry['Observed_Richness'].iloc[0] if not diversity_entry.empty and 'Observed_Richness' in diversity_entry.columns else np.nan\n",
    "            \n",
    "            plot_data_list.append({\n",
    "                orthogroup_col: og_id,\n",
    "                'Ref_ProteinID': ref_row['Ref_ProteinID'],\n",
    "                'Ref_Function': ref_row['Ref_Function'],\n",
    "                'APSI (%)': apsi_value,\n",
    "                '% Structurally Dark': percent_dark,\n",
    "                'Shannon_Entropy': shannon_entropy,\n",
    "                'Observed_Richness': observed_richness,\n",
    "                'Total_Members': num_members,\n",
    "                'Group': group_name,\n",
    "                # Create a combined label for x-axis\n",
    "                'OG_Label': f\"{og_id}<br>({ref_row['Ref_Function']})\"\n",
    "            })\n",
    "\n",
    "        df_plot_data = pd.DataFrame(plot_data_list)\n",
    "\n",
    "        # --- 4. Create the Faceted Figure ---\n",
    "        if not df_plot_data.empty:\n",
    "            num_case_studies = len(df_plot_data)\n",
    "            \n",
    "            # Assign colors to OGs\n",
    "            og_color_palette = arcadia_primary_palette + arcadia_secondary_palette\n",
    "            og_color_map = {\n",
    "                og: og_color_palette[i % len(og_color_palette)] \n",
    "                for i, og in enumerate(df_plot_data[orthogroup_col].unique())\n",
    "            }\n",
    "\n",
    "            metrics_to_plot = ['APSI (%)', '% Structurally Dark', 'Shannon_Entropy', 'Observed_Richness']\n",
    "            metric_titles = {\n",
    "                'APSI (%)': 'Intra-OG APSI (%)',\n",
    "                '% Structurally Dark': '% Structurally Dark Members',\n",
    "                'Shannon_Entropy': 'Shannon Entropy (Tree-based)',\n",
    "                'Observed_Richness': 'Observed Richness (Tree Tips)'\n",
    "            }\n",
    "            \n",
    "            fig = make_subplots(\n",
    "                rows=len(metrics_to_plot), \n",
    "                cols=1,\n",
    "                subplot_titles=[metric_titles[metric] for metric in metrics_to_plot],\n",
    "                shared_xaxes=True, # Share x-axis (OGs)\n",
    "                vertical_spacing=0.1 \n",
    "            )\n",
    "\n",
    "            for i, metric in enumerate(metrics_to_plot):\n",
    "                row_num = i + 1\n",
    "                for og_idx, og_data_row in df_plot_data.iterrows():\n",
    "                    fig.add_trace(go.Bar(\n",
    "                        x=[og_data_row['OG_Label']], # Category on x-axis\n",
    "                        y=[og_data_row[metric] if not pd.isna(og_data_row[metric]) else 0],\n",
    "                        name=og_data_row[orthogroup_col], # Legend entry for OG\n",
    "                        legendgroup=og_data_row[orthogroup_col], # Group bars by OG for consistent color\n",
    "                        marker_color=og_color_map.get(og_data_row[orthogroup_col]),\n",
    "                        showlegend=(row_num == 1), # Show legend only for the first subplot\n",
    "                        text=[f\"{og_data_row[metric]:.1f}\" if not pd.isna(og_data_row[metric]) and isinstance(og_data_row[metric], float) and metric != 'Observed_Richness' \n",
    "                              else (f\"{int(og_data_row[metric])}\" if not pd.isna(og_data_row[metric]) and metric == 'Observed_Richness' else \"N/A\")],\n",
    "                        textposition='outside'\n",
    "                    ), row=row_num, col=1)\n",
    "                \n",
    "                fig.update_yaxes(title_text=\"\", row=row_num, col=1, showgrid=False, zeroline=False, linecolor='black', linewidth=1.5, ticks=\"outside\", ticklen=5, tickwidth=1.5, tickcolor='black')\n",
    "                if metric == 'APSI (%)' or metric == '% Structurally Dark':\n",
    "                    fig.update_yaxes(range=[0, 105], row=row_num, col=1)\n",
    "                \n",
    "                # Apply default axis title font from plotly_layout_defaults\n",
    "                if 'plotly_layout_defaults' in locals():\n",
    "                    default_font_family = plotly_layout_defaults.font.family if plotly_layout_defaults.font else 'Arial'\n",
    "                    default_font_size_axis_title = plotly_layout_defaults.xaxis.title.font.size if plotly_layout_defaults.xaxis and plotly_layout_defaults.xaxis.title and plotly_layout_defaults.xaxis.title.font else 12\n",
    "                    fig.layout[f'yaxis{row_num}'].title.font.update(size=default_font_size_axis_title, family=default_font_family)\n",
    "\n",
    "\n",
    "            fig.update_xaxes(\n",
    "                showgrid=False, zeroline=False, linecolor='black', linewidth=1.5, \n",
    "                ticks=\"outside\", ticklen=5, tickwidth=1.5, tickcolor='black',\n",
    "                tickangle=0 # Keep labels horizontal if possible, or adjust if too crowded\n",
    "            )\n",
    "            # Update subplot title fonts\n",
    "            for annotation in fig.layout.annotations:\n",
    "                annotation.font.size = 14\n",
    "                if 'plotly_layout_defaults' in locals() and plotly_layout_defaults.font:\n",
    "                    annotation.font.family = plotly_layout_defaults.font.family\n",
    "\n",
    "\n",
    "            fig.update_layout(\n",
    "                height=250 * len(metrics_to_plot), \n",
    "                # title_text=\"Case Study Orthogroup Metrics\", # Main title\n",
    "                plot_bgcolor='rgba(0,0,0,0)', \n",
    "                paper_bgcolor='rgba(0,0,0,0)',\n",
    "                margin=dict(l=50, r=50, t=80, b=150), # Adjust bottom margin for labels\n",
    "                barmode='group', # Group bars for each OG side-by-side if multiple metrics were on same plot\n",
    "                legend_title_text='Orthogroup'\n",
    "            )\n",
    "            fig.show()\n",
    "\n",
    "            # --- Save Figure and Summary Data ---\n",
    "            fig_path_html = Path(output_figure_dir) / \"figure8_case_studies_faceted.html\"\n",
    "            fig.write_html(str(fig_path_html))\n",
    "            print(f\"Figure 8 (Faceted Case Studies) HTML saved to: {fig_path_html}\")\n",
    "            try:\n",
    "                fig_path_pdf = Path(output_figure_dir) / \"figure8_case_studies_faceted.pdf\"\n",
    "                fig.write_image(str(fig_path_pdf), width=max(800, 120*num_case_studies), height=250*len(metrics_to_plot))\n",
    "                print(f\"Figure 8 (Faceted Case Studies) PDF saved to: {fig_path_pdf}\")\n",
    "            except Exception as e: print(f\"Warning: Could not export Figure 8 to PDF. Error: {e}\")\n",
    "            try:\n",
    "                fig_path_svg = Path(output_figure_dir) / \"figure8_case_studies_faceted.svg\"\n",
    "                fig.write_image(str(fig_path_svg), width=max(800, 120*num_case_studies), height=250*len(metrics_to_plot))\n",
    "                print(f\"Figure 8 (Faceted Case Studies) SVG saved to: {fig_path_svg}\")\n",
    "            except Exception as e: print(f\"Warning: Could not export Figure 8 to SVG. Error: {e}\")\n",
    "\n",
    "            summary_path = Path(output_summary_dir_phase1) / \"figure8_case_studies_faceted_data.csv\"\n",
    "            try:\n",
    "                df_plot_data.to_csv(summary_path, index=False)\n",
    "                print(f\"Case study summary data saved to: {summary_path}\")\n",
    "            except Exception as e: print(f\"Error saving case study summary data: {e}\")\n",
    "        else:\n",
    "            print(\"No data processed for plotting case studies after gathering all metrics.\")\n",
    "\n",
    "print(\"\\n\\n--- Cell 12 (Figure 8 Faceted Case Studies) Complete ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999d68f7-906b-4c65-9f97-d0b2b12a4d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Figure 9 - Overview of Orthogroup Diversity Metrics\n",
    "\n",
    "# --- Imports & Setup Assumptions ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path\n",
    "\n",
    "# This cell assumes that pandas (pd), numpy (np), plotly.express (px),\n",
    "# plotly.graph_objects (go), Path from pathlib,\n",
    "# and all necessary variables/helper functions from Cell 1 (Setup) are available.\n",
    "# Key variables from Cell 1 & subsequent cells:\n",
    "# - df_og_summary: DataFrame created in Cell 9, containing OG-level summaries (Orthogroup, Intra_OG_APSI, Group).\n",
    "# - output_figure_dir, output_summary_dir_phase1: Directories to save plots/data.\n",
    "# - plotly_layout_defaults: Default layout for Plotly figures.\n",
    "# - arcadia_primary_palette, group_colors: Color palettes/maps.\n",
    "# - orthogroup_col, apsi_col ('Intra_OG_APSI'), group_col\n",
    "\n",
    "print(\"\\n\\n--- Generating Figure 9: Overview of Orthogroup Diversity Metrics (Cell 13) ---\")\n",
    "\n",
    "# --- Configuration ---\n",
    "diversity_metrics_path = \"orthogroup_diversity_metrics.csv\" # Path to the uploaded diversity file\n",
    "\n",
    "# --- Check if necessary DataFrames are available ---\n",
    "error_found = False\n",
    "if 'df_og_summary' not in locals() or df_og_summary.empty:\n",
    "    print(f\"ERROR: DataFrame 'df_og_summary' (with OG-level APSI and Group) not found or is empty. Please run Cell 9 first.\")\n",
    "    error_found = True\n",
    "elif not all(col in df_og_summary.columns for col in [orthogroup_col, apsi_col, group_col]):\n",
    "    missing_cols = [col for col in [orthogroup_col, apsi_col, group_col] if col not in df_og_summary.columns]\n",
    "    print(f\"ERROR: 'df_og_summary' is missing required columns: {missing_cols}.\")\n",
    "    error_found = True\n",
    "\n",
    "if error_found:\n",
    "    print(\"Cannot proceed with diversity overview generation due to missing base data.\")\n",
    "    # Create an empty df_merged_diversity to prevent further errors in this cell if it's used later\n",
    "    df_merged_diversity = pd.DataFrame()\n",
    "else:\n",
    "    # --- 1. Load Diversity Metrics ---\n",
    "    print(f\"\\n--- Loading Orthogroup Diversity Metrics from: {diversity_metrics_path} ---\")\n",
    "    try:\n",
    "        df_diversity = pd.read_csv(diversity_metrics_path)\n",
    "        if 'OG_ID' in df_diversity.columns and orthogroup_col not in df_diversity.columns:\n",
    "            df_diversity = df_diversity.rename(columns={'OG_ID': orthogroup_col})\n",
    "        \n",
    "        # Select and rename relevant diversity columns\n",
    "        diversity_cols_to_use = {\n",
    "            orthogroup_col: orthogroup_col,\n",
    "            'Tree_n_tips': 'Observed_Richness',\n",
    "            'Tree_entropy': 'Shannon_Entropy'\n",
    "        }\n",
    "        actual_diversity_cols = {k: v for k, v in diversity_cols_to_use.items() if k in df_diversity.columns}\n",
    "        df_diversity_subset = df_diversity[list(actual_diversity_cols.keys())].rename(columns=actual_diversity_cols).copy()\n",
    "        \n",
    "        # Ensure orthogroup_col is string for merging\n",
    "        if orthogroup_col in df_diversity_subset.columns:\n",
    "            df_diversity_subset[orthogroup_col] = df_diversity_subset[orthogroup_col].astype(str)\n",
    "            \n",
    "        print(f\"Loaded and processed diversity metrics for {len(df_diversity_subset)} orthogroups.\")\n",
    "        if df_diversity_subset.empty: print(\"WARNING: Diversity metrics DataFrame is empty.\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Diversity metrics file '{diversity_metrics_path}' not found. Diversity metrics will be missing.\")\n",
    "        df_diversity_subset = pd.DataFrame(columns=[orthogroup_col, 'Observed_Richness', 'Shannon_Entropy'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or processing diversity metrics file: {e}\")\n",
    "        df_diversity_subset = pd.DataFrame(columns=[orthogroup_col, 'Observed_Richness', 'Shannon_Entropy'])\n",
    "\n",
    "    # --- 2. Merge Diversity Metrics with df_og_summary (contains APSI and Group) ---\n",
    "    print(\"\\n--- Merging Diversity Metrics with OG Summary Data (APSI, Group) ---\")\n",
    "    if not df_diversity_subset.empty and orthogroup_col in df_diversity_subset.columns:\n",
    "        # Ensure orthogroup_col in df_og_summary is also string\n",
    "        df_og_summary[orthogroup_col] = df_og_summary[orthogroup_col].astype(str)\n",
    "        \n",
    "        df_merged_diversity = pd.merge(df_og_summary, df_diversity_subset, on=orthogroup_col, how='inner')\n",
    "        # Using 'inner' merge to only keep OGs present in both (i.e., those with diversity metrics and APSI/Group)\n",
    "        print(f\"Merged diversity data. Resulting DataFrame shape: {df_merged_diversity.shape}\")\n",
    "        if df_merged_diversity.empty:\n",
    "            print(\"WARNING: Merged diversity DataFrame is empty. No common OGs found or one of the DFs was empty.\")\n",
    "    else:\n",
    "        print(\"Skipping merge as diversity data is empty or missing orthogroup column.\")\n",
    "        df_merged_diversity = pd.DataFrame() # Ensure it's defined as empty\n",
    "\n",
    "    # --- 3. Generate Plots ---\n",
    "    if not df_merged_diversity.empty:\n",
    "        \n",
    "        # A. Distribution of Shannon Entropy\n",
    "        if 'Shannon_Entropy' in df_merged_diversity.columns:\n",
    "            print(\"\\n--- Plotting Distribution of Shannon Entropy ---\")\n",
    "            fig_shannon = px.histogram(\n",
    "                df_merged_diversity.dropna(subset=['Shannon_Entropy']), \n",
    "                x='Shannon_Entropy', \n",
    "                color=group_col,\n",
    "                marginal=\"box\", # or \"rug\", \"violin\"\n",
    "                barmode='overlay',\n",
    "                # title=\"Distribution of Shannon Entropy per Orthogroup\", # No title\n",
    "                labels={'Shannon_Entropy': 'Shannon Entropy (Tree-based)'},\n",
    "                color_discrete_map=group_colors # from Cell 1\n",
    "            )\n",
    "            fig_shannon.update_layout(plotly_layout_defaults)\n",
    "            fig_shannon.update_xaxes(title_text='Shannon Entropy (Tree-based)', showgrid=False)\n",
    "            fig_shannon.update_yaxes(title_text='Number of Orthogroups', showgrid=False)\n",
    "            fig_shannon.update_traces(opacity=0.75)\n",
    "            fig_shannon.show()\n",
    "            # Save plot\n",
    "            fig_shannon_path_html = Path(output_figure_dir) / \"figure9a_shannon_entropy_distribution.html\"\n",
    "            fig_shannon.write_html(str(fig_shannon_path_html))\n",
    "            print(f\"Figure 9A (Shannon Entropy) HTML saved to: {fig_shannon_path_html}\")\n",
    "            try:\n",
    "                fig_shannon_path_pdf = Path(output_figure_dir) / \"figure9a_shannon_entropy_distribution.pdf\"\n",
    "                fig_shannon.write_image(str(fig_shannon_path_pdf))\n",
    "            except Exception as e: print(f\"PDF export error for Shannon Entropy: {e}\")\n",
    "            try:\n",
    "                fig_shannon_path_svg = Path(output_figure_dir) / \"figure9a_shannon_entropy_distribution.svg\"\n",
    "                fig_shannon.write_image(str(fig_shannon_path_svg))\n",
    "            except Exception as e: print(f\"SVG export error for Shannon Entropy: {e}\")\n",
    "        else:\n",
    "            print(\"Skipping Shannon Entropy distribution: column not found.\")\n",
    "\n",
    "        # B. Distribution of Observed Richness\n",
    "        if 'Observed_Richness' in df_merged_diversity.columns:\n",
    "            print(\"\\n--- Plotting Distribution of Observed Richness (Tree Tips) ---\")\n",
    "            fig_richness = px.histogram(\n",
    "                df_merged_diversity.dropna(subset=['Observed_Richness']), \n",
    "                x='Observed_Richness', \n",
    "                color=group_col,\n",
    "                marginal=\"box\",\n",
    "                barmode='overlay',\n",
    "                # title=\"Distribution of Observed Richness (Tree Tips) per Orthogroup\", # No title\n",
    "                labels={'Observed_Richness': 'Observed Richness (Number of Tree Tips)'},\n",
    "                color_discrete_map=group_colors\n",
    "            )\n",
    "            fig_richness.update_layout(plotly_layout_defaults)\n",
    "            fig_richness.update_xaxes(title_text='Observed Richness (Number of Tree Tips)', showgrid=False) # Consider log scale if highly skewed: type=\"log\"\n",
    "            fig_richness.update_yaxes(title_text='Number of Orthogroups', showgrid=False)\n",
    "            fig_richness.update_traces(opacity=0.75)\n",
    "            fig_richness.show()\n",
    "            # Save plot\n",
    "            fig_richness_path_html = Path(output_figure_dir) / \"figure9b_observed_richness_distribution.html\"\n",
    "            fig_richness.write_html(str(fig_richness_path_html))\n",
    "            print(f\"Figure 9B (Observed Richness) HTML saved to: {fig_richness_path_html}\")\n",
    "            try:\n",
    "                fig_richness_path_pdf = Path(output_figure_dir) / \"figure9b_observed_richness_distribution.pdf\"\n",
    "                fig_richness.write_image(str(fig_richness_path_pdf))\n",
    "            except Exception as e: print(f\"PDF export error for Richness: {e}\")\n",
    "            try:\n",
    "                fig_richness_path_svg = Path(output_figure_dir) / \"figure9b_observed_richness_distribution.svg\"\n",
    "                fig_richness.write_image(str(fig_richness_path_svg))\n",
    "            except Exception as e: print(f\"SVG export error for Richness: {e}\")\n",
    "        else:\n",
    "            print(\"Skipping Observed Richness distribution: column not found.\")\n",
    "\n",
    "        # C. Shannon Entropy vs. Observed Richness\n",
    "        if 'Shannon_Entropy' in df_merged_diversity.columns and 'Observed_Richness' in df_merged_diversity.columns:\n",
    "            print(\"\\n--- Plotting Shannon Entropy vs. Observed Richness ---\")\n",
    "            fig_ent_vs_rich = px.scatter(\n",
    "                df_merged_diversity.dropna(subset=['Shannon_Entropy', 'Observed_Richness']),\n",
    "                x='Observed_Richness',\n",
    "                y='Shannon_Entropy',\n",
    "                color=group_col,\n",
    "                # title='Shannon Entropy vs. Observed Richness per Orthogroup', # No title\n",
    "                labels={'Observed_Richness': 'Observed Richness (Tree Tips)', 'Shannon_Entropy': 'Shannon Entropy (Tree-based)'},\n",
    "                color_discrete_map=group_colors,\n",
    "                opacity=0.7,\n",
    "                trendline=\"ols\", # Ordinary Least Squares trendline\n",
    "                trendline_scope=\"overall\" # or \"trace\" for per-group trendlines\n",
    "            )\n",
    "            fig_ent_vs_rich.update_layout(plotly_layout_defaults)\n",
    "            fig_ent_vs_rich.update_xaxes(title_text='Observed Richness (Tree Tips)', showgrid=False) # Consider log scale: type=\"log\"\n",
    "            fig_ent_vs_rich.update_yaxes(title_text='Shannon Entropy (Tree-based)', showgrid=False)\n",
    "            fig_ent_vs_rich.show()\n",
    "            # Save plot\n",
    "            fig_ent_rich_path_html = Path(output_figure_dir) / \"figure9c_entropy_vs_richness.html\"\n",
    "            fig_ent_vs_rich.write_html(str(fig_ent_rich_path_html))\n",
    "            print(f\"Figure 9C (Entropy vs Richness) HTML saved to: {fig_ent_rich_path_html}\")\n",
    "            try:\n",
    "                fig_ent_rich_path_pdf = Path(output_figure_dir) / \"figure9c_entropy_vs_richness.pdf\"\n",
    "                fig_ent_vs_rich.write_image(str(fig_ent_rich_path_pdf))\n",
    "            except Exception as e: print(f\"PDF export error for Entropy vs Richness: {e}\")\n",
    "            try:\n",
    "                fig_ent_rich_path_svg = Path(output_figure_dir) / \"figure9c_entropy_vs_richness.svg\"\n",
    "                fig_ent_vs_rich.write_image(str(fig_ent_rich_path_svg))\n",
    "            except Exception as e: print(f\"SVG export error for Entropy vs Richness: {e}\")\n",
    "        else:\n",
    "            print(\"Skipping Shannon Entropy vs. Observed Richness plot: columns not found.\")\n",
    "\n",
    "        # D. Shannon Entropy vs. APSI\n",
    "        if 'Shannon_Entropy' in df_merged_diversity.columns and apsi_col in df_merged_diversity.columns:\n",
    "            print(\"\\n--- Plotting Shannon Entropy vs. APSI ---\")\n",
    "            # Convert APSI to percentage for plotting if it's not already\n",
    "            df_merged_diversity['APSI_Percent'] = df_merged_diversity[apsi_col] * 100\n",
    "            \n",
    "            fig_ent_vs_apsi = px.scatter(\n",
    "                df_merged_diversity.dropna(subset=['Shannon_Entropy', 'APSI_Percent']),\n",
    "                x='APSI_Percent',\n",
    "                y='Shannon_Entropy',\n",
    "                color=group_col,\n",
    "                # title='Shannon Entropy vs. Intra-OG APSI', # No title\n",
    "                labels={'APSI_Percent': 'Intra-OG APSI (%)', 'Shannon_Entropy': 'Shannon Entropy (Tree-based)'},\n",
    "                color_discrete_map=group_colors,\n",
    "                opacity=0.7,\n",
    "                trendline=\"ols\",\n",
    "                trendline_scope=\"overall\"\n",
    "            )\n",
    "            fig_ent_vs_apsi.update_layout(plotly_layout_defaults)\n",
    "            fig_ent_vs_apsi.update_xaxes(title_text='Intra-OG APSI (%)', showgrid=False)\n",
    "            fig_ent_vs_apsi.update_yaxes(title_text='Shannon Entropy (Tree-based)', showgrid=False)\n",
    "            fig_ent_vs_apsi.show()\n",
    "            # Save plot\n",
    "            fig_ent_apsi_path_html = Path(output_figure_dir) / \"figure9d_entropy_vs_apsi.html\"\n",
    "            fig_ent_vs_apsi.write_html(str(fig_ent_apsi_path_html))\n",
    "            print(f\"Figure 9D (Entropy vs APSI) HTML saved to: {fig_ent_apsi_path_html}\")\n",
    "            try:\n",
    "                fig_ent_apsi_path_pdf = Path(output_figure_dir) / \"figure9d_entropy_vs_apsi.pdf\"\n",
    "                fig_ent_vs_apsi.write_image(str(fig_ent_apsi_path_pdf))\n",
    "            except Exception as e: print(f\"PDF export error for Entropy vs APSI: {e}\")\n",
    "            try:\n",
    "                fig_ent_apsi_path_svg = Path(output_figure_dir) / \"figure9d_entropy_vs_apsi.svg\"\n",
    "                fig_ent_vs_apsi.write_image(str(fig_ent_apsi_path_svg))\n",
    "            except Exception as e: print(f\"SVG export error for Entropy vs APSI: {e}\")\n",
    "        else:\n",
    "            print(\"Skipping Shannon Entropy vs. APSI plot: columns not found.\")\n",
    "\n",
    "        # E. Observed Richness vs. APSI\n",
    "        if 'Observed_Richness' in df_merged_diversity.columns and apsi_col in df_merged_diversity.columns:\n",
    "            print(\"\\n--- Plotting Observed Richness vs. APSI ---\")\n",
    "            # APSI_Percent column should exist from previous plot\n",
    "            \n",
    "            fig_rich_vs_apsi = px.scatter(\n",
    "                df_merged_diversity.dropna(subset=['Observed_Richness', 'APSI_Percent']),\n",
    "                x='APSI_Percent',\n",
    "                y='Observed_Richness',\n",
    "                color=group_col,\n",
    "                # title='Observed Richness vs. Intra-OG APSI', # No title\n",
    "                labels={'APSI_Percent': 'Intra-OG APSI (%)', 'Observed_Richness': 'Observed Richness (Tree Tips)'},\n",
    "                color_discrete_map=group_colors,\n",
    "                opacity=0.7,\n",
    "                trendline=\"ols\",\n",
    "                trendline_scope=\"overall\"\n",
    "            )\n",
    "            fig_rich_vs_apsi.update_layout(plotly_layout_defaults)\n",
    "            fig_rich_vs_apsi.update_xaxes(title_text='Intra-OG APSI (%)', showgrid=False)\n",
    "            fig_rich_vs_apsi.update_yaxes(title_text='Observed Richness (Tree Tips)', showgrid=False) # Consider log scale: type=\"log\"\n",
    "            fig_rich_vs_apsi.show()\n",
    "            # Save plot\n",
    "            fig_rich_apsi_path_html = Path(output_figure_dir) / \"figure9e_richness_vs_apsi.html\"\n",
    "            fig_rich_vs_apsi.write_html(str(fig_rich_apsi_path_html))\n",
    "            print(f\"Figure 9E (Richness vs APSI) HTML saved to: {fig_rich_apsi_path_html}\")\n",
    "            try:\n",
    "                fig_rich_apsi_path_pdf = Path(output_figure_dir) / \"figure9e_richness_vs_apsi.pdf\"\n",
    "                fig_rich_vs_apsi.write_image(str(fig_rich_apsi_path_pdf))\n",
    "            except Exception as e: print(f\"PDF export error for Richness vs APSI: {e}\")\n",
    "            try:\n",
    "                fig_rich_apsi_path_svg = Path(output_figure_dir) / \"figure9e_richness_vs_apsi.svg\"\n",
    "                fig_rich_vs_apsi.write_image(str(fig_rich_apsi_path_svg))\n",
    "            except Exception as e: print(f\"SVG export error for Richness vs APSI: {e}\")\n",
    "        else:\n",
    "            print(\"Skipping Observed Richness vs. APSI plot: columns not found.\")\n",
    "            \n",
    "        # Save the merged diversity data\n",
    "        merged_diversity_summary_path = Path(output_summary_dir_phase1) / \"figure9_merged_diversity_and_apsi_data.csv\"\n",
    "        try:\n",
    "            df_merged_diversity.to_csv(merged_diversity_summary_path, index=False)\n",
    "            print(f\"\\nMerged diversity and APSI data saved to: {merged_diversity_summary_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving merged diversity data: {e}\")\n",
    "            \n",
    "    else:\n",
    "        print(\"Skipping Figure 9 plots as 'df_merged_diversity' is empty.\")\n",
    "\n",
    "print(\"\\n\\n--- Cell 13 (Figure 9 - Overview of Orthogroup Diversity Metrics) Complete ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925aa19f-2aeb-487a-957f-ca579ec467e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Figure 10 - Orthogroup Diversity Metrics by Broad Functional Category\n",
    "\n",
    "# --- Imports & Setup Assumptions ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path\n",
    "\n",
    "# This cell assumes that pandas (pd), numpy (np), plotly.express (px),\n",
    "# plotly.graph_objects (go), Path from pathlib,\n",
    "# and all necessary variables/helper functions from Cell 1 (Setup) are available.\n",
    "# Key variables from previous cells:\n",
    "# - df_merged_diversity: DataFrame from Cell 13 (contains OG, APSI, Group, Shannon_Entropy, Observed_Richness).\n",
    "# - df_full: Main DataFrame (used to get Broad_Functional_Category if not in df_merged_diversity).\n",
    "# - output_figure_dir, output_summary_dir_phase1: Directories to save plots/data.\n",
    "# - plotly_layout_defaults: Default layout for Plotly figures.\n",
    "# - group_colors, broad_category_color_map: Color maps.\n",
    "# - orthogroup_col, apsi_col ('Intra_OG_APSI'), group_col, broad_func_cat_col\n",
    "\n",
    "print(\"\\n\\n--- Generating Figure 10: Orthogroup Diversity Metrics by Functional Category (Cell 14) ---\")\n",
    "\n",
    "# --- Check if necessary DataFrames and columns are available ---\n",
    "error_found = False\n",
    "if 'df_merged_diversity' not in locals() or df_merged_diversity.empty:\n",
    "    print(f\"ERROR: DataFrame 'df_merged_diversity' not found or is empty. Please run Cell 13 first.\")\n",
    "    error_found = True\n",
    "elif not all(col in df_merged_diversity.columns for col in [orthogroup_col, apsi_col, group_col, 'Shannon_Entropy', 'Observed_Richness']):\n",
    "    missing_cols = [col for col in [orthogroup_col, apsi_col, group_col, 'Shannon_Entropy', 'Observed_Richness'] if col not in df_merged_diversity.columns]\n",
    "    print(f\"ERROR: 'df_merged_diversity' is missing required metrics columns: {missing_cols}.\")\n",
    "    error_found = True\n",
    "\n",
    "# Ensure Broad_Functional_Category is in df_merged_diversity\n",
    "# If not, try to merge it from an OG-level aggregation of df_full\n",
    "if not error_found and broad_func_cat_col not in df_merged_diversity.columns:\n",
    "    print(f\"'{broad_func_cat_col}' not found in 'df_merged_diversity'. Attempting to add it.\")\n",
    "    if 'df_full' in locals() and not df_full.empty and orthogroup_col in df_full.columns and broad_func_cat_col in df_full.columns:\n",
    "        # Determine the most frequent functional category for each OG\n",
    "        # Using .mode()[0] can be problematic if there are ties or empty groups.\n",
    "        # A safer approach might be to take the first non-null category, or a placeholder.\n",
    "        def get_dominant_category(series):\n",
    "            mode = series.mode()\n",
    "            return mode[0] if not mode.empty else 'Unknown/Unclassified'\n",
    "\n",
    "        df_og_func_cat = df_full.groupby(orthogroup_col)[broad_func_cat_col].apply(get_dominant_category).reset_index()\n",
    "        df_og_func_cat[orthogroup_col] = df_og_func_cat[orthogroup_col].astype(str) # Ensure type for merge\n",
    "\n",
    "        df_merged_diversity = pd.merge(df_merged_diversity, df_og_func_cat, on=orthogroup_col, how='left')\n",
    "        df_merged_diversity[broad_func_cat_col] = df_merged_diversity[broad_func_cat_col].fillna('Unknown/Unclassified')\n",
    "        print(f\"Added '{broad_func_cat_col}' to 'df_merged_diversity'.\")\n",
    "        if broad_func_cat_col not in df_merged_diversity.columns: # Check if merge failed somehow\n",
    "             print(f\"ERROR: Failed to add '{broad_func_cat_col}' to 'df_merged_diversity'.\")\n",
    "             error_found = True\n",
    "    else:\n",
    "        print(f\"ERROR: Cannot add '{broad_func_cat_col}'. 'df_full' is missing or lacks necessary columns.\")\n",
    "        error_found = True\n",
    "\n",
    "if error_found:\n",
    "    print(\"Cannot proceed with Figure 10 generation due to missing data or columns.\")\n",
    "else:\n",
    "    # --- Create Plots ---\n",
    "    metrics_to_plot_vs_func = {\n",
    "        apsi_col: 'Intra-OG APSI (%)', # APSI is already 0-1, will multiply by 100 for plotting\n",
    "        'Shannon_Entropy': 'Shannon Entropy (Tree-based)',\n",
    "        'Observed_Richness': 'Observed Richness (Tree Tips)'\n",
    "    }\n",
    "\n",
    "    # Filter out categories like \"Unknown/Unclassified\" or \"Other Specific Annotation\" for cleaner plots if desired\n",
    "    categories_to_exclude_plot = ['Unknown/Unclassified', 'Other Specific Annotation', 'General Protein Features'] # User can adjust\n",
    "    df_plot_func = df_merged_diversity[~df_merged_diversity[broad_func_cat_col].isin(categories_to_exclude_plot)].copy()\n",
    "    if df_plot_func.empty:\n",
    "        print(f\"WARNING: No data remains after excluding categories: {categories_to_exclude_plot}. Using all data.\")\n",
    "        df_plot_func = df_merged_diversity.copy()\n",
    "\n",
    "\n",
    "    for metric_col, metric_label in metrics_to_plot_vs_func.items():\n",
    "        if metric_col not in df_plot_func.columns:\n",
    "            print(f\"Skipping plot for {metric_label}: column '{metric_col}' not found.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n--- Plotting {metric_label} by Broad Functional Category ---\")\n",
    "        \n",
    "        # Prepare data for plot (e.g. scale APSI)\n",
    "        plot_df_metric = df_plot_func.dropna(subset=[metric_col, broad_func_cat_col, group_col]).copy()\n",
    "        if metric_col == apsi_col: # APSI is 0-1, convert to percentage\n",
    "            plot_df_metric[metric_col] = plot_df_metric[metric_col] * 100\n",
    "        \n",
    "        if plot_df_metric.empty:\n",
    "            print(f\"No data to plot for {metric_label} after dropping NaNs.\")\n",
    "            continue\n",
    "\n",
    "        fig = px.box(\n",
    "            plot_df_metric,\n",
    "            x=broad_func_cat_col,\n",
    "            y=metric_col,\n",
    "            color=group_col,\n",
    "            # title=f\"{metric_label} by Functional Category\", # No title\n",
    "            labels={metric_col: metric_label, broad_func_cat_col: \"Broad Functional Category\"},\n",
    "            color_discrete_map=group_colors, # from Cell 1\n",
    "            points=False # \"all\", False, \"outliers\"\n",
    "        )\n",
    "        fig.update_layout(plotly_layout_defaults)\n",
    "        fig.update_xaxes(\n",
    "            title_text=\"Broad Functional Category\", \n",
    "            showgrid=False, \n",
    "            tickangle=45,\n",
    "            categoryorder='total descending' # Order categories by median value of the metric (or count)\n",
    "        )\n",
    "        fig.update_yaxes(title_text=metric_label, showgrid=False)\n",
    "        if metric_col == 'Observed_Richness': # Richness can be highly skewed\n",
    "            fig.update_yaxes(type=\"log\", title_text=f\"{metric_label} (Log Scale)\")\n",
    "        \n",
    "        fig.show()\n",
    "\n",
    "        # Save plot\n",
    "        safe_metric_name = metric_col.lower().replace(' ', '_').replace('(', '').replace(')', '').replace('%', 'pct')\n",
    "        fig_path_html = Path(output_figure_dir) / f\"figure10_{safe_metric_name}_vs_function.html\"\n",
    "        fig.write_html(str(fig_path_html))\n",
    "        print(f\"Figure 10 ({metric_label}) HTML saved to: {fig_path_html}\")\n",
    "        try:\n",
    "            fig_path_pdf = Path(output_figure_dir) / f\"figure10_{safe_metric_name}_vs_function.pdf\"\n",
    "            fig.write_image(str(fig_path_pdf))\n",
    "        except Exception as e: print(f\"PDF export error for {metric_label} vs Function: {e}\")\n",
    "        try:\n",
    "            fig_path_svg = Path(output_figure_dir) / f\"figure10_{safe_metric_name}_vs_function.svg\"\n",
    "            fig.write_image(str(fig_path_svg))\n",
    "        except Exception as e: print(f\"SVG export error for {metric_label} vs Function: {e}\")\n",
    "\n",
    "    # Save the data used for these plots\n",
    "    func_diversity_summary_path = Path(output_summary_dir_phase1) / \"figure10_functional_category_diversity_metrics.csv\"\n",
    "    try:\n",
    "        # Save the df_plot_func which has the categories and metrics\n",
    "        df_plot_func.to_csv(func_diversity_summary_path, index=False)\n",
    "        print(f\"\\nSummary data for functional category diversity saved to: {func_diversity_summary_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving functional category diversity summary data: {e}\")\n",
    "\n",
    "print(\"\\n\\n--- Cell 14 (Figure 10 - OG Diversity vs. Functional Category) Complete ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c00041-0e7c-4cd3-b2fe-500c7bb2827f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Figure 11 - Highlighting Orthogroups with Extreme/Interesting Diversity Profiles (Expanded & Revised Criteria)\n",
    "\n",
    "# --- Imports & Setup Assumptions ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# This cell assumes that pandas (pd), numpy (np), Path from pathlib,\n",
    "# and all necessary variables/helper functions from Cell 1 (Setup) are available.\n",
    "# Key variables from previous cells:\n",
    "# - df_merged_diversity: DataFrame from Cell 13 (contains OG, APSI, Group, Shannon_Entropy, Observed_Richness, Broad_Functional_Category).\n",
    "# - df_full: Main DataFrame (used to get more detailed info like % dark members).\n",
    "# - output_summary_dir_phase1: Directory to save data.\n",
    "# - orthogroup_col, apsi_col, group_col, broad_func_cat_col, structurally_dark_col\n",
    "\n",
    "print(\"\\n\\n--- Generating Figure 11: Highlighting OGs with Extreme & Interesting Diversity Profiles (Expanded & Revised Criteria) (Cell 15) ---\")\n",
    "\n",
    "# --- Check if necessary DataFrames and columns are available ---\n",
    "error_found = False\n",
    "if 'df_merged_diversity' not in locals() or df_merged_diversity.empty:\n",
    "    print(f\"ERROR: DataFrame 'df_merged_diversity' not found or is empty. Please run Cell 13 (and Cell 14 to ensure func cat) first.\")\n",
    "    error_found = True\n",
    "elif 'df_full' not in locals() or df_full.empty:\n",
    "    print(f\"ERROR: DataFrame 'df_full' not found or is empty. Please run Cell 1 first.\")\n",
    "    error_found = True\n",
    "elif not all(col in df_merged_diversity.columns for col in [orthogroup_col, apsi_col, group_col, 'Shannon_Entropy', 'Observed_Richness', broad_func_cat_col]):\n",
    "    missing_cols = [col for col in [orthogroup_col, apsi_col, group_col, 'Shannon_Entropy', 'Observed_Richness', broad_func_cat_col] if col not in df_merged_diversity.columns]\n",
    "    print(f\"ERROR: 'df_merged_diversity' is missing required columns: {missing_cols}.\")\n",
    "    error_found = True\n",
    "\n",
    "if error_found:\n",
    "    print(\"Cannot proceed with highlighting extreme OGs due to missing data or columns.\")\n",
    "else:\n",
    "    # --- Define Criteria for \"Extreme\" OGs ---\n",
    "    n_percentile_strict = 5  # For single-metric extremes (e.g., top/bottom 5%)\n",
    "    # Use a more relaxed percentile for finding combined \"interesting\" profiles\n",
    "    # This increases the chance of finding OGs in the intersection of two conditions.\n",
    "    combo_percentile_relaxed = 25 # e.g., top/bottom 25% for combined profiles\n",
    "    \n",
    "    extreme_ogs_data = []\n",
    "    og_reasons_map = {} \n",
    "\n",
    "    def add_extreme_og_reason(og_id, reason, df_merged_diversity_row, df_full_og_members):\n",
    "        if og_id not in og_reasons_map:\n",
    "            og_reasons_map[og_id] = []\n",
    "        \n",
    "        if reason not in og_reasons_map[og_id]:\n",
    "            og_reasons_map[og_id].append(reason)\n",
    "        \n",
    "        if not any(d['Orthogroup'] == og_id for d in extreme_ogs_data):\n",
    "            percent_dark = np.nan\n",
    "            num_members = 0\n",
    "            if not df_full_og_members.empty:\n",
    "                num_members = len(df_full_og_members)\n",
    "                if structurally_dark_col in df_full_og_members.columns:\n",
    "                    percent_dark = (df_full_og_members[structurally_dark_col].sum() / num_members) * 100 if num_members > 0 else 0.0\n",
    "            \n",
    "            extreme_ogs_data.append({\n",
    "                'Orthogroup': og_id,\n",
    "                'Group': df_merged_diversity_row[group_col],\n",
    "                'APSI (%)': df_merged_diversity_row[apsi_col] * 100 if not pd.isna(df_merged_diversity_row[apsi_col]) else np.nan,\n",
    "                'Shannon_Entropy': df_merged_diversity_row['Shannon_Entropy'],\n",
    "                'Observed_Richness': df_merged_diversity_row['Observed_Richness'],\n",
    "                'Total_Members_in_OG_from_df_full': num_members,\n",
    "                'Percent_Structurally_Dark (%)': percent_dark,\n",
    "                'Broad_Functional_Category': df_merged_diversity_row[broad_func_cat_col]\n",
    "            })\n",
    "\n",
    "    # --- Identify OGs based on individual metrics (using n_percentile_strict) ---\n",
    "    # 1. Highest Shannon Entropy\n",
    "    if 'Shannon_Entropy' in df_merged_diversity.columns:\n",
    "        top_entropy_threshold = df_merged_diversity['Shannon_Entropy'].quantile(1 - n_percentile_strict/100)\n",
    "        df_target = df_merged_diversity[df_merged_diversity['Shannon_Entropy'] >= top_entropy_threshold]\n",
    "        print(f\"\\n--- OGs with Highest Shannon Entropy (Top {n_percentile_strict}%, Threshold >= {top_entropy_threshold:.2f}) ---\")\n",
    "        for index, row in df_target.iterrows():\n",
    "            og_members = df_full[df_full[orthogroup_col] == row[orthogroup_col]]\n",
    "            add_extreme_og_reason(row[orthogroup_col], f\"Top {n_percentile_strict}% Shannon Entropy\", row, og_members)\n",
    "    \n",
    "    # 2. Lowest Shannon Entropy\n",
    "    if 'Shannon_Entropy' in df_merged_diversity.columns:\n",
    "        bottom_entropy_threshold = df_merged_diversity['Shannon_Entropy'].quantile(n_percentile_strict/100)\n",
    "        df_target = df_merged_diversity[df_merged_diversity['Shannon_Entropy'] <= bottom_entropy_threshold]\n",
    "        print(f\"\\n--- OGs with Lowest Shannon Entropy (Bottom {n_percentile_strict}%, Threshold <= {bottom_entropy_threshold:.2f}) ---\")\n",
    "        for index, row in df_target.iterrows():\n",
    "            og_members = df_full[df_full[orthogroup_col] == row[orthogroup_col]]\n",
    "            add_extreme_og_reason(row[orthogroup_col], f\"Bottom {n_percentile_strict}% Shannon Entropy\", row, og_members)\n",
    "\n",
    "    # 3. Lowest APSI (Most Divergent)\n",
    "    if apsi_col in df_merged_diversity.columns:\n",
    "        bottom_apsi_threshold = df_merged_diversity[apsi_col].quantile(n_percentile_strict/100)\n",
    "        df_target = df_merged_diversity[df_merged_diversity[apsi_col] <= bottom_apsi_threshold]\n",
    "        print(f\"\\n--- OGs with Lowest APSI (Bottom {n_percentile_strict}%, Threshold <= {bottom_apsi_threshold*100:.1f}%) ---\")\n",
    "        for index, row in df_target.iterrows():\n",
    "            og_members = df_full[df_full[orthogroup_col] == row[orthogroup_col]]\n",
    "            add_extreme_og_reason(row[orthogroup_col], f\"Bottom {n_percentile_strict}% APSI\", row, og_members)\n",
    "\n",
    "    # 4. Highest APSI (Most Conserved)\n",
    "    if apsi_col in df_merged_diversity.columns:\n",
    "        top_apsi_threshold = df_merged_diversity[apsi_col].quantile(1 - n_percentile_strict/100)\n",
    "        df_target = df_merged_diversity[df_merged_diversity[apsi_col] >= top_apsi_threshold]\n",
    "        print(f\"\\n--- OGs with Highest APSI (Top {n_percentile_strict}%, Threshold >= {top_apsi_threshold*100:.1f}%) ---\")\n",
    "        for index, row in df_target.iterrows():\n",
    "            og_members = df_full[df_full[orthogroup_col] == row[orthogroup_col]]\n",
    "            add_extreme_og_reason(row[orthogroup_col], f\"Top {n_percentile_strict}% APSI\", row, og_members)\n",
    "\n",
    "    # 5. Highest Observed Richness\n",
    "    if 'Observed_Richness' in df_merged_diversity.columns:\n",
    "        top_richness_threshold = df_merged_diversity['Observed_Richness'].quantile(1 - n_percentile_strict/100)\n",
    "        df_target = df_merged_diversity[df_merged_diversity['Observed_Richness'] >= top_richness_threshold]\n",
    "        print(f\"\\n--- OGs with Highest Observed Richness (Top {n_percentile_strict}%, Threshold >= {int(top_richness_threshold)}) ---\")\n",
    "        for index, row in df_target.iterrows():\n",
    "            og_members = df_full[df_full[orthogroup_col] == row[orthogroup_col]]\n",
    "            add_extreme_og_reason(row[orthogroup_col], f\"Top {n_percentile_strict}% Observed Richness\", row, og_members)\n",
    "            \n",
    "    # 6. Lowest Observed Richness\n",
    "    if 'Observed_Richness' in df_merged_diversity.columns:\n",
    "        bottom_richness_threshold = df_merged_diversity['Observed_Richness'].quantile(n_percentile_strict/100)\n",
    "        min_obs_richness = df_merged_diversity['Observed_Richness'].min()\n",
    "        bottom_richness_threshold = max(bottom_richness_threshold, min_obs_richness)\n",
    "        df_target = df_merged_diversity[df_merged_diversity['Observed_Richness'] <= bottom_richness_threshold]\n",
    "        print(f\"\\n--- OGs with Lowest Observed Richness (Bottom {n_percentile_strict}%, Threshold <= {int(bottom_richness_threshold)}) ---\")\n",
    "        for index, row in df_target.iterrows():\n",
    "            og_members = df_full[df_full[orthogroup_col] == row[orthogroup_col]]\n",
    "            add_extreme_og_reason(row[orthogroup_col], f\"Bottom {n_percentile_strict}% Observed Richness\", row, og_members)\n",
    "\n",
    "    # --- Identify OGs based on combined metrics (using combo_percentile_relaxed) ---\n",
    "    # 7. Combination: High Entropy AND Low APSI (\"Classic\" Diversification)\n",
    "    if 'Shannon_Entropy' in df_merged_diversity.columns and apsi_col in df_merged_diversity.columns:\n",
    "        top_entropy_threshold = df_merged_diversity['Shannon_Entropy'].quantile(1 - combo_percentile_relaxed/100)\n",
    "        bottom_apsi_threshold = df_merged_diversity[apsi_col].quantile(combo_percentile_relaxed/100)\n",
    "        df_target = df_merged_diversity[\n",
    "            (df_merged_diversity['Shannon_Entropy'] >= top_entropy_threshold) &\n",
    "            (df_merged_diversity[apsi_col] <= bottom_apsi_threshold)\n",
    "        ]\n",
    "        print(f\"\\n--- OGs with High Entropy (Top {combo_percentile_relaxed}%) AND Low APSI (Bottom {combo_percentile_relaxed}%) ---\")\n",
    "        for index, row in df_target.iterrows():\n",
    "            og_members = df_full[df_full[orthogroup_col] == row[orthogroup_col]]\n",
    "            add_extreme_og_reason(row[orthogroup_col], f\"High Entropy & Low APSI\", row, og_members)\n",
    "\n",
    "    # 8. Combination (Surprising): High Entropy AND High APSI\n",
    "    if 'Shannon_Entropy' in df_merged_diversity.columns and apsi_col in df_merged_diversity.columns:\n",
    "        top_entropy_threshold = df_merged_diversity['Shannon_Entropy'].quantile(1 - combo_percentile_relaxed/100)\n",
    "        top_apsi_threshold = df_merged_diversity[apsi_col].quantile(1 - combo_percentile_relaxed/100)\n",
    "        df_target = df_merged_diversity[\n",
    "            (df_merged_diversity['Shannon_Entropy'] >= top_entropy_threshold) &\n",
    "            (df_merged_diversity[apsi_col] >= top_apsi_threshold)\n",
    "        ]\n",
    "        print(f\"\\n--- OGs with High Entropy (Top {combo_percentile_relaxed}%) AND High APSI (Top {combo_percentile_relaxed}%) ---\")\n",
    "        for index, row in df_target.iterrows():\n",
    "            og_members = df_full[df_full[orthogroup_col] == row[orthogroup_col]]\n",
    "            add_extreme_og_reason(row[orthogroup_col], f\"High Entropy & High APSI\", row, og_members)\n",
    "\n",
    "    # 9. Combination (Surprising): High Richness AND High APSI\n",
    "    if 'Observed_Richness' in df_merged_diversity.columns and apsi_col in df_merged_diversity.columns:\n",
    "        top_richness_threshold = df_merged_diversity['Observed_Richness'].quantile(1 - combo_percentile_relaxed/100)\n",
    "        top_apsi_threshold = df_merged_diversity[apsi_col].quantile(1 - combo_percentile_relaxed/100)\n",
    "        df_target = df_merged_diversity[\n",
    "            (df_merged_diversity['Observed_Richness'] >= top_richness_threshold) &\n",
    "            (df_merged_diversity[apsi_col] >= top_apsi_threshold)\n",
    "        ]\n",
    "        print(f\"\\n--- OGs with High Richness (Top {combo_percentile_relaxed}%) AND High APSI (Top {combo_percentile_relaxed}%) ---\")\n",
    "        for index, row in df_target.iterrows():\n",
    "            og_members = df_full[df_full[orthogroup_col] == row[orthogroup_col]]\n",
    "            add_extreme_og_reason(row[orthogroup_col], f\"High Richness & High APSI\", row, og_members)\n",
    "            \n",
    "    # 10. Combination: Low Entropy AND Low APSI\n",
    "    if 'Shannon_Entropy' in df_merged_diversity.columns and apsi_col in df_merged_diversity.columns:\n",
    "        bottom_entropy_threshold = df_merged_diversity['Shannon_Entropy'].quantile(combo_percentile_relaxed/100)\n",
    "        bottom_apsi_threshold = df_merged_diversity[apsi_col].quantile(combo_percentile_relaxed/100)\n",
    "        df_target = df_merged_diversity[\n",
    "            (df_merged_diversity['Shannon_Entropy'] <= bottom_entropy_threshold) &\n",
    "            (df_merged_diversity[apsi_col] <= bottom_apsi_threshold)\n",
    "        ]\n",
    "        print(f\"\\n--- OGs with Low Entropy (Bottom {combo_percentile_relaxed}%) AND Low APSI (Bottom {combo_percentile_relaxed}%) ---\")\n",
    "        for index, row in df_target.iterrows():\n",
    "            og_members = df_full[df_full[orthogroup_col] == row[orthogroup_col]]\n",
    "            add_extreme_og_reason(row[orthogroup_col], f\"Low Entropy & Low APSI\", row, og_members)\n",
    "\n",
    "    # --- Finalize and Display Table of Highlighted OGs ---\n",
    "    if extreme_ogs_data:\n",
    "        df_extreme_ogs_summary = pd.DataFrame(extreme_ogs_data)\n",
    "        df_extreme_ogs_summary['Reason_for_Highlight'] = df_extreme_ogs_summary['Orthogroup'].map(lambda og_id: \"; \".join(sorted(list(set(og_reasons_map.get(og_id, []))))))\n",
    "        \n",
    "        df_extreme_ogs_summary = df_extreme_ogs_summary.sort_values('Orthogroup').reset_index(drop=True)\n",
    "        \n",
    "        print(\"\\n\\n--- Summary Table: Highlighted Orthogroups with Extreme/Interesting Diversity Profiles ---\")\n",
    "        print(f\"Total unique OGs highlighted: {len(df_extreme_ogs_summary)}\")\n",
    "        try:\n",
    "            display_cols = ['Orthogroup', 'Group', 'Reason_for_Highlight', 'APSI (%)', 'Shannon_Entropy', \n",
    "                            'Observed_Richness', 'Total_Members_in_OG_from_df_full', \n",
    "                            'Percent_Structurally_Dark (%)', 'Broad_Functional_Category']\n",
    "            display_cols = [col for col in display_cols if col in df_extreme_ogs_summary.columns]\n",
    "            print(df_extreme_ogs_summary[display_cols].to_markdown(index=False, floatfmt=\".2f\"))\n",
    "        except ImportError:\n",
    "            print(df_extreme_ogs_summary[display_cols])\n",
    "\n",
    "        extreme_ogs_summary_path_revised = Path(output_summary_dir_phase1) / \"figure11_extreme_diversity_ogs_summary_expanded_revised.csv\"\n",
    "        try:\n",
    "            df_extreme_ogs_summary.to_csv(extreme_ogs_summary_path_revised, index=False, float_format='%.3f')\n",
    "            print(f\"\\nSummary table of extreme OGs (revised criteria) saved to: {extreme_ogs_summary_path_revised}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving revised extreme OGs summary table: {e}\")\n",
    "    else:\n",
    "        print(\"\\nNo OGs met the defined extreme/interesting criteria with the revised percentiles.\")\n",
    "\n",
    "print(\"\\n\\n--- Cell 15 (Figure 11 - Highlighting Extreme & Interesting OGs - Revised Criteria) Complete ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fcc6a3-17bf-43a2-b88b-14b6605d7020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Figure 12 - Functional Breakdown of OGs in Extreme/Interesting Diversity Profiles\n",
    "\n",
    "# --- Imports & Setup Assumptions ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from pathlib import Path\n",
    "\n",
    "# This cell assumes that pandas (pd), numpy (np), plotly.express (px),\n",
    "# plotly.graph_objects (go), Path from pathlib, make_subplots,\n",
    "# and all necessary variables/helper functions from Cell 1 (Setup) are available.\n",
    "# Key variables from previous cells:\n",
    "# - output_summary_dir_phase1: Directory where Cell 15 saved its summary.\n",
    "# - output_figure_dir: Directory to save plots.\n",
    "# - plotly_layout_defaults: Default layout for Plotly figures.\n",
    "# - broad_category_color_map: Color map for functional categories (from Cell 1).\n",
    "# - orthogroup_col, broad_func_cat_col\n",
    "\n",
    "print(\"\\n\\n--- Generating Figure 12: Functional Breakdown of Extreme Diversity OG Profiles (Cell 16) ---\")\n",
    "\n",
    "# --- Configuration ---\n",
    "# Path to the summary file created by Cell 15\n",
    "extreme_ogs_summary_path = Path(output_summary_dir_phase1) / \"figure11_extreme_diversity_ogs_summary_expanded_revised.csv\"\n",
    "\n",
    "# Define the \"Reason_for_Highlight\" profiles we want to analyze\n",
    "# These must exactly match the strings used in Cell 15 for combined categories\n",
    "target_profiles = [\n",
    "    \"High Entropy & Low APSI\",       # Classic Diversification\n",
    "    \"High Entropy & High APSI\",      # Phylogenetically Diverse, Sequence-Conserved\n",
    "    \"High Richness & High APSI\",     # Large OG, Sequence-Conserved\n",
    "    \"Low Entropy & Low APSI\"       # Phylogenetically Constrained, Sequence-Diverged\n",
    "]\n",
    "# Shorter names for plot titles\n",
    "profile_short_names = {\n",
    "    \"High Entropy & Low APSI\": \"High Entropy, Low APSI\",\n",
    "    \"High Entropy & High APSI\": \"High Entropy, High APSI\",\n",
    "    \"High Richness & High APSI\": \"High Richness, High APSI\",\n",
    "    \"Low Entropy & Low APSI\": \"Low Entropy, Low APSI\"\n",
    "}\n",
    "\n",
    "\n",
    "# --- Load Data from Cell 15 ---\n",
    "if not extreme_ogs_summary_path.is_file():\n",
    "    print(f\"ERROR: Extreme OGs summary file not found at '{extreme_ogs_summary_path}'. Please run Cell 15 first.\")\n",
    "    df_extreme_ogs_summary = pd.DataFrame() # Ensure it's defined\n",
    "else:\n",
    "    try:\n",
    "        df_extreme_ogs_summary = pd.read_csv(extreme_ogs_summary_path)\n",
    "        print(f\"Successfully loaded extreme OGs summary. Shape: {df_extreme_ogs_summary.shape}\")\n",
    "        if broad_func_cat_col not in df_extreme_ogs_summary.columns:\n",
    "            print(f\"ERROR: Column '{broad_func_cat_col}' not found in the loaded summary file.\")\n",
    "            df_extreme_ogs_summary = pd.DataFrame() # Make it unusable\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading extreme OGs summary file: {e}\")\n",
    "        df_extreme_ogs_summary = pd.DataFrame()\n",
    "\n",
    "# --- Generate Plots ---\n",
    "if not df_extreme_ogs_summary.empty:\n",
    "    num_profiles = len(target_profiles)\n",
    "    \n",
    "    # Determine subplot layout (e.g., 2x2 or 4x1)\n",
    "    if num_profiles <= 0:\n",
    "        print(\"No target profiles defined. Skipping plot generation.\")\n",
    "    else:\n",
    "        # For 4 profiles, 2x2 is good. If more, adjust.\n",
    "        n_cols = 2 if num_profiles > 2 else 1\n",
    "        n_rows = (num_profiles + n_cols - 1) // n_cols # Calculate rows needed\n",
    "\n",
    "        fig = make_subplots(\n",
    "            rows=n_rows, \n",
    "            cols=n_cols,\n",
    "            subplot_titles=[profile_short_names.get(p, p) for p in target_profiles],\n",
    "            vertical_spacing=0.15, # Adjust as needed\n",
    "            horizontal_spacing=0.1\n",
    "        )\n",
    "        \n",
    "        plot_data_for_summary_table = [] # To store data for a summary table\n",
    "\n",
    "        current_row = 1\n",
    "        current_col = 1\n",
    "\n",
    "        for profile_reason in target_profiles:\n",
    "            # Filter for OGs that have this specific reason (can be part of a semicolon-separated list)\n",
    "            df_profile_subset = df_extreme_ogs_summary[\n",
    "                df_extreme_ogs_summary['Reason_for_Highlight'].str.contains(profile_reason, regex=False, na=False)\n",
    "            ].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "            if df_profile_subset.empty:\n",
    "                print(f\"No OGs found for profile: '{profile_reason}'. Skipping its subplot.\")\n",
    "                # Advance subplot position if layout is fixed\n",
    "                if n_cols > 1:\n",
    "                    current_col += 1\n",
    "                    if current_col > n_cols:\n",
    "                        current_col = 1\n",
    "                        current_row += 1\n",
    "                else:\n",
    "                    current_row +=1\n",
    "                continue\n",
    "\n",
    "            # Calculate functional category percentages for this profile\n",
    "            func_cat_counts = df_profile_subset[broad_func_cat_col].value_counts(normalize=True).mul(100).reset_index()\n",
    "            func_cat_counts.columns = [broad_func_cat_col, 'Percentage']\n",
    "            \n",
    "            # Sort by percentage for better visualization\n",
    "            func_cat_counts = func_cat_counts.sort_values('Percentage', ascending=False)\n",
    "\n",
    "            # Store for summary table\n",
    "            for _, cat_row in func_cat_counts.iterrows():\n",
    "                plot_data_for_summary_table.append({\n",
    "                    'Profile': profile_short_names.get(profile_reason, profile_reason),\n",
    "                    broad_func_cat_col: cat_row[broad_func_cat_col],\n",
    "                    'Percentage': cat_row['Percentage']\n",
    "                })\n",
    "\n",
    "            # Add bar trace to subplot\n",
    "            fig.add_trace(go.Bar(\n",
    "                x=func_cat_counts[broad_func_cat_col],\n",
    "                y=func_cat_counts['Percentage'],\n",
    "                name=profile_short_names.get(profile_reason, profile_reason), # Legend for the profile\n",
    "                marker_color=[broad_category_color_map.get(cat, '#cccccc') for cat in func_cat_counts[broad_func_cat_col]],\n",
    "                text=func_cat_counts['Percentage'].apply(lambda x: f\"{x:.1f}%\"),\n",
    "                textposition='none' # Hide direct text on bars for now, hover will show\n",
    "            ), row=current_row, col=current_col)\n",
    "\n",
    "            fig.update_xaxes(title_text=\"\", tickangle=45, row=current_row, col=current_col, categoryorder='total descending', showgrid=False, zeroline=False, linecolor='black', linewidth=1.5, ticks=\"outside\", ticklen=5, tickwidth=1.5, tickcolor='black')\n",
    "            fig.update_yaxes(title_text=\"Percentage of OGs\", range=[0, max(50, func_cat_counts['Percentage'].max() * 1.1 if not func_cat_counts.empty else 10)], row=current_row, col=current_col, showgrid=False, zeroline=False, linecolor='black', linewidth=1.5, ticks=\"outside\", ticklen=5, tickwidth=1.5, tickcolor='black')\n",
    "\n",
    "            # Advance subplot position\n",
    "            if n_cols > 1:\n",
    "                current_col += 1\n",
    "                if current_col > n_cols:\n",
    "                    current_col = 1\n",
    "                    current_row += 1\n",
    "            else:\n",
    "                current_row +=1\n",
    "                \n",
    "        fig.update_layout(\n",
    "            height=max(400 * n_rows, 600), # Adjust height\n",
    "            # title_text=\"Functional Composition of Orthogroups in Extreme Diversity Profiles\", # Main title\n",
    "            showlegend=False, # Colors are by category, legend per bar not needed for this setup\n",
    "            plot_bgcolor='rgba(0,0,0,0)', \n",
    "            paper_bgcolor='rgba(0,0,0,0)',\n",
    "            margin=dict(l=60, r=40, t=100, b=150) # Adjust margins\n",
    "        )\n",
    "        if 'plotly_layout_defaults' in locals():\n",
    "            fig.update_layout(font=plotly_layout_defaults.font) # Apply global font\n",
    "            for i in range(1, len(fig.layout.annotations) + 1): # Subplot titles\n",
    "                 if fig.layout.annotations[i-1]: # Check if annotation exists\n",
    "                    fig.layout.annotations[i-1].font.size = 14\n",
    "                    if plotly_layout_defaults.font:\n",
    "                        fig.layout.annotations[i-1].font.family = plotly_layout_defaults.font.family\n",
    "        \n",
    "        fig.show()\n",
    "\n",
    "        # --- Save Figure and Summary Data ---\n",
    "        fig_path_html = Path(output_figure_dir) / \"figure12_extreme_profile_functions.html\"\n",
    "        fig.write_html(str(fig_path_html))\n",
    "        print(f\"Figure 12 (Functional Breakdown of Extreme Profiles) HTML saved to: {fig_path_html}\")\n",
    "        try:\n",
    "            fig_path_pdf = Path(output_figure_dir) / \"figure12_extreme_profile_functions.pdf\"\n",
    "            fig.write_image(str(fig_path_pdf), width=max(800, 400*n_cols), height=max(400*n_rows, 600))\n",
    "            print(f\"Figure 12 PDF saved to: {fig_path_pdf}\")\n",
    "        except Exception as e: print(f\"Warning: Could not export Figure 12 to PDF. Error: {e}\")\n",
    "        try:\n",
    "            fig_path_svg = Path(output_figure_dir) / \"figure12_extreme_profile_functions.svg\"\n",
    "            fig.write_image(str(fig_path_svg), width=max(800, 400*n_cols), height=max(400*n_rows, 600))\n",
    "            print(f\"Figure 12 SVG saved to: {fig_path_svg}\")\n",
    "        except Exception as e: print(f\"Warning: Could not export Figure 12 to SVG. Error: {e}\")\n",
    "\n",
    "        # Save the aggregated data used for the plots\n",
    "        if plot_data_for_summary_table:\n",
    "            df_plot_summary_table = pd.DataFrame(plot_data_for_summary_table)\n",
    "            summary_table_path = Path(output_summary_dir_phase1) / \"figure12_extreme_profile_functions_summary.csv\"\n",
    "            try:\n",
    "                df_plot_summary_table.to_csv(summary_table_path, index=False, float_format='%.2f')\n",
    "                print(f\"Summary data for Figure 12 plots saved to: {summary_table_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving summary data for Figure 12: {e}\")\n",
    "        else:\n",
    "            print(\"No data was generated for the plot summary table.\")\n",
    "else:\n",
    "    print(\"Skipping Figure 12 generation as the extreme OGs summary data is not available or empty.\")\n",
    "\n",
    "print(\"\\n\\n--- Cell 16 (Figure 12 - Functional Breakdown of Extreme Profiles) Complete ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438f67f0-d35f-423c-8101-89ba296a88fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Figure 9 (Part 2) - Correlation Analysis of Diversity Metrics\n",
    "\n",
    "# --- Imports & Setup Assumptions ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# This cell assumes that pandas (pd), numpy (np), plotly.express (px),\n",
    "# plotly.graph_objects (go), Path from pathlib, pearsonr, spearmanr\n",
    "# and all necessary variables/helper functions from Cell 1 (Setup) are available.\n",
    "# Key variables from previous cells:\n",
    "# - df_merged_diversity: DataFrame from Cell 13 (contains OG, APSI, Group, Shannon_Entropy, Observed_Richness).\n",
    "# - output_figure_dir, output_summary_dir_phase1: Directories to save plots/data.\n",
    "# - plotly_layout_defaults: Default layout for Plotly figures.\n",
    "# - group_colors: Color map for Asgard/GV.\n",
    "# - orthogroup_col, apsi_col ('Intra_OG_APSI'), group_col\n",
    "\n",
    "print(\"\\n\\n--- Generating Figure 9 (Part 2): Correlation Analysis of Diversity Metrics (Cell 17) ---\")\n",
    "\n",
    "# --- Check if necessary DataFrames and columns are available ---\n",
    "error_found = False\n",
    "if 'df_merged_diversity' not in locals() or df_merged_diversity.empty:\n",
    "    print(f\"ERROR: DataFrame 'df_merged_diversity' not found or is empty. Please run Cell 13 first.\")\n",
    "    error_found = True\n",
    "elif not all(col in df_merged_diversity.columns for col in [apsi_col, 'Shannon_Entropy', 'Observed_Richness', group_col]):\n",
    "    missing_cols = [col for col in [apsi_col, 'Shannon_Entropy', 'Observed_Richness', group_col] if col not in df_merged_diversity.columns]\n",
    "    print(f\"ERROR: 'df_merged_diversity' is missing required metrics/group columns: {missing_cols}.\")\n",
    "    error_found = True\n",
    "\n",
    "if error_found:\n",
    "    print(\"Cannot proceed with correlation analysis due to missing data or columns.\")\n",
    "    df_correlations_summary = pd.DataFrame() # Ensure it's defined\n",
    "else:\n",
    "    # --- Calculate Correlation Coefficients ---\n",
    "    correlation_results = []\n",
    "\n",
    "    metrics_for_corr = {\n",
    "        'APSI (%)': apsi_col, # Use original APSI (0-1) for calculation, label as %\n",
    "        'Shannon_Entropy': 'Shannon_Entropy',\n",
    "        'Observed_Richness': 'Observed_Richness'\n",
    "    }\n",
    "    \n",
    "    # Ensure APSI is scaled 0-1 for calculations if it was previously scaled to 0-100 for plotting\n",
    "    # If 'APSI_Percent' exists from Cell 13, use that and divide by 100, else use apsi_col directly\n",
    "    if 'APSI_Percent' in df_merged_diversity.columns:\n",
    "        df_merged_diversity_corr = df_merged_diversity.copy()\n",
    "        df_merged_diversity_corr[apsi_col] = df_merged_diversity_corr['APSI_Percent'] / 100\n",
    "    else:\n",
    "        df_merged_diversity_corr = df_merged_diversity.copy()\n",
    "\n",
    "\n",
    "    datasets_to_correlate = {\n",
    "        \"All OGs\": df_merged_diversity_corr,\n",
    "        \"Asgard OGs\": df_merged_diversity_corr[df_merged_diversity_corr[group_col] == 'Asgard'],\n",
    "        \"GV OGs\": df_merged_diversity_corr[df_merged_diversity_corr[group_col] == 'GV']\n",
    "    }\n",
    "\n",
    "    metric_pairs = [\n",
    "        ('APSI (%)', 'Shannon_Entropy'),\n",
    "        ('APSI (%)', 'Observed_Richness'),\n",
    "        ('Shannon_Entropy', 'Observed_Richness')\n",
    "    ]\n",
    "\n",
    "    for dataset_name, df_data in datasets_to_correlate.items():\n",
    "        if df_data.empty:\n",
    "            print(f\"Skipping correlations for {dataset_name}: DataFrame is empty.\")\n",
    "            continue\n",
    "        print(f\"\\nCalculating correlations for: {dataset_name} ({len(df_data)} OGs)\")\n",
    "        for m1_label, m2_label in metric_pairs:\n",
    "            m1_col = metrics_for_corr[m1_label]\n",
    "            m2_col = metrics_for_corr[m2_label]\n",
    "\n",
    "            if m1_col not in df_data.columns or m2_col not in df_data.columns:\n",
    "                print(f\"  Skipping {m1_label} vs {m2_label}: one or both columns missing.\")\n",
    "                continue\n",
    "            \n",
    "            # Drop NaNs for the specific pair of columns before calculating correlation\n",
    "            df_pair_data = df_data[[m1_col, m2_col]].dropna()\n",
    "            \n",
    "            if len(df_pair_data) < 3: # Need at least 3 data points for meaningful correlation\n",
    "                print(f\"  Skipping {m1_label} vs {m2_label}: Insufficient data points after dropping NaNs ({len(df_pair_data)}).\")\n",
    "                pearson_coef, pearson_p = np.nan, np.nan\n",
    "                spearman_coef, spearman_p = np.nan, np.nan\n",
    "            else:\n",
    "                pearson_coef, pearson_p = pearsonr(df_pair_data[m1_col], df_pair_data[m2_col])\n",
    "                spearman_coef, spearman_p = spearmanr(df_pair_data[m1_col], df_pair_data[m2_col])\n",
    "            \n",
    "            correlation_results.append({\n",
    "                'Dataset': dataset_name,\n",
    "                'Metric 1': m1_label,\n",
    "                'Metric 2': m2_label,\n",
    "                'N_Obs': len(df_pair_data),\n",
    "                'Pearson_Correlation': pearson_coef,\n",
    "                'Pearson_P_Value': pearson_p,\n",
    "                'Spearman_Correlation': spearman_coef,\n",
    "                'Spearman_P_Value': spearman_p\n",
    "            })\n",
    "\n",
    "    df_correlations_summary = pd.DataFrame(correlation_results)\n",
    "    \n",
    "    print(\"\\n\\n--- Correlation Coefficients Summary ---\")\n",
    "    if not df_correlations_summary.empty:\n",
    "        try:\n",
    "            print(df_correlations_summary.to_markdown(index=False, floatfmt=\".3f\"))\n",
    "        except ImportError:\n",
    "            print(df_correlations_summary)\n",
    "        \n",
    "        # Save the summary table\n",
    "        corr_summary_path = Path(output_summary_dir_phase1) / \"figure9_part2_correlation_summary.csv\"\n",
    "        try:\n",
    "            df_correlations_summary.to_csv(corr_summary_path, index=False, float_format='%.4f')\n",
    "            print(f\"\\nCorrelation summary table saved to: {corr_summary_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving correlation summary table: {e}\")\n",
    "    else:\n",
    "        print(\"No correlation results to display.\")\n",
    "\n",
    "    # --- Generate Scatter Plots with Trendlines (Re-using plots from Cell 13 / Fig 9 D & E logic) ---\n",
    "    # These plots visually support the correlation table.\n",
    "    \n",
    "    # Plot 1: Shannon Entropy vs. APSI\n",
    "    if 'Shannon_Entropy' in df_merged_diversity_corr.columns and apsi_col in df_merged_diversity_corr.columns:\n",
    "        print(\"\\n--- Plotting Shannon Entropy vs. APSI (with Trendlines) ---\")\n",
    "        df_merged_diversity_corr['APSI_Percent_Plot'] = df_merged_diversity_corr[apsi_col] * 100\n",
    "        \n",
    "        fig_ent_vs_apsi_corr = px.scatter(\n",
    "            df_merged_diversity_corr.dropna(subset=['Shannon_Entropy', 'APSI_Percent_Plot']),\n",
    "            x='APSI_Percent_Plot',\n",
    "            y='Shannon_Entropy',\n",
    "            color=group_col,\n",
    "            labels={'APSI_Percent_Plot': 'Intra-OG APSI (%)', 'Shannon_Entropy': 'Shannon Entropy (Tree-based)'},\n",
    "            color_discrete_map=group_colors,\n",
    "            opacity=0.6,\n",
    "            trendline=\"ols\", \n",
    "            trendline_scope=\"overall\", # Single trendline for all data\n",
    "            # To add per-group trendlines, you might need to plot traces separately or use facets\n",
    "        )\n",
    "        # Add per-group trendlines manually if desired for more detail\n",
    "        # Example for Asgard:\n",
    "        # df_asgard_corr = df_merged_diversity_corr[df_merged_diversity_corr[group_col] == 'Asgard'].dropna(subset=['Shannon_Entropy', 'APSI_Percent_Plot'])\n",
    "        # if len(df_asgard_corr) > 1:\n",
    "        #     results_asgard = px.scatter(df_asgard_corr, x='APSI_Percent_Plot', y='Shannon_Entropy', trendline=\"ols\").get_traces()[-1] # Get trendline trace\n",
    "        #     results_asgard.update(line_color=group_colors.get('Asgard', 'blue'), name='Asgard Trend')\n",
    "        #     fig_ent_vs_apsi_corr.add_trace(results_asgard)\n",
    "        # (Repeat for GV)\n",
    "\n",
    "        fig_ent_vs_apsi_corr.update_layout(plotly_layout_defaults)\n",
    "        fig_ent_vs_apsi_corr.update_xaxes(title_text='Intra-OG APSI (%)', showgrid=False)\n",
    "        fig_ent_vs_apsi_corr.update_yaxes(title_text='Shannon Entropy (Tree-based)', showgrid=False)\n",
    "        fig_ent_vs_apsi_corr.show()\n",
    "        # Save plot\n",
    "        fig_path_html = Path(output_figure_dir) / \"figure9_part2_entropy_vs_apsi_scatter.html\"\n",
    "        fig_ent_vs_apsi_corr.write_html(str(fig_path_html))\n",
    "        print(f\"Shannon Entropy vs APSI scatter plot HTML saved to: {fig_path_html}\")\n",
    "        # PDF/SVG saving as in previous cells\n",
    "    else:\n",
    "        print(\"Skipping Shannon Entropy vs. APSI scatter plot: columns not found.\")\n",
    "\n",
    "    # Plot 2: Observed Richness vs. APSI\n",
    "    if 'Observed_Richness' in df_merged_diversity_corr.columns and apsi_col in df_merged_diversity_corr.columns:\n",
    "        print(\"\\n--- Plotting Observed Richness vs. APSI (with Trendlines) ---\")\n",
    "        fig_rich_vs_apsi_corr = px.scatter(\n",
    "            df_merged_diversity_corr.dropna(subset=['Observed_Richness', 'APSI_Percent_Plot']),\n",
    "            x='APSI_Percent_Plot',\n",
    "            y='Observed_Richness',\n",
    "            color=group_col,\n",
    "            labels={'APSI_Percent_Plot': 'Intra-OG APSI (%)', 'Observed_Richness': 'Observed Richness (Tree Tips)'},\n",
    "            color_discrete_map=group_colors,\n",
    "            opacity=0.6,\n",
    "            trendline=\"ols\",\n",
    "            trendline_scope=\"overall\"\n",
    "        )\n",
    "        fig_rich_vs_apsi_corr.update_layout(plotly_layout_defaults)\n",
    "        fig_rich_vs_apsi_corr.update_xaxes(title_text='Intra-OG APSI (%)', showgrid=False)\n",
    "        fig_rich_vs_apsi_corr.update_yaxes(title_text='Observed Richness (Tree Tips)', showgrid=False) # Consider log type=\"log\"\n",
    "        fig_rich_vs_apsi_corr.show()\n",
    "        # Save plot\n",
    "        fig_path_html = Path(output_figure_dir) / \"figure9_part2_richness_vs_apsi_scatter.html\"\n",
    "        fig_rich_vs_apsi_corr.write_html(str(fig_path_html))\n",
    "        print(f\"Observed Richness vs APSI scatter plot HTML saved to: {fig_path_html}\")\n",
    "        # PDF/SVG saving as in previous cells\n",
    "    else:\n",
    "        print(\"Skipping Observed Richness vs. APSI scatter plot: columns not found.\")\n",
    "\n",
    "print(\"\\n\\n--- Cell 17 (Figure 9 Part 2 - Correlation Analysis) Complete ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c170cc8c-249e-447d-a2e4-201355bcb0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: Figure 12 (Panel A) - Prevalence of OGs in Extreme/Interesting Diversity Profiles\n",
    "\n",
    "# --- Imports & Setup Assumptions ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path\n",
    "\n",
    "# This cell assumes that pandas (pd), numpy (np), plotly.express (px),\n",
    "# plotly.graph_objects (go), Path from pathlib,\n",
    "# and all necessary variables/helper functions from Cell 1 (Setup) are available.\n",
    "# Key variables from previous cells:\n",
    "# - df_merged_diversity: DataFrame from Cell 13 (contains OG, APSI, Group, Shannon_Entropy, Observed_Richness).\n",
    "# - output_summary_dir_phase1: Directory where Cell 15 saved its summary.\n",
    "# - output_figure_dir: Directory to save plots.\n",
    "# - plotly_layout_defaults: Default layout for Plotly figures.\n",
    "# - group_colors: Color map for Asgard/GV.\n",
    "# - orthogroup_col, group_col\n",
    "\n",
    "print(\"\\n\\n--- Generating Figure 12 (Panel A): Prevalence of Extreme Diversity Profiles (Cell 18) ---\")\n",
    "\n",
    "# --- Configuration ---\n",
    "extreme_ogs_summary_path = Path(output_summary_dir_phase1) / \"figure11_extreme_diversity_ogs_summary_expanded_revised.csv\"\n",
    "\n",
    "# Define the 10 \"Reason_for_Highlight\" categories exactly as they appear as substrings in Cell 15's output\n",
    "# These are the individual reasons, not the combined string from the 'Reason_for_Highlight' column yet.\n",
    "# The combined strings in the CSV are like \"Top 5% Shannon Entropy; High Entropy & Low APSI\"\n",
    "# We need to check for the presence of these specific reason components.\n",
    "\n",
    "# These are the individual component reasons used in Cell 15.\n",
    "# The combined reasons (like \"High Entropy & Low APSI\") are also distinct categories we defined.\n",
    "profile_definitions_for_counting = {\n",
    "    f\"Top {n_percentile_strict}% Shannon Entropy\": f\"Top {n_percentile_strict}% Shannon Entropy\",\n",
    "    f\"Bottom {n_percentile_strict}% Shannon Entropy\": f\"Bottom {n_percentile_strict}% Shannon Entropy\",\n",
    "    f\"Bottom {n_percentile_strict}% APSI\": f\"Bottom {n_percentile_strict}% APSI\",\n",
    "    f\"Top {n_percentile_strict}% APSI\": f\"Top {n_percentile_strict}% APSI\",\n",
    "    f\"Top {n_percentile_strict}% Observed Richness\": f\"Top {n_percentile_strict}% Observed Richness\",\n",
    "    f\"Bottom {n_percentile_strict}% Observed Richness\": f\"Bottom {n_percentile_strict}% Observed Richness\",\n",
    "    \"High Entropy & Low APSI\": \"High Entropy & Low APSI\", # This is a combined category\n",
    "    \"High Entropy & High APSI\": \"High Entropy & High APSI\", # This is a combined category\n",
    "    \"High Richness & High APSI\": \"High Richness & High APSI\", # This is a combined category\n",
    "    \"Low Entropy & Low APSI\": \"Low Entropy & Low APSI\"    # This is a combined category\n",
    "}\n",
    "# Use n_percentile_strict and combo_percentile_relaxed from Cell 15 if they are in scope,\n",
    "# otherwise, define them here based on Cell 15's logic.\n",
    "# For safety, let's define them here if not found.\n",
    "if 'n_percentile_strict' not in locals(): n_percentile_strict = 5\n",
    "if 'combo_percentile_relaxed' not in locals(): combo_percentile_relaxed = 25 # Matching revised Cell 15\n",
    "\n",
    "# Update profile_definitions_for_counting keys to reflect the actual strings\n",
    "# The values will be the shorter names for plotting\n",
    "profile_definitions_for_counting = {\n",
    "    f\"Top {n_percentile_strict}% Shannon Entropy\": f\"Top {n_percentile_strict}% Ent.\",\n",
    "    f\"Bottom {n_percentile_strict}% Shannon Entropy\": f\"Bot. {n_percentile_strict}% Ent.\",\n",
    "    f\"Bottom {n_percentile_strict}% APSI\": f\"Bot. {n_percentile_strict}% APSI\",\n",
    "    f\"Top {n_percentile_strict}% APSI\": f\"Top {n_percentile_strict}% APSI\",\n",
    "    f\"Top {n_percentile_strict}% Observed Richness\": f\"Top {n_percentile_strict}% Rich.\",\n",
    "    f\"Bottom {n_percentile_strict}% Observed Richness\": f\"Bot. {n_percentile_strict}% Rich.\",\n",
    "    \"High Entropy & Low APSI\": \"Hi Ent, Lo APSI\",\n",
    "    \"High Entropy & High APSI\": \"Hi Ent, Hi APSI\",\n",
    "    \"High Richness & High APSI\": \"Hi Rich, Hi APSI\",\n",
    "    \"Low Entropy & Low APSI\": \"Lo Ent, Lo APSI\"\n",
    "}\n",
    "\n",
    "\n",
    "# --- Load Data ---\n",
    "error_found_loading = False\n",
    "if not extreme_ogs_summary_path.is_file():\n",
    "    print(f\"ERROR: Extreme OGs summary file not found at '{extreme_ogs_summary_path}'. Please run Cell 15 (revised) first.\")\n",
    "    df_extreme_ogs_summary = pd.DataFrame()\n",
    "    error_found_loading = True\n",
    "else:\n",
    "    try:\n",
    "        df_extreme_ogs_summary = pd.read_csv(extreme_ogs_summary_path)\n",
    "        print(f\"Successfully loaded extreme OGs summary. Shape: {df_extreme_ogs_summary.shape}\")\n",
    "        if orthogroup_col not in df_extreme_ogs_summary.columns or group_col not in df_extreme_ogs_summary.columns or 'Reason_for_Highlight' not in df_extreme_ogs_summary.columns:\n",
    "            print(f\"ERROR: Loaded summary file is missing critical columns: '{orthogroup_col}', '{group_col}', or 'Reason_for_Highlight'.\")\n",
    "            df_extreme_ogs_summary = pd.DataFrame()\n",
    "            error_found_loading = True\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading extreme OGs summary file: {e}\")\n",
    "        df_extreme_ogs_summary = pd.DataFrame()\n",
    "        error_found_loading = True\n",
    "\n",
    "if 'df_merged_diversity' not in locals() or df_merged_diversity.empty:\n",
    "    print(f\"ERROR: DataFrame 'df_merged_diversity' (for total OG counts) not found or is empty. Please run Cell 13 first.\")\n",
    "    error_found_loading = True\n",
    "\n",
    "if error_found_loading:\n",
    "    print(\"Cannot proceed with Figure 12 Panel A due to missing input data.\")\n",
    "else:\n",
    "    # --- Calculate Total OGs for Asgard and GV ---\n",
    "    total_asgard_ogs = df_merged_diversity[df_merged_diversity[group_col] == 'Asgard'][orthogroup_col].nunique()\n",
    "    total_gv_ogs = df_merged_diversity[df_merged_diversity[group_col] == 'GV'][orthogroup_col].nunique()\n",
    "    print(f\"Total Asgard OGs in dataset: {total_asgard_ogs}\")\n",
    "    print(f\"Total GV OGs in dataset: {total_gv_ogs}\")\n",
    "\n",
    "    # --- Calculate Percentage of OGs in Each Profile Category ---\n",
    "    profile_prevalence_data = []\n",
    "\n",
    "    for reason_str_to_check, short_label in profile_definitions_for_counting.items():\n",
    "        # Filter OGs whose 'Reason_for_Highlight' string CONTAINS the specific reason string\n",
    "        # This correctly counts OGs that fall into this category, even if they also fall into others.\n",
    "        df_matching_profile = df_extreme_ogs_summary[\n",
    "            df_extreme_ogs_summary['Reason_for_Highlight'].str.contains(reason_str_to_check, regex=False, na=False)\n",
    "        ]\n",
    "        \n",
    "        count_asgard = df_matching_profile[df_matching_profile[group_col] == 'Asgard'][orthogroup_col].nunique()\n",
    "        count_gv = df_matching_profile[df_matching_profile[group_col] == 'GV'][orthogroup_col].nunique()\n",
    "        \n",
    "        percent_asgard = (count_asgard / total_asgard_ogs * 100) if total_asgard_ogs > 0 else 0\n",
    "        percent_gv = (count_gv / total_gv_ogs * 100) if total_gv_ogs > 0 else 0\n",
    "        \n",
    "        profile_prevalence_data.append({'Profile_Category': short_label, 'Group': 'Asgard', 'Percentage': percent_asgard, 'Count': count_asgard})\n",
    "        profile_prevalence_data.append({'Profile_Category': short_label, 'Group': 'GV', 'Percentage': percent_gv, 'Count': count_gv})\n",
    "\n",
    "    df_profile_prevalence = pd.DataFrame(profile_prevalence_data)\n",
    "\n",
    "    print(\"\\n--- Prevalence of OGs in Defined Diversity Profiles ---\")\n",
    "    if not df_profile_prevalence.empty:\n",
    "        try:\n",
    "            # Display counts for verification\n",
    "            df_display_counts = df_profile_prevalence.pivot(index='Profile_Category', columns='Group', values='Count').fillna(0).astype(int)\n",
    "            print(\"Counts of OGs per profile category:\")\n",
    "            print(df_display_counts.to_markdown())\n",
    "            df_display_perc = df_profile_prevalence.pivot(index='Profile_Category', columns='Group', values='Percentage').fillna(0)\n",
    "            print(\"\\nPercentage of OGs per profile category:\")\n",
    "            print(df_display_perc.to_markdown(floatfmt=\".2f\"))\n",
    "\n",
    "        except ImportError:\n",
    "            print(df_profile_prevalence)\n",
    "\n",
    "        # --- Create Plot for Panel A ---\n",
    "        fig_panel_a = px.bar(\n",
    "            df_profile_prevalence,\n",
    "            x='Profile_Category',\n",
    "            y='Percentage',\n",
    "            color='Group',\n",
    "            barmode='group',\n",
    "            labels={'Percentage': '% of Total OGs in Group', 'Profile_Category': 'Diversity Profile Category'},\n",
    "            color_discrete_map=group_colors # from Cell 1\n",
    "        )\n",
    "        fig_panel_a.update_layout(plotly_layout_defaults)\n",
    "        fig_panel_a.update_xaxes(\n",
    "            title_text=\"Diversity Profile Category\", \n",
    "            showgrid=False, \n",
    "            tickangle=45,\n",
    "            categoryorder='array', # Ensure order matches definition\n",
    "            categoryarray=list(profile_definitions_for_counting.values())\n",
    "        )\n",
    "        fig_panel_a.update_yaxes(title_text=\"% of Total OGs in Group\", showgrid=False, range=[0, df_profile_prevalence['Percentage'].max() * 1.15 if not df_profile_prevalence.empty else 10])\n",
    "        fig_panel_a.update_layout(legend_title_text='Group')\n",
    "        \n",
    "        fig_panel_a.show()\n",
    "\n",
    "        # Save plot\n",
    "        fig_path_html = Path(output_figure_dir) / \"figure12_panel_a_profile_prevalence.html\"\n",
    "        fig_panel_a.write_html(str(fig_path_html))\n",
    "        print(f\"Figure 12 Panel A (Profile Prevalence) HTML saved to: {fig_path_html}\")\n",
    "        try:\n",
    "            fig_path_pdf = Path(output_figure_dir) / \"figure12_panel_a_profile_prevalence.pdf\"\n",
    "            fig_panel_a.write_image(str(fig_path_pdf))\n",
    "            print(f\"Figure 12 Panel A PDF saved to: {fig_path_pdf}\")\n",
    "        except Exception as e: print(f\"PDF export error for Figure 12 Panel A: {e}\")\n",
    "        try:\n",
    "            fig_path_svg = Path(output_figure_dir) / \"figure12_panel_a_profile_prevalence.svg\"\n",
    "            fig_panel_a.write_image(str(fig_path_svg))\n",
    "            print(f\"Figure 12 Panel A SVG saved to: {fig_path_svg}\")\n",
    "        except Exception as e: print(f\"SVG export error for Figure 12 Panel A: {e}\")\n",
    "        \n",
    "        # Save the data used for this panel\n",
    "        panel_a_data_path = Path(output_summary_dir_phase1) / \"figure12_panel_a_profile_prevalence_data.csv\"\n",
    "        try:\n",
    "            df_profile_prevalence.to_csv(panel_a_data_path, index=False, float_format='%.3f')\n",
    "            print(f\"Data for Figure 12 Panel A saved to: {panel_a_data_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving data for Figure 12 Panel A: {e}\")\n",
    "            \n",
    "    else:\n",
    "        print(\"No data generated for profile prevalence plot.\")\n",
    "\n",
    "print(\"\\n\\n--- Cell 18 (Figure 12 Panel A - Prevalence of Extreme Profiles) Complete ---\")\n",
    "print(\"Next: Cell 19 for Panels B, C, D (Functional Breakdown of Selected Profiles)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a3f3f1-5a18-427c-93cc-849316c58260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: Figure 12 (Panels B, C, D) - Functional Breakdown of Selected Profiles & Correlation Plot\n",
    "\n",
    "# --- Imports & Setup Assumptions ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from pathlib import Path\n",
    "from scipy.stats import spearmanr # For correlation\n",
    "\n",
    "# This cell assumes that pandas (pd), numpy (np), plotly.express (px),\n",
    "# plotly.graph_objects (go), Path from pathlib, make_subplots, spearmanr\n",
    "# and all necessary variables/helper functions from Cell 1 (Setup) are available.\n",
    "# Key variables from previous cells:\n",
    "# - df_merged_diversity: DataFrame from Cell 13 (OG, APSI, Group, Shannon_Entropy, Observed_Richness, Broad_Functional_Category).\n",
    "# - output_summary_dir_phase1: Directory where Cell 15 saved its summary.\n",
    "# - output_figure_dir: Directory to save plots.\n",
    "# - plotly_layout_defaults: Default layout for Plotly figures.\n",
    "# - group_colors, broad_category_color_map: Color maps.\n",
    "# - orthogroup_col, group_col, apsi_col, broad_func_cat_col\n",
    "\n",
    "print(\"\\n\\n--- Generating Figure 12 (Panels B, C, D): Functional Breakdown & Correlation (Cell 19) ---\")\n",
    "\n",
    "# --- Configuration & Definitions ---\n",
    "extreme_ogs_summary_path = Path(output_summary_dir_phase1) / \"figure11_extreme_diversity_ogs_summary_expanded_revised.csv\"\n",
    "\n",
    "# Profiles for functional breakdown (Panels B and C)\n",
    "# Keys must exactly match the strings used in Cell 15's \"Reason_for_Highlight\" generation\n",
    "profile_for_panel_b_key = \"High Entropy & High APSI\"\n",
    "profile_for_panel_c_key = \"High Richness & High APSI\"\n",
    "\n",
    "# Corresponding short labels for titles (can be fetched from profile_definitions_for_counting if that dict is made global/redefined)\n",
    "# For safety, defining here:\n",
    "if 'n_percentile_strict' not in locals(): n_percentile_strict = 5\n",
    "if 'combo_percentile_relaxed' not in locals(): combo_percentile_relaxed = 25\n",
    "\n",
    "profile_labels = {\n",
    "    \"High Entropy & High APSI\": \"Hi Ent, Hi APSI\",\n",
    "    \"High Richness & High APSI\": \"Hi Rich, Hi APSI\",\n",
    "}\n",
    "profile_for_panel_b_label = profile_labels.get(profile_for_panel_b_key, profile_for_panel_b_key)\n",
    "profile_for_panel_c_label = profile_labels.get(profile_for_panel_c_key, profile_for_panel_c_key)\n",
    "\n",
    "\n",
    "# --- Load Data ---\n",
    "error_found_loading = False\n",
    "df_extreme_ogs_summary = pd.DataFrame()\n",
    "df_merged_diversity_fig12_panels_bcd = pd.DataFrame()\n",
    "\n",
    "if not extreme_ogs_summary_path.is_file():\n",
    "    print(f\"ERROR: Extreme OGs summary file not found at '{extreme_ogs_summary_path}'. Please run Cell 15 (revised) first.\")\n",
    "    error_found_loading = True\n",
    "else:\n",
    "    try:\n",
    "        df_extreme_ogs_summary = pd.read_csv(extreme_ogs_summary_path)\n",
    "        if not all(k in df_extreme_ogs_summary.columns for k in [orthogroup_col, group_col, 'Reason_for_Highlight', broad_func_cat_col]):\n",
    "            print(f\"ERROR: Loaded extreme OGs summary file is missing critical columns.\")\n",
    "            error_found_loading = True\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading extreme OGs summary file: {e}\")\n",
    "        error_found_loading = True\n",
    "\n",
    "if 'df_merged_diversity' not in locals() or df_merged_diversity.empty:\n",
    "    print(f\"ERROR: DataFrame 'df_merged_diversity' not found or is empty. Please run Cell 13 first.\")\n",
    "    error_found_loading = True\n",
    "else:\n",
    "    df_merged_diversity_fig12_panels_bcd = df_merged_diversity.copy()\n",
    "    if not all(k in df_merged_diversity_fig12_panels_bcd.columns for k in [orthogroup_col, group_col, apsi_col, 'Shannon_Entropy']):\n",
    "        print(f\"ERROR: 'df_merged_diversity' is missing critical columns for Panel D.\")\n",
    "        error_found_loading = True\n",
    "\n",
    "if error_found_loading:\n",
    "    print(\"Cannot proceed with Figure 12 Panels B,C,D due to missing input data.\")\n",
    "else:\n",
    "    print(f\"Successfully loaded data. df_extreme_ogs_summary: {df_extreme_ogs_summary.shape}, df_merged_diversity: {df_merged_diversity_fig12_panels_bcd.shape}\")\n",
    "\n",
    "    # --- Create Figure with Subplots (3 rows, 1 col for these three panels) ---\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=1,\n",
    "        subplot_titles=(\n",
    "            f\"<b>B.</b> Functional Categories of '{profile_for_panel_b_label}' OGs\",\n",
    "            f\"<b>C.</b> Functional Categories of '{profile_for_panel_c_label}' OGs\",\n",
    "            \"<b>D.</b> Shannon Entropy vs. Intra-OG APSI (with Correlations)\"\n",
    "        ),\n",
    "        vertical_spacing=0.15, # Increased spacing for clarity\n",
    "        row_heights=[0.33, 0.33, 0.34] # Adjust relative heights\n",
    "    )\n",
    "    \n",
    "    panel_b_c_plot_data_summary = []\n",
    "\n",
    "    # --- Panel B: Functional Composition of \"High Entropy & High APSI\" Profile ---\n",
    "    df_panel_b_subset = df_extreme_ogs_summary[\n",
    "        df_extreme_ogs_summary['Reason_for_Highlight'].str.contains(profile_for_panel_b_key, regex=False, na=False)\n",
    "    ].copy()\n",
    "\n",
    "    if not df_panel_b_subset.empty and broad_func_cat_col in df_panel_b_subset.columns:\n",
    "        func_cat_counts_b = df_panel_b_subset.groupby(group_col)[broad_func_cat_col].value_counts(normalize=True).mul(100).rename('Percentage').reset_index()\n",
    "        func_cat_counts_b = func_cat_counts_b.sort_values(['Group', 'Percentage'], ascending=[True, False])\n",
    "        \n",
    "        for _, row_data in func_cat_counts_b.iterrows():\n",
    "            panel_b_c_plot_data_summary.append({\n",
    "                'Panel': 'B', 'Profile': profile_for_panel_b_label, 'Group': row_data[group_col],\n",
    "                broad_func_cat_col: row_data[broad_func_cat_col], 'Percentage': row_data['Percentage']\n",
    "            })\n",
    "        \n",
    "        # Create grouped bar chart for Panel B\n",
    "        # This requires plotting each functional category as a trace if we want them colored by func_cat and grouped by Asgard/GV\n",
    "        # A simpler approach for now: show overall functional breakdown, or facet by Asgard/GV if px.bar was used separately\n",
    "        # For this multi-panel, let's plot overall distribution for this profile, or sum Asgard/GV for simplicity if too few points\n",
    "        overall_func_cat_counts_b = df_panel_b_subset[broad_func_cat_col].value_counts(normalize=True).mul(100).reset_index()\n",
    "        overall_func_cat_counts_b.columns = [broad_func_cat_col, 'Percentage']\n",
    "        overall_func_cat_counts_b = overall_func_cat_counts_b.sort_values('Percentage', ascending=False)\n",
    "        \n",
    "        if not overall_func_cat_counts_b.empty:\n",
    "            fig.add_trace(go.Bar(\n",
    "                x=overall_func_cat_counts_b[broad_func_cat_col],\n",
    "                y=overall_func_cat_counts_b['Percentage'],\n",
    "                marker_color=[broad_category_color_map.get(cat, '#cccccc') for cat in overall_func_cat_counts_b[broad_func_cat_col]],\n",
    "                name=profile_for_panel_b_label # Single legend entry for the profile type\n",
    "            ), row=1, col=1) # Corresponds to subplot \"B\"\n",
    "            fig.update_xaxes(title_text=\"\", tickangle=45, categoryorder='total descending', row=1, col=1)\n",
    "            fig.update_yaxes(title_text=\"% of OGs in Profile\", range=[0, max(10, overall_func_cat_counts_b['Percentage'].max() * 1.1)], row=1, col=1)\n",
    "        else:\n",
    "            print(f\"No data to plot for Panel B ({profile_for_panel_b_label}).\")\n",
    "    else:\n",
    "        print(f\"No OGs for profile '{profile_for_panel_b_key}' or functional category column missing for Panel B.\")\n",
    "\n",
    "    # --- Panel C: Functional Composition of \"High Richness & High APSI\" Profile ---\n",
    "    df_panel_c_subset = df_extreme_ogs_summary[\n",
    "        df_extreme_ogs_summary['Reason_for_Highlight'].str.contains(profile_for_panel_c_key, regex=False, na=False)\n",
    "    ].copy()\n",
    "\n",
    "    if not df_panel_c_subset.empty and broad_func_cat_col in df_panel_c_subset.columns:\n",
    "        func_cat_counts_c = df_panel_c_subset.groupby(group_col)[broad_func_cat_col].value_counts(normalize=True).mul(100).rename('Percentage').reset_index()\n",
    "        func_cat_counts_c = func_cat_counts_c.sort_values(['Group', 'Percentage'], ascending=[True, False])\n",
    "\n",
    "        for _, row_data in func_cat_counts_c.iterrows():\n",
    "            panel_b_c_plot_data_summary.append({\n",
    "                'Panel': 'C', 'Profile': profile_for_panel_c_label, 'Group': row_data[group_col],\n",
    "                broad_func_cat_col: row_data[broad_func_cat_col], 'Percentage': row_data['Percentage']\n",
    "            })\n",
    "\n",
    "        overall_func_cat_counts_c = df_panel_c_subset[broad_func_cat_col].value_counts(normalize=True).mul(100).reset_index()\n",
    "        overall_func_cat_counts_c.columns = [broad_func_cat_col, 'Percentage']\n",
    "        overall_func_cat_counts_c = overall_func_cat_counts_c.sort_values('Percentage', ascending=False)\n",
    "\n",
    "        if not overall_func_cat_counts_c.empty:\n",
    "            fig.add_trace(go.Bar(\n",
    "                x=overall_func_cat_counts_c[broad_func_cat_col],\n",
    "                y=overall_func_cat_counts_c['Percentage'],\n",
    "                marker_color=[broad_category_color_map.get(cat, '#cccccc') for cat in overall_func_cat_counts_c[broad_func_cat_col]],\n",
    "                name=profile_for_panel_c_label # Single legend entry for the profile type\n",
    "            ), row=2, col=1) # Corresponds to subplot \"C\"\n",
    "            fig.update_xaxes(title_text=\"\", tickangle=45, categoryorder='total descending', row=2, col=1)\n",
    "            fig.update_yaxes(title_text=\"% of OGs in Profile\", range=[0, max(10, overall_func_cat_counts_c['Percentage'].max() * 1.1)], row=2, col=1)\n",
    "        else:\n",
    "            print(f\"No data to plot for Panel C ({profile_for_panel_c_label}).\")\n",
    "    else:\n",
    "        print(f\"No OGs for profile '{profile_for_panel_c_key}' or functional category column missing for Panel C.\")\n",
    "\n",
    "    # --- Panel D: Shannon Entropy vs. APSI with Correlation Coefficients ---\n",
    "    if not df_merged_diversity_fig12_panels_bcd.empty and 'Shannon_Entropy' in df_merged_diversity_fig12_panels_bcd.columns and apsi_col in df_merged_diversity_fig12_panels_bcd.columns:\n",
    "        # Use a fresh copy for this plot to avoid modification issues from previous plots\n",
    "        df_panel_d_plot = df_merged_diversity_fig12_panels_bcd.copy()\n",
    "        df_panel_d_plot['APSI_Percent_Plot'] = df_panel_d_plot[apsi_col] * 100\n",
    "        \n",
    "        annotation_y_start = 0.95\n",
    "        annotation_y_step = 0.07\n",
    "        \n",
    "        for group_name_panel_d, group_df_panel_d in df_panel_d_plot.groupby(group_col):\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=group_df_panel_d['APSI_Percent_Plot'],\n",
    "                y=group_df_panel_d['Shannon_Entropy'],\n",
    "                mode='markers',\n",
    "                name=group_name_panel_d, # For legend\n",
    "                legendgroup=group_name_panel_d, # Group legend items\n",
    "                marker=dict(color=group_colors.get(group_name_panel_d), opacity=0.5, size=5)\n",
    "            ), row=3, col=1) # Corresponds to subplot \"D\"\n",
    "\n",
    "            # Calculate and add Spearman correlation annotation\n",
    "            df_corr_subset = group_df_panel_d[['APSI_Percent_Plot', 'Shannon_Entropy']].dropna()\n",
    "            if len(df_corr_subset) > 2:\n",
    "                rho, p_val = spearmanr(df_corr_subset['APSI_Percent_Plot'], df_corr_subset['Shannon_Entropy'])\n",
    "                annotation_text = f\"{group_name_panel_d}: ρ={rho:.2f} (p={p_val:.2g})\"\n",
    "                fig.add_annotation(\n",
    "                    xref=\"x3 domain\", yref=\"y3 domain\", # Relative to panel D domain\n",
    "                    x=0.02, y=annotation_y_start, \n",
    "                    text=annotation_text, showarrow=False,\n",
    "                    font=dict(size=10, color=group_colors.get(group_name_panel_d)),\n",
    "                    align=\"left\", xanchor=\"left\"\n",
    "                )\n",
    "                annotation_y_start -= annotation_y_step # Move next annotation down\n",
    "        \n",
    "        fig.update_xaxes(title_text=\"Intra-OG APSI (%)\", row=3, col=1)\n",
    "        fig.update_yaxes(title_text=\"Shannon Entropy (Tree-based)\", row=3, col=1)\n",
    "    else:\n",
    "        print(\"Skipping Panel D: df_merged_diversity is empty or missing columns.\")\n",
    "\n",
    "    # --- Final Layout Updates for the Entire Figure ---\n",
    "    fig.update_layout(\n",
    "        height=1350, # Increased height for 3 distinct panels\n",
    "        plot_bgcolor='rgba(0,0,0,0)', \n",
    "        paper_bgcolor='rgba(0,0,0,0)',\n",
    "        margin=dict(l=80, r=50, t=100, b=100), # General margins\n",
    "        showlegend=True # Show legend for Panel D colors\n",
    "    )\n",
    "    if 'plotly_layout_defaults' in locals() and plotly_layout_defaults.font:\n",
    "        fig.update_layout(font=plotly_layout_defaults.font)\n",
    "    for i in range(len(fig.layout.annotations)): # Update subplot titles\n",
    "        if fig.layout.annotations[i].text.startswith(\"<b>\"): # Heuristic\n",
    "            fig.layout.annotations[i].font.size = 14\n",
    "            if 'plotly_layout_defaults' in locals() and plotly_layout_defaults.font:\n",
    "                fig.layout.annotations[i].font.family = plotly_layout_defaults.font.family\n",
    "    \n",
    "    # Common axis styling for all subplots\n",
    "    for i in range(1, 4): # For rows 1, 2, 3\n",
    "        fig.update_xaxes(showgrid=False, zeroline=False, linecolor='black', linewidth=1, ticks=\"outside\", ticklen=5, tickwidth=1, tickcolor='black', row=i, col=1)\n",
    "        fig.update_yaxes(showgrid=False, zeroline=False, linecolor='black', linewidth=1, ticks=\"outside\", ticklen=5, tickwidth=1, tickcolor='black', row=i, col=1)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # --- Save Figure ---\n",
    "    fig_path_html = Path(output_figure_dir) / \"figure12_panels_BCD.html\"\n",
    "    fig.write_html(str(fig_path_html))\n",
    "    print(f\"Figure 12 Panels B,C,D HTML saved to: {fig_path_html}\")\n",
    "    try:\n",
    "        fig_path_pdf = Path(output_figure_dir) / \"figure12_panels_BCD.pdf\"\n",
    "        fig.write_image(str(fig_path_pdf), width=800, height=1350)\n",
    "        print(f\"Figure 12 Panels B,C,D PDF saved to: {fig_path_pdf}\")\n",
    "    except Exception as e: print(f\"Warning: Could not export Figure 12 BCD to PDF. Error: {e}\")\n",
    "    \n",
    "    # Save the aggregated data used for panels B and C\n",
    "    if panel_b_c_plot_data_summary:\n",
    "        df_panel_b_c_summary = pd.DataFrame(panel_b_c_plot_data_summary)\n",
    "        summary_table_path = Path(output_summary_dir_phase1) / \"figure12_panels_BC_func_breakdown_summary.csv\"\n",
    "        try:\n",
    "            df_panel_b_c_summary.to_csv(summary_table_path, index=False, float_format='%.2f')\n",
    "            print(f\"Summary data for Figure 12 Panels B & C saved to: {summary_table_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving summary data for Figure 12 Panels B & C: {e}\")\n",
    "\n",
    "print(\"\\n\\n--- Cell 19 (Figure 12 Panels B, C, D) Complete ---\")\n",
    "print(\"You can now combine the plot from Cell 18 (Panel A) with this plot (Panels B, C, D) externally to create your full Figure 12.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020d62cc-7759-48b7-9594-3a5ebf8ebd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20: Figure 12 - Consolidated Diversity Profiles, Functional Enrichment, and Correlations\n",
    "\n",
    "# --- Imports & Setup Assumptions ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from pathlib import Path\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# This cell assumes that pandas (pd), numpy (np), plotly.express (px),\n",
    "# plotly.graph_objects (go), Path from pathlib, make_subplots, spearmanr\n",
    "# and all necessary variables/helper functions from Cell 1 (Setup) are available.\n",
    "# Key variables from previous cells:\n",
    "# - df_merged_diversity: DataFrame from Cell 13 (OG, APSI, Group, Shannon_Entropy, Observed_Richness, Broad_Functional_Category).\n",
    "# - output_summary_dir_phase1: Directory where Cell 15 saved its summary.\n",
    "# - output_figure_dir: Directory to save plots.\n",
    "# - plotly_layout_defaults: Default layout for Plotly figures.\n",
    "# - group_colors, broad_category_color_map: Color maps.\n",
    "# - orthogroup_col, group_col, apsi_col, broad_func_cat_col\n",
    "\n",
    "print(\"\\n\\n--- Generating Figure 12: Consolidated Diversity, Functional Enrichment, & Correlations (Cell 20) ---\")\n",
    "\n",
    "# --- Configuration & Definitions (mirroring Cell 15 & 18 for consistency) ---\n",
    "extreme_ogs_summary_path = Path(output_summary_dir_phase1) / \"figure11_extreme_diversity_ogs_summary_expanded_revised.csv\"\n",
    "\n",
    "if 'n_percentile_strict' not in locals(): n_percentile_strict = 5\n",
    "if 'combo_percentile_relaxed' not in locals(): combo_percentile_relaxed = 25 # From revised Cell 15\n",
    "\n",
    "# Define the profiles for Panel A (Prevalence) - select key ones\n",
    "# Keys are the search strings, values are plot labels\n",
    "profiles_for_panel_a = {\n",
    "    f\"Top {n_percentile_strict}% Shannon Entropy\": f\"Top {n_percentile_strict}% Ent.\",\n",
    "    f\"Bottom {n_percentile_strict}% APSI\": f\"Bot. {n_percentile_strict}% APSI\",\n",
    "    \"High Entropy & Low APSI\": \"Hi Ent, Lo APSI\",\n",
    "    \"High Entropy & High APSI\": \"Hi Ent, Hi APSI\",\n",
    "    \"Low Entropy & Low APSI\": \"Lo Ent, Lo APSI\",\n",
    "    \"High Richness & High APSI\": \"Hi Rich, Hi APSI\"\n",
    "}\n",
    "profile_keys_for_panel_a = list(profiles_for_panel_a.keys())\n",
    "profile_labels_for_panel_a = list(profiles_for_panel_a.values())\n",
    "\n",
    "# Define the \"surprising\" profiles for Panel B (Functional Enrichment Heatmap)\n",
    "profiles_for_panel_b_heatmap = {\n",
    "    \"High Entropy & High APSI\": \"Hi Ent, Hi APSI\",\n",
    "    \"Low Entropy & Low APSI\": \"Lo Ent, Lo APSI\"\n",
    "    # Can add \"High Richness & High APSI\" if desired, making heatmap wider or using a second heatmap\n",
    "}\n",
    "\n",
    "# --- Load Data ---\n",
    "error_found_loading = False\n",
    "df_extreme_ogs_summary = pd.DataFrame()\n",
    "df_merged_diversity_fig12 = pd.DataFrame()\n",
    "\n",
    "if not extreme_ogs_summary_path.is_file():\n",
    "    print(f\"ERROR: Extreme OGs summary file not found at '{extreme_ogs_summary_path}'. Please run Cell 15 (revised) first.\")\n",
    "    error_found_loading = True\n",
    "else:\n",
    "    try:\n",
    "        df_extreme_ogs_summary = pd.read_csv(extreme_ogs_summary_path)\n",
    "        if not all(k in df_extreme_ogs_summary.columns for k in [orthogroup_col, group_col, 'Reason_for_Highlight', broad_func_cat_col]):\n",
    "            print(f\"ERROR: Loaded extreme OGs summary file is missing critical columns.\")\n",
    "            df_extreme_ogs_summary = pd.DataFrame() # Invalidate if critical cols missing\n",
    "            error_found_loading = True\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading extreme OGs summary file: {e}\")\n",
    "        df_extreme_ogs_summary = pd.DataFrame()\n",
    "        error_found_loading = True\n",
    "\n",
    "if 'df_merged_diversity' not in locals() or df_merged_diversity.empty:\n",
    "    print(f\"ERROR: DataFrame 'df_merged_diversity' not found or is empty. Please run Cell 13 first.\")\n",
    "    error_found_loading = True\n",
    "else:\n",
    "    df_merged_diversity_fig12 = df_merged_diversity.copy()\n",
    "    if not all(k in df_merged_diversity_fig12.columns for k in [orthogroup_col, group_col, apsi_col, 'Shannon_Entropy', broad_func_cat_col]):\n",
    "        print(f\"ERROR: 'df_merged_diversity' is missing critical columns for Panels B/C.\")\n",
    "        error_found_loading = True\n",
    "\n",
    "if error_found_loading:\n",
    "    print(\"Cannot proceed with Figure 12 generation due to missing input data.\")\n",
    "else:\n",
    "    print(f\"Data loaded. df_extreme_ogs_summary: {df_extreme_ogs_summary.shape}, df_merged_diversity_fig12: {df_merged_diversity_fig12.shape}\")\n",
    "\n",
    "    # --- Create Figure with Subplots (Adjust layout: 2 rows for A & C, 1 wider row for B) ---\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=2, # Increased columns to give heatmap more space\n",
    "        specs=[[{}, {}],           # Row 1: Panel A (Prevalence) spans 2 cols\n",
    "               [{\"colspan\": 2}, None], # Row 2: Panel B (Heatmap) spans 2 cols\n",
    "               [{}, {}]            # Row 3: Panel C (Correlation) spans 2 cols\n",
    "              ],\n",
    "        subplot_titles=(\n",
    "            \"<b>A.</b> Prevalence of OGs in Key Diversity Profiles\", None,\n",
    "            \"<b>B.</b> Functional Enrichment in 'Surprising' Diversity Profiles\", None, # Title for heatmap row\n",
    "            \"<b>C.</b> Shannon Entropy vs. Intra-OG APSI\", None\n",
    "        ),\n",
    "        vertical_spacing=0.18, # Adjusted spacing\n",
    "        row_heights=[0.3, 0.4, 0.3] # Relative heights: Prevalence, Heatmap, Correlation\n",
    "    )\n",
    "\n",
    "    # --- Panel A: Prevalence of Key Diversity Profiles ---\n",
    "    total_asgard_ogs = df_merged_diversity_fig12[df_merged_diversity_fig12[group_col] == 'Asgard'][orthogroup_col].nunique()\n",
    "    total_gv_ogs = df_merged_diversity_fig12[df_merged_diversity_fig12[group_col] == 'GV'][orthogroup_col].nunique()\n",
    "    panel_a_plot_data = []\n",
    "    for reason_key, short_label in profiles_for_panel_a.items():\n",
    "        df_matching = df_extreme_ogs_summary[df_extreme_ogs_summary['Reason_for_Highlight'].str.contains(reason_key, regex=False, na=False)]\n",
    "        count_asgard = df_matching[df_matching[group_col] == 'Asgard'][orthogroup_col].nunique()\n",
    "        count_gv = df_matching[df_matching[group_col] == 'GV'][orthogroup_col].nunique()\n",
    "        panel_a_plot_data.append({'Profile': short_label, 'Group': 'Asgard', 'Percentage': (count_asgard / total_asgard_ogs * 100) if total_asgard_ogs > 0 else 0})\n",
    "        panel_a_plot_data.append({'Profile': short_label, 'Group': 'GV', 'Percentage': (count_gv / total_gv_ogs * 100) if total_gv_ogs > 0 else 0})\n",
    "    df_panel_a_plot = pd.DataFrame(panel_a_plot_data)\n",
    "\n",
    "    if not df_panel_a_plot.empty:\n",
    "        # Using plotly express for simplicity in adding grouped bars to subplot\n",
    "        fig_a_temp = px.bar(df_panel_a_plot, x='Profile', y='Percentage', color='Group', barmode='group', color_discrete_map=group_colors)\n",
    "        for trace in fig_a_temp.data:\n",
    "            fig.add_trace(trace, row=1, col=1) # Add to the first subplot area\n",
    "        fig.update_xaxes(categoryorder='array', categoryarray=profile_labels_for_panel_a, tickangle=45, row=1, col=1, showgrid=False, zeroline=False, linecolor='black', linewidth=1)\n",
    "        fig.update_yaxes(title_text=\"% of Total OGs\", range=[0, df_panel_a_plot['Percentage'].max()*1.15 if not df_panel_a_plot.empty else 10], row=1, col=1, showgrid=False, zeroline=False, linecolor='black', linewidth=1)\n",
    "        fig.update_layout(legend_title_text='Group')\n",
    "        # Manually set colspan for the x and y axes of panel A\n",
    "        fig.layout.xaxis.domain = [0.0, 1.0] # Span both columns\n",
    "        fig.layout.yaxis.domain = fig.layout.yaxis.domain # Keep its original y-domain for row 1\n",
    "\n",
    "    else:\n",
    "        print(\"No data for Panel A.\")\n",
    "\n",
    "\n",
    "    # --- Panel B: Functional Enrichment Heatmap ---\n",
    "    heatmap_data_list = []\n",
    "    baseline_func_dist_asgard = df_merged_diversity_fig12[df_merged_diversity_fig12[group_col] == 'Asgard'][broad_func_cat_col].value_counts(normalize=True).mul(100)\n",
    "    baseline_func_dist_gv = df_merged_diversity_fig12[df_merged_diversity_fig12[group_col] == 'GV'][broad_func_cat_col].value_counts(normalize=True).mul(100)\n",
    "\n",
    "    for profile_key, short_profile_label in profiles_for_panel_b_heatmap.items():\n",
    "        for group_val in ['Asgard', 'GV']:\n",
    "            df_profile_group_subset = df_extreme_ogs_summary[\n",
    "                df_extreme_ogs_summary['Reason_for_Highlight'].str.contains(profile_key, regex=False, na=False) &\n",
    "                (df_extreme_ogs_summary[group_col] == group_val)\n",
    "            ]\n",
    "            if df_profile_group_subset.empty: continue\n",
    "\n",
    "            observed_dist = df_profile_group_subset[broad_func_cat_col].value_counts(normalize=True).mul(100)\n",
    "            baseline_dist = baseline_func_dist_asgard if group_val == 'Asgard' else baseline_func_dist_gv\n",
    "            \n",
    "            for func_cat, obs_perc in observed_dist.items():\n",
    "                exp_perc = baseline_dist.get(func_cat, 0) # Get expected, default to 0 if func_cat not in baseline\n",
    "                log2_fold_change = np.log2((obs_perc + 1e-9) / (exp_perc + 1e-9)) # Add small epsilon to avoid log(0) or div by zero\n",
    "                heatmap_data_list.append({\n",
    "                    'Functional_Category': func_cat,\n",
    "                    'Profile_Group': f\"{group_val} - {short_profile_label}\",\n",
    "                    'Log2_Fold_Change': log2_fold_change\n",
    "                })\n",
    "    \n",
    "    if heatmap_data_list:\n",
    "        df_heatmap = pd.DataFrame(heatmap_data_list)\n",
    "        df_heatmap_pivot = df_heatmap.pivot(index='Functional_Category', columns='Profile_Group', values='Log2_Fold_Change').fillna(0)\n",
    "        \n",
    "        # Sort categories for better readability (e.g., by mean enrichment or a predefined order)\n",
    "        # For now, sort alphabetically\n",
    "        df_heatmap_pivot = df_heatmap_pivot.sort_index()\n",
    "        \n",
    "        # Determine a symmetric color scale midpoint\n",
    "        abs_max = max(abs(df_heatmap_pivot.min().min()), abs(df_heatmap_pivot.max().max()))\n",
    "\n",
    "        fig.add_trace(go.Heatmap(\n",
    "            z=df_heatmap_pivot.values,\n",
    "            x=df_heatmap_pivot.columns,\n",
    "            y=df_heatmap_pivot.index,\n",
    "            colorscale='RdBu', # Red-Blue diverging scale\n",
    "            zmid=0, # Center color at 0\n",
    "            zmin=-abs_max if abs_max >0 else -1, \n",
    "            zmax=abs_max if abs_max >0 else 1,\n",
    "            colorbar_title='Log2 (Obs/Exp %)'\n",
    "        ), row=2, col=1) # This trace goes into the second row, first column (which spans 2)\n",
    "        fig.update_xaxes(tickangle=30, row=2, col=1)\n",
    "        fig.update_yaxes(categoryorder='array', categoryarray=df_heatmap_pivot.index.tolist(), row=2, col=1) # Keep sorted order\n",
    "    else:\n",
    "        print(\"No data for Panel B heatmap.\")\n",
    "\n",
    "\n",
    "    # --- Panel C: Shannon Entropy vs. APSI with Correlation Coefficients ---\n",
    "    if not df_merged_diversity_fig12.empty and 'Shannon_Entropy' in df_merged_diversity_fig12.columns and apsi_col in df_merged_diversity_fig12.columns:\n",
    "        df_panel_c_plot = df_merged_diversity_fig12.copy()\n",
    "        df_panel_c_plot['APSI_Percent_Plot'] = df_panel_c_plot[apsi_col] * 100\n",
    "        \n",
    "        annotation_y_start_c = 0.95\n",
    "        annotation_y_step_c = 0.07\n",
    "        \n",
    "        for group_name_panel_c, group_df_panel_c in df_panel_c_plot.groupby(group_col):\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=group_df_panel_c['APSI_Percent_Plot'],\n",
    "                y=group_df_panel_c['Shannon_Entropy'],\n",
    "                mode='markers', name=group_name_panel_c, legendgroup=group_name_panel_c, # Unique legend group for this panel\n",
    "                marker=dict(color=group_colors.get(group_name_panel_c), opacity=0.5, size=4)\n",
    "            ), row=3, col=1) # This trace goes into the third row, first column (which spans 2)\n",
    "\n",
    "            df_corr_subset_c = group_df_panel_c[['APSI_Percent_Plot', 'Shannon_Entropy']].dropna()\n",
    "            if len(df_corr_subset_c) > 2:\n",
    "                rho, p_val = spearmanr(df_corr_subset_c['APSI_Percent_Plot'], df_corr_subset_c['Shannon_Entropy'])\n",
    "                fig.add_annotation(\n",
    "                    xref=\"x3 domain\", yref=\"y3 domain\", x=0.02, y=annotation_y_start_c, \n",
    "                    text=f\"{group_name_panel_c}: ρ={rho:.2f} (p={p_val:.2g})\", showarrow=False,\n",
    "                    font=dict(size=10, color=group_colors.get(group_name_panel_c)), align=\"left\", xanchor=\"left\"\n",
    "                )\n",
    "                annotation_y_start_c -= annotation_y_step_c\n",
    "        \n",
    "        fig.update_xaxes(title_text=\"Intra-OG APSI (%)\", row=3, col=1, showgrid=False, zeroline=False, linecolor='black', linewidth=1)\n",
    "        fig.update_yaxes(title_text=\"Shannon Entropy\", row=3, col=1, showgrid=False, zeroline=False, linecolor='black', linewidth=1)\n",
    "        # Manually set colspan for the x and y axes of panel C\n",
    "        fig.layout.xaxis3.domain = [0.0, 1.0] # Span both columns\n",
    "        fig.layout.yaxis3.domain = fig.layout.yaxis3.domain # Keep its original y-domain for row 3\n",
    "\n",
    "    else:\n",
    "        print(\"Skipping Panel C: df_merged_diversity is empty or missing columns.\")\n",
    "\n",
    "    # --- Final Layout Updates for the Entire Figure ---\n",
    "    fig.update_layout(\n",
    "        height=1500, # Adjusted height for 3 rows\n",
    "        plot_bgcolor='rgba(0,0,0,0)', paper_bgcolor='rgba(0,0,0,0)',\n",
    "        margin=dict(l=100, r=50, t=120, b=100), # Adjusted margins\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1) # Position legend\n",
    "    )\n",
    "    if 'plotly_layout_defaults' in locals() and plotly_layout_defaults.font:\n",
    "        fig.update_layout(font=plotly_layout_defaults.font)\n",
    "    \n",
    "    # Apply subplot title font styling\n",
    "    for i in range(len(fig.layout.annotations)):\n",
    "        # Check if it's a subplot title (they are added as annotations by make_subplots)\n",
    "        # A common heuristic is that they are bolded or have a specific y anchor\n",
    "        if fig.layout.annotations[i].text.startswith(\"<b>\") :\n",
    "            fig.layout.annotations[i].font.size = 16 # Larger subplot titles\n",
    "            if 'plotly_layout_defaults' in locals() and plotly_layout_defaults.font:\n",
    "                fig.layout.annotations[i].font.family = plotly_layout_defaults.font.family\n",
    "    \n",
    "    # Apply axis styling from plotly_layout_defaults to all axes\n",
    "    if 'plotly_layout_defaults' in locals():\n",
    "        for axis_name_template in ['xaxis', 'yaxis']:\n",
    "            for i in range(1, 4): # For 3 rows\n",
    "                 axis_ref = f\"{axis_name_template}{i if i > 1 else ''}\" # xaxis, xaxis2, xaxis3 etc.\n",
    "                 if hasattr(fig.layout, axis_ref) and hasattr(plotly_layout_defaults, axis_name_template):\n",
    "                     default_axis_style = getattr(plotly_layout_defaults, axis_name_template)\n",
    "                     current_axis = getattr(fig.layout, axis_ref)\n",
    "                     if default_axis_style.title and default_axis_style.title.font:\n",
    "                         current_axis.title.font.size = default_axis_style.title.font.size\n",
    "                         current_axis.title.font.family = default_axis_style.title.font.family\n",
    "                         current_axis.title.font.weight = default_axis_style.title.font.weight\n",
    "                     if default_axis_style.tickfont:\n",
    "                         current_axis.tickfont.size = default_axis_style.tickfont.size\n",
    "                         current_axis.tickfont.family = default_axis_style.tickfont.family\n",
    "\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # --- Save Figure ---\n",
    "    fig_path_html = Path(output_figure_dir) / \"figure12_consolidated_diversity_final.html\"\n",
    "    fig.write_html(str(fig_path_html))\n",
    "    print(f\"Figure 12 (Consolidated Diversity Final) HTML saved to: {fig_path_html}\")\n",
    "    try:\n",
    "        fig_path_pdf = Path(output_figure_dir) / \"figure12_consolidated_diversity_final.pdf\"\n",
    "        fig.write_image(str(fig_path_pdf), width=900, height=1500) # Adjusted width\n",
    "        print(f\"Figure 12 PDF saved to: {fig_path_pdf}\")\n",
    "    except Exception as e: print(f\"Warning: Could not export Figure 12 to PDF. Error: {e}\")\n",
    "    \n",
    "    # Save summary data for heatmap\n",
    "    if heatmap_data_list:\n",
    "        df_heatmap_summary = pd.DataFrame(heatmap_data_list)\n",
    "        heatmap_summary_path = Path(output_summary_dir_phase1) / \"figure12_panel_b_heatmap_data.csv\"\n",
    "        try:\n",
    "            df_heatmap_summary.to_csv(heatmap_summary_path, index=False, float_format='%.3f')\n",
    "            print(f\"Heatmap data for Figure 12 Panel B saved to: {heatmap_summary_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving heatmap data: {e}\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n--- Cell 20 (Figure 12 - Consolidated Diversity Exploration) Complete ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a89364-b63f-4b39-8427-83eafe8611cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20: Figure 12 - Consolidated Diversity Profiles, Functional Enrichment, and Correlations (Revised Panel B & SVG Export)\n",
    "\n",
    "# --- Imports & Setup Assumptions ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from pathlib import Path\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# This cell assumes that pandas (pd), numpy (np), plotly.express (px),\n",
    "# plotly.graph_objects (go), Path from pathlib, make_subplots, spearmanr\n",
    "# and all necessary variables/helper functions from Cell 1 (Setup) are available.\n",
    "# Key variables from previous cells:\n",
    "# - df_merged_diversity: DataFrame from Cell 13 (OG, APSI, Group, Shannon_Entropy, Observed_Richness, Broad_Functional_Category).\n",
    "# - output_summary_dir_phase1: Directory where Cell 15 saved its summary.\n",
    "# - output_figure_dir: Directory to save plots.\n",
    "# - plotly_layout_defaults: Default layout for Plotly figures.\n",
    "# - group_colors, broad_category_color_map: Color maps.\n",
    "# - orthogroup_col, group_col, apsi_col, broad_func_cat_col\n",
    "\n",
    "print(\"\\n\\n--- Generating Figure 12: Consolidated Diversity, Functional Enrichment, & Correlations (Revised Panel B & SVG Export) (Cell 20) ---\")\n",
    "\n",
    "# --- Configuration & Definitions (mirroring Cell 15 & 18 for consistency) ---\n",
    "extreme_ogs_summary_path = Path(output_summary_dir_phase1) / \"figure11_extreme_diversity_ogs_summary_expanded_revised.csv\"\n",
    "\n",
    "if 'n_percentile_strict' not in locals(): n_percentile_strict = 5\n",
    "if 'combo_percentile_relaxed' not in locals(): combo_percentile_relaxed = 25 # From revised Cell 15\n",
    "\n",
    "profiles_for_panel_a = {\n",
    "    f\"Top {n_percentile_strict}% Shannon Entropy\": f\"Top {n_percentile_strict}% Ent.\",\n",
    "    f\"Bottom {n_percentile_strict}% APSI\": f\"Bot. {n_percentile_strict}% APSI\",\n",
    "    \"High Entropy & Low APSI\": \"Hi Ent, Lo APSI\",\n",
    "    \"High Entropy & High APSI\": \"Hi Ent, Hi APSI\",\n",
    "    \"Low Entropy & Low APSI\": \"Lo Ent, Lo APSI\",\n",
    "    \"High Richness & High APSI\": \"Hi Rich, Hi APSI\"\n",
    "}\n",
    "profile_keys_for_panel_a = list(profiles_for_panel_a.keys())\n",
    "profile_labels_for_panel_a = list(profiles_for_panel_a.values())\n",
    "\n",
    "profiles_for_panel_b_heatmap = {\n",
    "    \"High Entropy & High APSI\": \"Hi Ent, Hi APSI\",\n",
    "    \"Low Entropy & Low APSI\": \"Lo Ent, Lo APSI\"\n",
    "}\n",
    "# Define categories to EXCLUDE from the heatmap in Panel B\n",
    "categories_to_exclude_from_heatmap = [\"Other Specific Annotation\", \"General Protein Features\", \"Unknown/Unclassified\"]\n",
    "\n",
    "\n",
    "# --- Load Data ---\n",
    "error_found_loading = False\n",
    "df_extreme_ogs_summary = pd.DataFrame()\n",
    "df_merged_diversity_fig12 = pd.DataFrame()\n",
    "\n",
    "if not extreme_ogs_summary_path.is_file():\n",
    "    print(f\"ERROR: Extreme OGs summary file not found at '{extreme_ogs_summary_path}'. Please run Cell 15 (revised) first.\")\n",
    "    error_found_loading = True\n",
    "else:\n",
    "    try:\n",
    "        df_extreme_ogs_summary = pd.read_csv(extreme_ogs_summary_path)\n",
    "        if not all(k in df_extreme_ogs_summary.columns for k in [orthogroup_col, group_col, 'Reason_for_Highlight', broad_func_cat_col]):\n",
    "            print(f\"ERROR: Loaded extreme OGs summary file is missing critical columns.\")\n",
    "            df_extreme_ogs_summary = pd.DataFrame() \n",
    "            error_found_loading = True\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading extreme OGs summary file: {e}\")\n",
    "        df_extreme_ogs_summary = pd.DataFrame()\n",
    "        error_found_loading = True\n",
    "\n",
    "if 'df_merged_diversity' not in locals() or df_merged_diversity.empty:\n",
    "    print(f\"ERROR: DataFrame 'df_merged_diversity' not found or is empty. Please run Cell 13 first.\")\n",
    "    error_found_loading = True\n",
    "else:\n",
    "    df_merged_diversity_fig12 = df_merged_diversity.copy()\n",
    "    if not all(k in df_merged_diversity_fig12.columns for k in [orthogroup_col, group_col, apsi_col, 'Shannon_Entropy', broad_func_cat_col]):\n",
    "        print(f\"ERROR: 'df_merged_diversity' is missing critical columns for Panels B/C.\")\n",
    "        error_found_loading = True\n",
    "\n",
    "if error_found_loading:\n",
    "    print(\"Cannot proceed with Figure 12 generation due to missing input data.\")\n",
    "else:\n",
    "    print(f\"Data loaded. df_extreme_ogs_summary: {df_extreme_ogs_summary.shape}, df_merged_diversity_fig12: {df_merged_diversity_fig12.shape}\")\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=1, \n",
    "        subplot_titles=(\n",
    "            \"<b>A.</b> Prevalence of OGs in Key Diversity Profiles\",\n",
    "            \"<b>B.</b> Functional Enrichment in 'Surprising' Diversity Profiles\",\n",
    "            \"<b>C.</b> Shannon Entropy vs. Intra-OG APSI\"\n",
    "        ),\n",
    "        vertical_spacing=0.12, \n",
    "        row_heights=[0.3, 0.4, 0.3] \n",
    "    )\n",
    "\n",
    "    # --- Panel A: Prevalence of Key Diversity Profiles ---\n",
    "    total_asgard_ogs = df_merged_diversity_fig12[df_merged_diversity_fig12[group_col] == 'Asgard'][orthogroup_col].nunique()\n",
    "    total_gv_ogs = df_merged_diversity_fig12[df_merged_diversity_fig12[group_col] == 'GV'][orthogroup_col].nunique()\n",
    "    panel_a_plot_data = []\n",
    "    for reason_key, short_label in profiles_for_panel_a.items():\n",
    "        df_matching = df_extreme_ogs_summary[df_extreme_ogs_summary['Reason_for_Highlight'].str.contains(reason_key, regex=False, na=False)]\n",
    "        count_asgard = df_matching[df_matching[group_col] == 'Asgard'][orthogroup_col].nunique()\n",
    "        count_gv = df_matching[df_matching[group_col] == 'GV'][orthogroup_col].nunique()\n",
    "        panel_a_plot_data.append({'Profile': short_label, 'Group': 'Asgard', 'Percentage': (count_asgard / total_asgard_ogs * 100) if total_asgard_ogs > 0 else 0})\n",
    "        panel_a_plot_data.append({'Profile': short_label, 'Group': 'GV', 'Percentage': (count_gv / total_gv_ogs * 100) if total_gv_ogs > 0 else 0})\n",
    "    df_panel_a_plot = pd.DataFrame(panel_a_plot_data)\n",
    "\n",
    "    if not df_panel_a_plot.empty:\n",
    "        fig_a_temp = px.bar(df_panel_a_plot, x='Profile', y='Percentage', color='Group', barmode='group', color_discrete_map=group_colors)\n",
    "        for trace in fig_a_temp.data:\n",
    "            fig.add_trace(trace, row=1, col=1)\n",
    "        fig.update_xaxes(categoryorder='array', categoryarray=profile_labels_for_panel_a, tickangle=35, row=1, col=1) \n",
    "        fig.update_yaxes(title_text=\"% of Total OGs\", range=[0, df_panel_a_plot['Percentage'].max()*1.15 if not df_panel_a_plot.empty else 10], row=1, col=1)\n",
    "        fig.update_layout(legend_title_text='Group', legend=dict(tracegroupgap=5)) \n",
    "    else:\n",
    "        print(\"No data for Panel A.\")\n",
    "\n",
    "    # --- Panel B: Functional Enrichment Heatmap (Revised) ---\n",
    "    heatmap_data_list = []\n",
    "    # Calculate baseline distributions *after* excluding unwanted categories\n",
    "    baseline_func_dist_asgard = df_merged_diversity_fig12[\n",
    "        (df_merged_diversity_fig12[group_col] == 'Asgard') & \n",
    "        (~df_merged_diversity_fig12[broad_func_cat_col].isin(categories_to_exclude_from_heatmap))\n",
    "    ][broad_func_cat_col].value_counts(normalize=True).mul(100)\n",
    "    \n",
    "    baseline_func_dist_gv = df_merged_diversity_fig12[\n",
    "        (df_merged_diversity_fig12[group_col] == 'GV') &\n",
    "        (~df_merged_diversity_fig12[broad_func_cat_col].isin(categories_to_exclude_from_heatmap))\n",
    "    ][broad_func_cat_col].value_counts(normalize=True).mul(100)\n",
    "\n",
    "    for profile_key, short_profile_label in profiles_for_panel_b_heatmap.items():\n",
    "        for group_val in ['Asgard', 'GV']:\n",
    "            # Filter the extreme OGs for the current profile, group, AND exclude unwanted functional categories\n",
    "            df_profile_group_subset = df_extreme_ogs_summary[\n",
    "                df_extreme_ogs_summary['Reason_for_Highlight'].str.contains(profile_key, regex=False, na=False) &\n",
    "                (df_extreme_ogs_summary[group_col] == group_val) &\n",
    "                (~df_extreme_ogs_summary[broad_func_cat_col].isin(categories_to_exclude_from_heatmap)) \n",
    "            ]\n",
    "            if df_profile_group_subset.empty: continue\n",
    "\n",
    "            # Calculate observed distribution for the filtered subset\n",
    "            observed_dist = df_profile_group_subset[broad_func_cat_col].value_counts(normalize=True).mul(100)\n",
    "            baseline_dist = baseline_func_dist_asgard if group_val == 'Asgard' else baseline_func_dist_gv\n",
    "            \n",
    "            # Iterate through functional categories present in the OBSERVED distribution for this profile/group\n",
    "            for func_cat, obs_perc in observed_dist.items():\n",
    "                # Ensure this func_cat is not in the exclusion list (double check, though subset should handle it)\n",
    "                if func_cat in categories_to_exclude_from_heatmap:\n",
    "                    continue\n",
    "                exp_perc = baseline_dist.get(func_cat, 0) # Get expected, default to 0 if func_cat not in baseline (e.g. if it was filtered out)\n",
    "                log2_fold_change = np.log2((obs_perc + 1e-9) / (exp_perc + 1e-9)) # Add small epsilon\n",
    "                heatmap_data_list.append({\n",
    "                    'Functional_Category': func_cat,\n",
    "                    'Profile_Group': f\"{group_val} - {short_profile_label}\",\n",
    "                    'Log2_Fold_Change': log2_fold_change\n",
    "                })\n",
    "    \n",
    "    if heatmap_data_list:\n",
    "        df_heatmap = pd.DataFrame(heatmap_data_list)\n",
    "        if not df_heatmap.empty:\n",
    "            df_heatmap_pivot = df_heatmap.pivot(index='Functional_Category', columns='Profile_Group', values='Log2_Fold_Change').fillna(0)\n",
    "            # Further filter pivot table rows if any excluded categories slipped through (shouldn't if logic above is correct)\n",
    "            df_heatmap_pivot = df_heatmap_pivot[~df_heatmap_pivot.index.isin(categories_to_exclude_from_heatmap)]\n",
    "            df_heatmap_pivot = df_heatmap_pivot.sort_index() \n",
    "            \n",
    "            if not df_heatmap_pivot.empty:\n",
    "                abs_max_val = df_heatmap_pivot.abs().max().max() \n",
    "                z_min = -abs_max_val if abs_max_val > 0 else -1\n",
    "                z_max = abs_max_val if abs_max_val > 0 else 1\n",
    "\n",
    "                fig.add_trace(go.Heatmap(\n",
    "                    z=df_heatmap_pivot.values,\n",
    "                    x=df_heatmap_pivot.columns,\n",
    "                    y=df_heatmap_pivot.index,\n",
    "                    colorscale='RdBu', \n",
    "                    zmid=0, \n",
    "                    zmin=z_min, \n",
    "                    zmax=z_max,\n",
    "                    colorbar=dict(\n",
    "                        title=dict(text='Log2 (Obs/Exp %)', side='right'), # Corrected title attribute\n",
    "                        tickfont=dict(size=8), # Smaller font for colorbar ticks\n",
    "                        len=0.75, y=0.47, yanchor='middle' # Adjust length and position of colorbar if needed\n",
    "                    )\n",
    "                ), row=2, col=1)\n",
    "                fig.update_xaxes(tickangle=20, row=2, col=1, tickfont=dict(size=10)) # Adjusted angle\n",
    "                fig.update_yaxes(categoryorder='array', categoryarray=df_heatmap_pivot.index.tolist(), row=2, col=1, tickfont=dict(size=10))\n",
    "            else:\n",
    "                print(\"Heatmap pivot table is empty after filtering and pivoting for Panel B.\")\n",
    "        else:\n",
    "            print(\"No data for Panel B heatmap after processing (df_heatmap was empty).\")\n",
    "    else:\n",
    "        print(\"No data for Panel B heatmap initially (heatmap_data_list was empty).\")\n",
    "\n",
    "    # --- Panel C: Shannon Entropy vs. APSI with Correlation Coefficients ---\n",
    "    if not df_merged_diversity_fig12.empty and 'Shannon_Entropy' in df_merged_diversity_fig12.columns and apsi_col in df_merged_diversity_fig12.columns:\n",
    "        df_panel_c_plot = df_merged_diversity_fig12.copy()\n",
    "        df_panel_c_plot['APSI_Percent_Plot'] = df_panel_c_plot[apsi_col] * 100\n",
    "        \n",
    "        annotation_y_start_c = 0.95\n",
    "        annotation_y_step_c = 0.08 \n",
    "        \n",
    "        for group_name_panel_c, group_df_panel_c in df_panel_c_plot.groupby(group_col):\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=group_df_panel_c['APSI_Percent_Plot'],\n",
    "                y=group_df_panel_c['Shannon_Entropy'],\n",
    "                mode='markers', name=f\"{group_name_panel_c} (Panel C)\", legendgroup=group_name_panel_c, \n",
    "                marker=dict(color=group_colors.get(group_name_panel_c), opacity=0.5, size=4)\n",
    "            ), row=3, col=1)\n",
    "\n",
    "            df_corr_subset_c = group_df_panel_c[['APSI_Percent_Plot', 'Shannon_Entropy']].dropna()\n",
    "            if len(df_corr_subset_c) > 2:\n",
    "                rho, p_val = spearmanr(df_corr_subset_c['APSI_Percent_Plot'], df_corr_subset_c['Shannon_Entropy'])\n",
    "                fig.add_annotation(\n",
    "                    xref=\"x3 domain\", yref=\"y3 domain\", x=0.02, y=annotation_y_start_c, \n",
    "                    text=f\"{group_name_panel_c}: ρ={rho:.2f} (p={p_val:.2g})\", showarrow=False,\n",
    "                    font=dict(size=10, color=group_colors.get(group_name_panel_c)), align=\"left\", xanchor=\"left\"\n",
    "                )\n",
    "                annotation_y_start_c -= annotation_y_step_c\n",
    "        \n",
    "        fig.update_xaxes(title_text=\"Intra-OG APSI (%)\", row=3, col=1)\n",
    "        fig.update_yaxes(title_text=\"Shannon Entropy\", row=3, col=1)\n",
    "    else:\n",
    "        print(\"Skipping Panel C: df_merged_diversity is empty or missing columns.\")\n",
    "\n",
    "    # --- Final Layout Updates for the Entire Figure ---\n",
    "    fig.update_layout(\n",
    "        height=1400, \n",
    "        plot_bgcolor='rgba(0,0,0,0)', paper_bgcolor='rgba(0,0,0,0)',\n",
    "        margin=dict(l=120, r=50, t=100, b=120), \n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1) \n",
    "    )\n",
    "    if 'plotly_layout_defaults' in locals() and plotly_layout_defaults.font:\n",
    "        fig.update_layout(font=plotly_layout_defaults.font)\n",
    "    \n",
    "    for i in range(len(fig.layout.annotations)):\n",
    "        if fig.layout.annotations[i].text.startswith(\"<b>\"): \n",
    "            fig.layout.annotations[i].font.size = 14 \n",
    "            if 'plotly_layout_defaults' in locals() and plotly_layout_defaults.font:\n",
    "                fig.layout.annotations[i].font.family = plotly_layout_defaults.font.family\n",
    "    \n",
    "    if 'plotly_layout_defaults' in locals():\n",
    "        for axis_name_template in ['xaxis', 'yaxis']:\n",
    "            for i in range(1, 4): \n",
    "                 axis_ref = f\"{axis_name_template}{i if i > 1 else ''}\" \n",
    "                 if hasattr(fig.layout, axis_ref) and hasattr(plotly_layout_defaults, axis_name_template):\n",
    "                     default_axis_style = getattr(plotly_layout_defaults, axis_name_template)\n",
    "                     current_axis = getattr(fig.layout, axis_ref)\n",
    "                     if default_axis_style.title and default_axis_style.title.font:\n",
    "                         current_axis.title.font.size = default_axis_style.title.font.size if current_axis.title.text else 10 \n",
    "                         current_axis.title.font.family = default_axis_style.title.font.family\n",
    "                         current_axis.title.font.weight = default_axis_style.title.font.weight\n",
    "                     if default_axis_style.tickfont:\n",
    "                         current_axis.tickfont.size = default_axis_style.tickfont.size\n",
    "                         current_axis.tickfont.family = default_axis_style.tickfont.family\n",
    "                     current_axis.showgrid = False\n",
    "                     current_axis.zeroline = False\n",
    "                     current_axis.linecolor = 'black'\n",
    "                     current_axis.linewidth = 1\n",
    "                     current_axis.ticks=\"outside\"\n",
    "                     current_axis.ticklen=5\n",
    "                     current_axis.tickwidth=1\n",
    "                     current_axis.tickcolor='black'\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # --- Save Figure ---\n",
    "    fig_path_html = Path(output_figure_dir) / \"figure12_consolidated_diversity_final_revised.html\"\n",
    "    fig.write_html(str(fig_path_html))\n",
    "    print(f\"Figure 12 (Consolidated Diversity Final Revised) HTML saved to: {fig_path_html}\")\n",
    "    try:\n",
    "        fig_path_pdf = Path(output_figure_dir) / \"figure12_consolidated_diversity_final_revised.pdf\"\n",
    "        fig.write_image(str(fig_path_pdf), width=850, height=1400) \n",
    "        print(f\"Figure 12 PDF saved to: {fig_path_pdf}\")\n",
    "    except Exception as e: print(f\"Warning: Could not export Figure 12 to PDF. Error: {e}\")\n",
    "    # Ensure SVG export is present\n",
    "    try:\n",
    "        fig_path_svg = Path(output_figure_dir) / \"figure12_consolidated_diversity_final_revised.svg\"\n",
    "        fig.write_image(str(fig_path_svg), width=850, height=1400)\n",
    "        print(f\"Figure 12 SVG saved to: {fig_path_svg}\")\n",
    "    except Exception as e: print(f\"Warning: Could not export Figure 12 to SVG. Error: {e}\")\n",
    "\n",
    "    \n",
    "    if heatmap_data_list:\n",
    "        df_heatmap_summary = pd.DataFrame(heatmap_data_list)\n",
    "        heatmap_summary_path = Path(output_summary_dir_phase1) / \"figure12_panel_b_heatmap_data_revised.csv\"\n",
    "        try:\n",
    "            df_heatmap_summary.to_csv(heatmap_summary_path, index=False, float_format='%.3f')\n",
    "            print(f\"Heatmap data for Figure 12 Panel B (revised) saved to: {heatmap_summary_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving revised heatmap data: {e}\")\n",
    "\n",
    "print(\"\\n\\n--- Cell 20 (Figure 12 - Consolidated Diversity Exploration - Revised) Complete ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc70233-9a30-4b69-9c69-1496e218d71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 21: Figure 13 - Characterizing Eukaryotic Hits for Asgard and Giant Virus Proteins\n",
    "\n",
    "# --- Imports & Setup Assumptions ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from pathlib import Path\n",
    "\n",
    "# This cell assumes that pandas (pd), numpy (np), plotly.express (px),\n",
    "# plotly.graph_objects (go), Path from pathlib, make_subplots,\n",
    "# and all necessary variables/helper functions from Cell 1 (Setup) are available.\n",
    "# Key variables from Cell 1:\n",
    "# - output_figure_dir, output_summary_dir_phase1: Directories to save plots/data.\n",
    "# - plotly_layout_defaults: Default layout for Plotly figures.\n",
    "# - arcadia_primary_palette, arcadia_secondary_palette: Color palettes.\n",
    "# - group_col, protein_id_col,\n",
    "# - Has_Euk_DIAMOND_Hit, Euk_Hit_Organism, Euk_Hit_Protein_Name (column names)\n",
    "# - clean_protein_name (helper function from Cell 1)\n",
    "\n",
    "print(\"\\n\\n--- Generating Figure 13: Characterizing Eukaryotic Hits (Cell 21) ---\")\n",
    "\n",
    "# --- Configuration ---\n",
    "DB_PATH_V2_4 = 'proteome_database_v2.4_gv_euk_hits.csv' # INPUT: Database with GV Euk Hits\n",
    "TOP_N_ORGANISMS = 25  # Number of top organisms to display in Panels A & B\n",
    "TOP_N_PROTEIN_NAMES = 30 # Number of top protein names for Panel C\n",
    "\n",
    "# --- Load Data ---\n",
    "error_found_loading = False\n",
    "df_db_v2_4 = pd.DataFrame()\n",
    "\n",
    "if not Path(DB_PATH_V2_4).is_file():\n",
    "    print(f\"ERROR: Database file '{DB_PATH_V2_4}' not found. Please ensure it was created correctly.\")\n",
    "    error_found_loading = True\n",
    "else:\n",
    "    try:\n",
    "        df_db_v2_4 = pd.read_csv(DB_PATH_V2_4, low_memory=False)\n",
    "        # Ensure key columns exist\n",
    "        required_cols = [group_col, 'Has_Euk_DIAMOND_Hit', 'Euk_Hit_Organism', 'Euk_Hit_Protein_Name', protein_id_col]\n",
    "        if not all(col in df_db_v2_4.columns for col in required_cols):\n",
    "            missing = [col for col in required_cols if col not in df_db_v2_4.columns]\n",
    "            print(f\"ERROR: Loaded database '{DB_PATH_V2_4}' is missing critical columns: {missing}\")\n",
    "            error_found_loading = True\n",
    "        else:\n",
    "            print(f\"Successfully loaded database '{DB_PATH_V2_4}'. Shape: {df_db_v2_4.shape}\")\n",
    "            # Data type conversions for safety\n",
    "            df_db_v2_4['Has_Euk_DIAMOND_Hit'] = df_db_v2_4['Has_Euk_DIAMOND_Hit'].fillna(False).astype(bool)\n",
    "            df_db_v2_4['Euk_Hit_Organism'] = df_db_v2_4['Euk_Hit_Organism'].fillna('Unknown').astype(str)\n",
    "            df_db_v2_4['Euk_Hit_Protein_Name'] = df_db_v2_4['Euk_Hit_Protein_Name'].fillna('Unknown').astype(str)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading database '{DB_PATH_V2_4}': {e}\")\n",
    "        error_found_loading = True\n",
    "\n",
    "if error_found_loading:\n",
    "    print(\"Cannot proceed with Figure 13 generation due to missing input data.\")\n",
    "else:\n",
    "    # --- Create Figure with Subplots ---\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=1,\n",
    "        subplot_titles=(\n",
    "            \"<b>A.</b> Top Eukaryotic Organisms Hit by Asgard Proteins\",\n",
    "            \"<b>B.</b> Top Eukaryotic Organisms Hit by Giant Virus Proteins\",\n",
    "            \"<b>C.</b> Top Eukaryotic Protein Functions Hit by Giant Virus Proteins\"\n",
    "        ),\n",
    "        vertical_spacing=0.1 # Adjust spacing\n",
    "    )\n",
    "\n",
    "    # --- Panel A: Top Eukaryotic Organisms Hit by Asgard Proteins ---\n",
    "    print(\"\\n--- Generating Panel A: Asgard Eukaryotic Hit Organisms ---\")\n",
    "    df_asgard_hits_panel_a = df_db_v2_4[\n",
    "        (df_db_v2_4[group_col] == 'Asgard') &\n",
    "        (df_db_v2_4['Has_Euk_DIAMOND_Hit'] == True)\n",
    "    ].copy()\n",
    "\n",
    "    if not df_asgard_hits_panel_a.empty:\n",
    "        asgard_org_counts = df_asgard_hits_panel_a['Euk_Hit_Organism'].value_counts().nlargest(TOP_N_ORGANISMS).reset_index()\n",
    "        asgard_org_counts.columns = ['Euk_Hit_Organism', 'Count']\n",
    "        \n",
    "        if not asgard_org_counts.empty:\n",
    "            fig.add_trace(go.Bar(\n",
    "                x=asgard_org_counts['Euk_Hit_Organism'],\n",
    "                y=asgard_org_counts['Count'],\n",
    "                name='Asgard Hits',\n",
    "                marker_color=arcadia_primary_palette[0 % len(arcadia_primary_palette)]\n",
    "            ), row=1, col=1)\n",
    "            fig.update_xaxes(title_text=\"\", tickangle=45, categoryorder='total descending', row=1, col=1)\n",
    "            fig.update_yaxes(title_text=\"Number of Asgard Proteins\", row=1, col=1)\n",
    "            \n",
    "            # Save summary data for Panel A\n",
    "            panel_a_data_path = Path(output_summary_dir_phase1) / \"figure13_panel_a_asgard_euk_org_hits.csv\"\n",
    "            try:\n",
    "                df_asgard_hits_panel_a['Euk_Hit_Organism'].value_counts().reset_index().to_csv(panel_a_data_path, index=False)\n",
    "                print(f\"Data for Panel A saved to {panel_a_data_path}\")\n",
    "            except Exception as e: print(f\"Error saving Panel A data: {e}\")\n",
    "        else:\n",
    "            print(\"No eukaryotic hits found for Asgard proteins to plot for Panel A.\")\n",
    "    else:\n",
    "        print(\"No Asgard proteins with eukaryotic hits found for Panel A.\")\n",
    "\n",
    "    # --- Panel B: Top Eukaryotic Organisms Hit by Giant Virus Proteins ---\n",
    "    print(\"\\n--- Generating Panel B: Giant Virus Eukaryotic Hit Organisms ---\")\n",
    "    df_gv_hits_panel_b = df_db_v2_4[\n",
    "        (df_db_v2_4[group_col] == 'GV') &  # Make sure 'GV' is the correct value in your 'Group' column\n",
    "        (df_db_v2_4['Has_Euk_DIAMOND_Hit'] == True)\n",
    "    ].copy()\n",
    "\n",
    "    if not df_gv_hits_panel_b.empty:\n",
    "        gv_org_counts = df_gv_hits_panel_b['Euk_Hit_Organism'].value_counts().nlargest(TOP_N_ORGANISMS).reset_index()\n",
    "        gv_org_counts.columns = ['Euk_Hit_Organism', 'Count']\n",
    "\n",
    "        if not gv_org_counts.empty:\n",
    "            fig.add_trace(go.Bar(\n",
    "                x=gv_org_counts['Euk_Hit_Organism'],\n",
    "                y=gv_org_counts['Count'],\n",
    "                name='GV Hits',\n",
    "                marker_color=arcadia_primary_palette[1 % len(arcadia_primary_palette)]\n",
    "            ), row=2, col=1)\n",
    "            fig.update_xaxes(title_text=\"\", tickangle=45, categoryorder='total descending', row=2, col=1)\n",
    "            fig.update_yaxes(title_text=\"Number of GV Proteins\", row=2, col=1)\n",
    "\n",
    "            # Save summary data for Panel B\n",
    "            panel_b_data_path = Path(output_summary_dir_phase1) / \"figure13_panel_b_gv_euk_org_hits.csv\"\n",
    "            try:\n",
    "                df_gv_hits_panel_b['Euk_Hit_Organism'].value_counts().reset_index().to_csv(panel_b_data_path, index=False)\n",
    "                print(f\"Data for Panel B saved to {panel_b_data_path}\")\n",
    "            except Exception as e: print(f\"Error saving Panel B data: {e}\")\n",
    "        else:\n",
    "            print(\"No eukaryotic hits found for GV proteins to plot for Panel B.\")\n",
    "    else:\n",
    "        print(\"No GV proteins with eukaryotic hits found for Panel B.\")\n",
    "\n",
    "    # --- Panel C: Top Eukaryotic Protein Functions Hit by Giant Virus Proteins ---\n",
    "    print(\"\\n--- Generating Panel C: Giant Virus Eukaryotic Hit Protein Functions ---\")\n",
    "    if not df_gv_hits_panel_b.empty: # Reuse df_gv_hits_panel_b from above\n",
    "        if 'clean_protein_name' in locals() and callable(clean_protein_name):\n",
    "            df_gv_hits_panel_b['Cleaned_Euk_Hit_Protein_Name'] = df_gv_hits_panel_b['Euk_Hit_Protein_Name'].apply(clean_protein_name)\n",
    "        else:\n",
    "            print(\"WARNING: 'clean_protein_name' function not found. Using raw Euk_Hit_Protein_Name for Panel C.\")\n",
    "            df_gv_hits_panel_b['Cleaned_Euk_Hit_Protein_Name'] = df_gv_hits_panel_b['Euk_Hit_Protein_Name']\n",
    "\n",
    "        gv_prot_name_counts = df_gv_hits_panel_b['Cleaned_Euk_Hit_Protein_Name'].value_counts().nlargest(TOP_N_PROTEIN_NAMES).reset_index()\n",
    "        gv_prot_name_counts.columns = ['Cleaned_Euk_Hit_Protein_Name', 'Count']\n",
    "        \n",
    "        # Filter out very generic names if they dominate, e.g., \"Hypothetical protein\"\n",
    "        gv_prot_name_counts = gv_prot_name_counts[\n",
    "            ~gv_prot_name_counts['Cleaned_Euk_Hit_Protein_Name'].str.contains(\"Hypothetical|Unknown|Uncharacterized\", case=False, na=False)\n",
    "        ]\n",
    "\n",
    "\n",
    "        if not gv_prot_name_counts.empty:\n",
    "            fig.add_trace(go.Bar(\n",
    "                x=gv_prot_name_counts['Cleaned_Euk_Hit_Protein_Name'],\n",
    "                y=gv_prot_name_counts['Count'],\n",
    "                name='GV Hit Protein Functions',\n",
    "                marker_color=arcadia_secondary_palette[0 % len(arcadia_secondary_palette)]\n",
    "            ), row=3, col=1)\n",
    "            fig.update_xaxes(title_text=\"Eukaryotic Protein Function (Cleaned Name)\", tickangle=45, categoryorder='total descending', row=3, col=1)\n",
    "            fig.update_yaxes(title_text=\"Number of GV Protein Hits\", row=3, col=1)\n",
    "            \n",
    "            # Save summary data for Panel C\n",
    "            panel_c_data_path = Path(output_summary_dir_phase1) / \"figure13_panel_c_gv_euk_prot_func_hits.csv\"\n",
    "            try:\n",
    "                # Save the full list of cleaned names and counts before nlargest\n",
    "                full_gv_prot_name_counts = df_gv_hits_panel_b['Cleaned_Euk_Hit_Protein_Name'].value_counts().reset_index()\n",
    "                full_gv_prot_name_counts.columns = ['Cleaned_Euk_Hit_Protein_Name', 'Count']\n",
    "                full_gv_prot_name_counts.to_csv(panel_c_data_path, index=False)\n",
    "                print(f\"Data for Panel C saved to {panel_c_data_path}\")\n",
    "            except Exception as e: print(f\"Error saving Panel C data: {e}\")\n",
    "        else:\n",
    "            print(\"No eukaryotic protein names found for GV hits to plot for Panel C after filtering generic names.\")\n",
    "            \n",
    "    else:\n",
    "        print(\"No GV proteins with eukaryotic hits found for Panel C analysis.\")\n",
    "\n",
    "\n",
    "    # --- Final Layout Updates for the Entire Figure ---\n",
    "    fig.update_layout(\n",
    "        height=1300, # Adjusted height for 3 panels\n",
    "        plot_bgcolor='rgba(0,0,0,0)', \n",
    "        paper_bgcolor='rgba(0,0,0,0)',\n",
    "        margin=dict(l=80, r=50, t=100, b=200), # Increased bottom margin for angled labels\n",
    "        showlegend=False # Individual traces are self-explanatory by panel title\n",
    "    )\n",
    "    # Apply global font and subplot title styling from plotly_layout_defaults\n",
    "    if 'plotly_layout_defaults' in locals():\n",
    "        fig.update_layout(font=plotly_layout_defaults.font)\n",
    "        for i in range(len(fig.layout.annotations)): # Subplot titles\n",
    "            if fig.layout.annotations[i].text.startswith(\"<b>\"): \n",
    "                fig.layout.annotations[i].font.size = 14\n",
    "                if plotly_layout_defaults.font:\n",
    "                    fig.layout.annotations[i].font.family = plotly_layout_defaults.font.family\n",
    "    \n",
    "    # Apply common axis styling from plotly_layout_defaults to all axes\n",
    "    if 'plotly_layout_defaults' in locals():\n",
    "        for axis_name_template in ['xaxis', 'yaxis']:\n",
    "            for i in range(1, 4): # For 3 rows\n",
    "                 axis_ref_name = f\"{axis_name_template}{i if i > 1 else ''}\" # e.g., xaxis, yaxis, xaxis2, yaxis2...\n",
    "                 if hasattr(fig.layout, axis_ref_name) and hasattr(plotly_layout_defaults, axis_name_template):\n",
    "                     default_axis_style = getattr(plotly_layout_defaults, axis_name_template)\n",
    "                     current_axis = getattr(fig.layout, axis_ref_name)\n",
    "                     \n",
    "                     # Apply title font if title text exists for this specific axis\n",
    "                     if current_axis.title and current_axis.title.text and default_axis_style.title and default_axis_style.title.font:\n",
    "                         current_axis.title.font.size = default_axis_style.title.font.size\n",
    "                         current_axis.title.font.family = default_axis_style.title.font.family\n",
    "                         current_axis.title.font.weight = default_axis_style.title.font.weight\n",
    "                     elif default_axis_style.title and default_axis_style.title.font: # Apply to default title if no text\n",
    "                         current_axis.title.font.size = default_axis_style.title.font.size\n",
    "                         current_axis.title.font.family = default_axis_style.title.font.family\n",
    "                         current_axis.title.font.weight = default_axis_style.title.font.weight\n",
    "\n",
    "\n",
    "                     if default_axis_style.tickfont:\n",
    "                         current_axis.tickfont.size = default_axis_style.tickfont.size\n",
    "                         current_axis.tickfont.family = default_axis_style.tickfont.family\n",
    "                     \n",
    "                     current_axis.showgrid = False\n",
    "                     current_axis.zeroline = False\n",
    "                     current_axis.linecolor = 'black'\n",
    "                     current_axis.linewidth = 1.5 # Match your preferred style\n",
    "                     current_axis.ticks=\"outside\"\n",
    "                     current_axis.ticklen=5\n",
    "                     current_axis.tickwidth=1.5\n",
    "                     current_axis.tickcolor='black'\n",
    "    fig.show()\n",
    "\n",
    "    # --- Save Figure ---\n",
    "    fig_path_html = Path(output_figure_dir) / \"figure13_eukaryotic_hit_characterization.html\"\n",
    "    fig.write_html(str(fig_path_html))\n",
    "    print(f\"Figure 13 (Eukaryotic Hit Characterization) HTML saved to: {fig_path_html}\")\n",
    "    try:\n",
    "        fig_path_pdf = Path(output_figure_dir) / \"figure13_eukaryotic_hit_characterization.pdf\"\n",
    "        fig.write_image(str(fig_path_pdf), width=900, height=1300)\n",
    "        print(f\"Figure 13 PDF saved to: {fig_path_pdf}\")\n",
    "    except Exception as e: print(f\"Warning: Could not export Figure 13 to PDF. Error: {e}\")\n",
    "    try:\n",
    "        fig_path_svg = Path(output_figure_dir) / \"figure13_eukaryotic_hit_characterization.svg\"\n",
    "        fig.write_image(str(fig_path_svg), width=900, height=1300)\n",
    "        print(f\"Figure 13 SVG saved to: {fig_path_svg}\")\n",
    "    except Exception as e: print(f\"Warning: Could not export Figure 13 to SVG. Error: {e}\")\n",
    "\n",
    "print(\"\\n\\n--- Cell 21 (Figure 13 - Eukaryotic Hit Characterization) Complete ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aa22d5-eae7-4ddc-99cc-b5f17e2e0155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 21: Figure 13 - Characterizing Eukaryotic Hits for Asgard and Giant Virus Proteins (Consistent Colors)\n",
    "\n",
    "# --- Imports & Setup Assumptions ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from pathlib import Path\n",
    "\n",
    "# This cell assumes that pandas (pd), numpy (np), plotly.express (px),\n",
    "# plotly.graph_objects (go), Path from pathlib, make_subplots,\n",
    "# and all necessary variables/helper functions from Cell 1 (Setup) are available.\n",
    "# Key variables from Cell 1:\n",
    "# - output_figure_dir, output_summary_dir_phase1: Directories to save plots/data.\n",
    "# - plotly_layout_defaults: Default layout for Plotly figures.\n",
    "# - arcadia_primary_palette, arcadia_secondary_palette: Color palettes.\n",
    "# - group_col, protein_id_col,\n",
    "# - 'Has_Euk_DIAMOND_Hit', 'Euk_Hit_Organism', 'Euk_Hit_Protein_Name' (column names)\n",
    "# - clean_protein_name (helper function from Cell 1)\n",
    "\n",
    "print(\"\\n\\n--- Generating Figure 13: Characterizing Eukaryotic Hits (Consistent Colors) (Cell 21) ---\")\n",
    "\n",
    "# --- Configuration ---\n",
    "DB_PATH_V2_4 = 'proteome_database_v2.5.csv' # INPUT: Database with GV Euk Hits\n",
    "TOP_N_ORGANISMS = 25\n",
    "TOP_N_PROTEIN_NAMES = 30\n",
    "\n",
    "# --- Load Data ---\n",
    "error_found_loading = False\n",
    "df_db_v2_4 = pd.DataFrame()\n",
    "\n",
    "if not Path(DB_PATH_V2_4).is_file():\n",
    "    print(f\"ERROR: Database file '{DB_PATH_V2_4}' not found. Please ensure it was created correctly.\")\n",
    "    error_found_loading = True\n",
    "else:\n",
    "    try:\n",
    "        df_db_v2_4 = pd.read_csv(DB_PATH_V2_4, low_memory=False)\n",
    "        required_cols = [group_col, 'Has_Euk_DIAMOND_Hit', 'Euk_Hit_Organism', 'Euk_Hit_Protein_Name', protein_id_col]\n",
    "        if not all(col in df_db_v2_4.columns for col in required_cols):\n",
    "            missing = [col for col in required_cols if col not in df_db_v2_4.columns]\n",
    "            print(f\"ERROR: Loaded database '{DB_PATH_V2_4}' is missing critical columns: {missing}\")\n",
    "            error_found_loading = True\n",
    "        else:\n",
    "            print(f\"Successfully loaded database '{DB_PATH_V2_4}'. Shape: {df_db_v2_4.shape}\")\n",
    "            df_db_v2_4['Has_Euk_DIAMOND_Hit'] = df_db_v2_4['Has_Euk_DIAMOND_Hit'].fillna(False).astype(bool)\n",
    "            df_db_v2_4['Euk_Hit_Organism'] = df_db_v2_4['Euk_Hit_Organism'].fillna('Unknown').astype(str).str.strip()\n",
    "            df_db_v2_4['Euk_Hit_Protein_Name'] = df_db_v2_4['Euk_Hit_Protein_Name'].fillna('Unknown').astype(str)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading database '{DB_PATH_V2_4}': {e}\")\n",
    "        error_found_loading = True\n",
    "\n",
    "if error_found_loading:\n",
    "    print(\"Cannot proceed with Figure 13 generation due to missing input data.\")\n",
    "else:\n",
    "    # --- Prepare Data for Panels A and B ---\n",
    "    df_asgard_hits_panel_a = df_db_v2_4[\n",
    "        (df_db_v2_4[group_col] == 'Asgard') &\n",
    "        (df_db_v2_4['Has_Euk_DIAMOND_Hit'] == True)\n",
    "    ].copy()\n",
    "    \n",
    "    df_gv_hits_panel_b = df_db_v2_4[\n",
    "        (df_db_v2_4.get(group_col) == 'GV') & # Use .get for safety if group_col might be missing\n",
    "        (df_db_v2_4['Has_Euk_DIAMOND_Hit'] == True)\n",
    "    ].copy()\n",
    "\n",
    "    # --- Create a Consistent Color Map for Eukaryotic Organisms ---\n",
    "    print(\"\\n--- Creating Consistent Color Map for Eukaryotic Organisms ---\")\n",
    "    top_asgard_orgs_list = []\n",
    "    if not df_asgard_hits_panel_a.empty:\n",
    "        top_asgard_orgs_list = df_asgard_hits_panel_a['Euk_Hit_Organism'].value_counts().nlargest(TOP_N_ORGANISMS).index.tolist()\n",
    "\n",
    "    top_gv_orgs_list = []\n",
    "    if not df_gv_hits_panel_b.empty:\n",
    "        top_gv_orgs_list = df_gv_hits_panel_b['Euk_Hit_Organism'].value_counts().nlargest(TOP_N_ORGANISMS).index.tolist()\n",
    "\n",
    "    # Combine and get unique organisms from the top lists of both groups\n",
    "    all_top_organisms = sorted(list(set(top_asgard_orgs_list + top_gv_orgs_list) - {'Unknown'}))\n",
    "    \n",
    "    euk_organism_color_map = {}\n",
    "    # Use a combined palette for more variety if many unique organisms\n",
    "    combined_palette = arcadia_primary_palette + arcadia_secondary_palette + arcadia_neutrals_palette \n",
    "    # Ensure 'Unknown' gets a specific color if it appears\n",
    "    euk_organism_color_map['Unknown'] = '#cccccc' # A neutral gray for Unknown\n",
    "\n",
    "    for i, org_name in enumerate(all_top_organisms):\n",
    "        euk_organism_color_map[org_name] = combined_palette[i % len(combined_palette)]\n",
    "    \n",
    "    print(f\"Created color map for {len(euk_organism_color_map)} unique eukaryotic organisms (including Unknown).\")\n",
    "\n",
    "    # --- Create Figure with Subplots ---\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=1,\n",
    "        subplot_titles=(\n",
    "            \"<b>A.</b> Top Eukaryotic Organisms Hit by Asgard Proteins\",\n",
    "            \"<b>B.</b> Top Eukaryotic Organisms Hit by Giant Virus Proteins\",\n",
    "            \"<b>C.</b> Top Eukaryotic Protein Functions Hit by Giant Virus Proteins\"\n",
    "        ),\n",
    "        vertical_spacing=0.1 \n",
    "    )\n",
    "\n",
    "    # --- Panel A: Top Eukaryotic Organisms Hit by Asgard Proteins ---\n",
    "    print(\"\\n--- Generating Panel A: Asgard Eukaryotic Hit Organisms ---\")\n",
    "    if not df_asgard_hits_panel_a.empty:\n",
    "        asgard_org_counts = df_asgard_hits_panel_a['Euk_Hit_Organism'].value_counts().nlargest(TOP_N_ORGANISMS).reset_index()\n",
    "        asgard_org_counts.columns = ['Euk_Hit_Organism', 'Count']\n",
    "        \n",
    "        if not asgard_org_counts.empty:\n",
    "            fig.add_trace(go.Bar(\n",
    "                x=asgard_org_counts['Euk_Hit_Organism'],\n",
    "                y=asgard_org_counts['Count'],\n",
    "                name='Asgard Hits', # Will not be shown in legend if showlegend=False for the trace\n",
    "                marker_color=[euk_organism_color_map.get(org, '#999999') for org in asgard_org_counts['Euk_Hit_Organism']] # Apply consistent map\n",
    "            ), row=1, col=1)\n",
    "            fig.update_xaxes(title_text=\"\", tickangle=45, categoryorder='total descending', row=1, col=1)\n",
    "            fig.update_yaxes(title_text=\"Number of Asgard Proteins\", row=1, col=1)\n",
    "            \n",
    "            panel_a_data_path = Path(output_summary_dir_phase1) / \"figure13_panel_a_asgard_euk_org_hits.csv\"\n",
    "            try:\n",
    "                df_asgard_hits_panel_a['Euk_Hit_Organism'].value_counts().reset_index().to_csv(panel_a_data_path, index=False)\n",
    "                print(f\"Data for Panel A saved to {panel_a_data_path}\")\n",
    "            except Exception as e: print(f\"Error saving Panel A data: {e}\")\n",
    "        else:\n",
    "            print(\"No eukaryotic hits found for Asgard proteins to plot for Panel A.\")\n",
    "    else:\n",
    "        print(\"No Asgard proteins with eukaryotic hits found for Panel A.\")\n",
    "\n",
    "    # --- Panel B: Top Eukaryotic Organisms Hit by Giant Virus Proteins ---\n",
    "    print(\"\\n--- Generating Panel B: Giant Virus Eukaryotic Hit Organisms ---\")\n",
    "    if not df_gv_hits_panel_b.empty:\n",
    "        gv_org_counts = df_gv_hits_panel_b['Euk_Hit_Organism'].value_counts().nlargest(TOP_N_ORGANISMS).reset_index()\n",
    "        gv_org_counts.columns = ['Euk_Hit_Organism', 'Count']\n",
    "\n",
    "        if not gv_org_counts.empty:\n",
    "            fig.add_trace(go.Bar(\n",
    "                x=gv_org_counts['Euk_Hit_Organism'],\n",
    "                y=gv_org_counts['Count'],\n",
    "                name='GV Hits',\n",
    "                marker_color=[euk_organism_color_map.get(org, '#999999') for org in gv_org_counts['Euk_Hit_Organism']] # Apply consistent map\n",
    "            ), row=2, col=1)\n",
    "            fig.update_xaxes(title_text=\"\", tickangle=45, categoryorder='total descending', row=2, col=1)\n",
    "            fig.update_yaxes(title_text=\"Number of GV Proteins\", row=2, col=1)\n",
    "\n",
    "            panel_b_data_path = Path(output_summary_dir_phase1) / \"figure13_panel_b_gv_euk_org_hits.csv\"\n",
    "            try:\n",
    "                df_gv_hits_panel_b['Euk_Hit_Organism'].value_counts().reset_index().to_csv(panel_b_data_path, index=False)\n",
    "                print(f\"Data for Panel B saved to {panel_b_data_path}\")\n",
    "            except Exception as e: print(f\"Error saving Panel B data: {e}\")\n",
    "        else:\n",
    "            print(\"No eukaryotic hits found for GV proteins to plot for Panel B.\")\n",
    "    else:\n",
    "        print(\"No GV proteins with eukaryotic hits found for Panel B.\")\n",
    "\n",
    "    # --- Panel C: Top Eukaryotic Protein Functions Hit by Giant Virus Proteins ---\n",
    "    print(\"\\n--- Generating Panel C: Giant Virus Eukaryotic Hit Protein Functions ---\")\n",
    "    if not df_gv_hits_panel_b.empty: \n",
    "        if 'clean_protein_name' in locals() and callable(clean_protein_name):\n",
    "            df_gv_hits_panel_b['Cleaned_Euk_Hit_Protein_Name'] = df_gv_hits_panel_b['Euk_Hit_Protein_Name'].apply(clean_protein_name)\n",
    "        else:\n",
    "            print(\"WARNING: 'clean_protein_name' function not found. Using raw Euk_Hit_Protein_Name for Panel C.\")\n",
    "            df_gv_hits_panel_b['Cleaned_Euk_Hit_Protein_Name'] = df_gv_hits_panel_b['Euk_Hit_Protein_Name']\n",
    "\n",
    "        gv_prot_name_counts_all = df_gv_hits_panel_b['Cleaned_Euk_Hit_Protein_Name'].value_counts()\n",
    "        # Filter out very generic names AFTER counting, before selecting top N for plot\n",
    "        gv_prot_name_counts_filtered = gv_prot_name_counts_all[\n",
    "            ~gv_prot_name_counts_all.index.astype(str).str.contains(\"Hypothetical|Unknown|Uncharacterized|^nan$\", case=False, na=False, regex=True)\n",
    "        ]\n",
    "        gv_prot_name_counts_plot = gv_prot_name_counts_filtered.nlargest(TOP_N_PROTEIN_NAMES).reset_index()\n",
    "        gv_prot_name_counts_plot.columns = ['Cleaned_Euk_Hit_Protein_Name', 'Count']\n",
    "        \n",
    "        if not gv_prot_name_counts_plot.empty:\n",
    "            fig.add_trace(go.Bar(\n",
    "                x=gv_prot_name_counts_plot['Cleaned_Euk_Hit_Protein_Name'],\n",
    "                y=gv_prot_name_counts_plot['Count'],\n",
    "                name='GV Hit Protein Functions',\n",
    "                marker_color=arcadia_secondary_palette[0 % len(arcadia_secondary_palette)] # Different palette for this panel\n",
    "            ), row=3, col=1)\n",
    "            fig.update_xaxes(title_text=\"Eukaryotic Protein Function (Cleaned Name)\", tickangle=45, categoryorder='total descending', row=3, col=1)\n",
    "            fig.update_yaxes(title_text=\"Number of GV Protein Hits\", row=3, col=1)\n",
    "            \n",
    "            panel_c_data_path = Path(output_summary_dir_phase1) / \"figure13_panel_c_gv_euk_prot_func_hits.csv\"\n",
    "            try:\n",
    "                gv_prot_name_counts_all.reset_index().to_csv(panel_c_data_path, index=False) # Save all counts\n",
    "                print(f\"Data for Panel C saved to {panel_c_data_path}\")\n",
    "            except Exception as e: print(f\"Error saving Panel C data: {e}\")\n",
    "        else:\n",
    "            print(\"No eukaryotic protein names found for GV hits to plot for Panel C after filtering generic names.\")\n",
    "    else:\n",
    "        print(\"No GV proteins with eukaryotic hits found for Panel C analysis.\")\n",
    "\n",
    "    # --- Final Layout Updates for the Entire Figure ---\n",
    "    fig.update_layout(\n",
    "        height=1400, # Increased height for better label spacing\n",
    "        plot_bgcolor='rgba(0,0,0,0)', \n",
    "        paper_bgcolor='rgba(0,0,0,0)',\n",
    "        margin=dict(l=80, r=50, t=100, b=220), # Increased bottom margin\n",
    "        showlegend=False \n",
    "    )\n",
    "    if 'plotly_layout_defaults' in locals():\n",
    "        fig.update_layout(font=plotly_layout_defaults.font)\n",
    "        for i in range(len(fig.layout.annotations)): \n",
    "            if fig.layout.annotations[i].text.startswith(\"<b>\"): \n",
    "                fig.layout.annotations[i].font.size = 14\n",
    "                if plotly_layout_defaults.font:\n",
    "                    fig.layout.annotations[i].font.family = plotly_layout_defaults.font.family\n",
    "    \n",
    "    if 'plotly_layout_defaults' in locals():\n",
    "        for axis_name_template in ['xaxis', 'yaxis']:\n",
    "            for i in range(1, 4): \n",
    "                 axis_ref_name = f\"{axis_name_template}{i if i > 1 else ''}\" \n",
    "                 if hasattr(fig.layout, axis_ref_name) and hasattr(plotly_layout_defaults, axis_name_template):\n",
    "                     default_axis_style = getattr(plotly_layout_defaults, axis_name_template)\n",
    "                     current_axis = getattr(fig.layout, axis_ref_name)\n",
    "                     if current_axis.title and current_axis.title.text and default_axis_style.title and default_axis_style.title.font:\n",
    "                         current_axis.title.font.size = default_axis_style.title.font.size\n",
    "                         current_axis.title.font.family = default_axis_style.title.font.family\n",
    "                         current_axis.title.font.weight = default_axis_style.title.font.weight\n",
    "                     elif default_axis_style.title and default_axis_style.title.font: \n",
    "                         current_axis.title.font.size = default_axis_style.title.font.size\n",
    "                         current_axis.title.font.family = default_axis_style.title.font.family\n",
    "                         current_axis.title.font.weight = default_axis_style.title.font.weight\n",
    "                     if default_axis_style.tickfont:\n",
    "                         current_axis.tickfont.size = default_axis_style.tickfont.size\n",
    "                         current_axis.tickfont.family = default_axis_style.tickfont.family\n",
    "                     current_axis.showgrid = False\n",
    "                     current_axis.zeroline = False\n",
    "                     current_axis.linecolor = 'black'\n",
    "                     current_axis.linewidth = 1.5 \n",
    "                     current_axis.ticks=\"outside\"\n",
    "                     current_axis.ticklen=5\n",
    "                     current_axis.tickwidth=1.5\n",
    "                     current_axis.tickcolor='black'\n",
    "    fig.show()\n",
    "\n",
    "    # --- Save Figure ---\n",
    "    fig_path_html = Path(output_figure_dir) / \"figure13_eukaryotic_hit_characterization_consistent_colors.html\"\n",
    "    fig.write_html(str(fig_path_html))\n",
    "    print(f\"Figure 13 (Consistent Colors) HTML saved to: {fig_path_html}\")\n",
    "    try:\n",
    "        fig_path_pdf = Path(output_figure_dir) / \"figure13_eukaryotic_hit_characterization_consistent_colors.pdf\"\n",
    "        fig.write_image(str(fig_path_pdf), width=900, height=1400)\n",
    "        print(f\"Figure 13 PDF saved to: {fig_path_pdf}\")\n",
    "    except Exception as e: print(f\"Warning: Could not export Figure 13 to PDF. Error: {e}\")\n",
    "    try:\n",
    "        fig_path_svg = Path(output_figure_dir) / \"figure13_eukaryotic_hit_characterization_consistent_colors.svg\"\n",
    "        fig.write_image(str(fig_path_svg), width=900, height=1400)\n",
    "        print(f\"Figure 13 SVG saved to: {fig_path_svg}\")\n",
    "    except Exception as e: print(f\"Warning: Could not export Figure 13 to SVG. Error: {e}\")\n",
    "\n",
    "print(\"\\n\\n--- Cell 21 (Figure 13 - Eukaryotic Hit Characterization with Consistent Colors) Complete ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0180c4-69e7-4bc6-b81d-870f5a1ceb3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (asgard_gv_env)",
   "language": "python",
   "name": "asgard_gv_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
