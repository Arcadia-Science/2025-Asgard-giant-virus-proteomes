{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dce24b-a3e5-41fa-9526-81177d48ea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Paste the simple column-checking snippet here in a python interpreter or jupyter cell)\n",
    "import pandas as pd\n",
    "parquet_metadata_file = \"integrated_asgard_gv_ortho_interpro.parquet\"\n",
    "try:\n",
    "    df_meta = pd.read_parquet(parquet_metadata_file, engine='auto')\n",
    "    print(\"Columns found in the parquet file:\")\n",
    "    print(df_meta.columns.tolist())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Parquet file not found at '{parquet_metadata_file}'\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5885bc3-0fb7-489c-b40c-9b21434e50cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def assemble_core_csv_with_fasta(\n",
    "    parquet_file: str,\n",
    "    pdb_results_file: str,\n",
    "    filtered_fasta_file: str, # Path to the FASTA with sequences\n",
    "    output_csv_file: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Assembles the initial core data CSV by combining metadata from a parquet file\n",
    "    with sequence search hit information and sequences from a FASTA file.\n",
    "\n",
    "    Args:\n",
    "        parquet_file: Path to the input parquet file containing integrated metadata.\n",
    "        pdb_results_file: Path to the MMseqs2 results file from the PDB search.\n",
    "        filtered_fasta_file: Path to the FASTA file containing sequences for filtering.\n",
    "        output_csv_file: Path where the assembled CSV file will be written.\n",
    "    \"\"\"\n",
    "    print(\"Starting CSV assembly...\")\n",
    "    print(f\"  Input Parquet: {parquet_file}\")\n",
    "    print(f\"  PDB Results: {pdb_results_file}\")\n",
    "    print(f\"  Input FASTA: {filtered_fasta_file}\")\n",
    "    print(f\"  Output CSV: {output_csv_file}\")\n",
    "\n",
    "    # --- 1. Load main metadata from Parquet ---\n",
    "    try:\n",
    "        print(f\"\\nReading metadata from {parquet_file}...\")\n",
    "        df = pd.read_parquet(parquet_file, engine='auto')\n",
    "        print(f\"  Loaded {len(df):,} records from parquet file.\")\n",
    "        print(f\"  Parquet Columns Found: {df.columns.tolist()}\")\n",
    "\n",
    "        # --- Column mapping based on user-provided list ---\n",
    "        required_cols = {\n",
    "            'ProteinID': 'ProteinID',\n",
    "            'GenomeID': 'Source_Genome_Assembly_Accession',\n",
    "            'OriginalName': 'Source_Protein_Annotation',\n",
    "            'Dataset': 'Source_Dataset',\n",
    "            'Phylum': 'Taxonomy_Phylum',\n",
    "            'Taxonomy': 'Taxonomy_Species',\n",
    "            'OG_ID': 'Orthogroup',\n",
    "            'All_IPR_Hits': 'IPR_Signatures',\n",
    "            'All_GO_Terms': 'IPR_GO_Terms',\n",
    "            'Num_Domains': 'Num_Domains',\n",
    "            'Domain_Architecture': 'Domain_Architecture',\n",
    "            'Type': 'Type',\n",
    "            'Is_Hypothetical': 'Is_Hypothetical',\n",
    "            'Has_Known_Structure': 'Has_Known_Structure',\n",
    "            'ncbi_taxid_placeholder': 'NCBI_TaxID',\n",
    "            'taxonomy_supergroup_placeholder': 'Taxonomy_Supergroup',\n",
    "            'taxonomy_class_placeholder': 'Taxonomy_Class',\n",
    "            'uniprot_ac_placeholder': 'UniProtKB_AC',\n",
    "            'afdb_status_placeholder': 'AFDB_Status'\n",
    "        }\n",
    "        protein_id_col_name = 'ProteinID' # Confirmed column name\n",
    "        if protein_id_col_name not in df.columns:\n",
    "             print(f\"Error: Crucial column for ProteinID ('{protein_id_col_name}') not found in {parquet_file}.\")\n",
    "             sys.exit(1)\n",
    "        print(f\"  Using '{protein_id_col_name}' as the ProteinID column from parquet.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Parquet file not found at {parquet_file}\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading or processing parquet file {parquet_file}: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- 2. Load Sequences from FASTA ---\n",
    "    sequences = {}\n",
    "    try:\n",
    "        print(f\"\\nReading sequences from {filtered_fasta_file}...\")\n",
    "        count_fasta = 0\n",
    "        with open(filtered_fasta_file, 'r') as fastafile:\n",
    "             for record in SeqIO.parse(fastafile, \"fasta\"):\n",
    "                  sequences[record.id] = str(record.seq)\n",
    "                  count_fasta += 1\n",
    "        print(f\"  Loaded {len(sequences):,} sequences from FASTA file.\")\n",
    "        if count_fasta == 0: print(f\"  Warning: No sequences found in {filtered_fasta_file}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Filtered FASTA file not found at {filtered_fasta_file}\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading FASTA file {filtered_fasta_file}: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- 3. Load PDB hit IDs ---\n",
    "    pdb_hit_ids = set()\n",
    "    try:\n",
    "        print(f\"\\nReading PDB hit IDs from {pdb_results_file}...\")\n",
    "        if os.path.exists(pdb_results_file) and os.path.getsize(pdb_results_file) > 0:\n",
    "             pdb_hits_df = pd.read_csv(\n",
    "                  pdb_results_file, sep='\\t', header=None, usecols=[0],\n",
    "                  names=['query'], comment='#', low_memory=False\n",
    "             )\n",
    "             pdb_hit_ids = set(pdb_hits_df['query'].dropna().unique())\n",
    "             print(f\"  Found {len(pdb_hit_ids):,} unique ProteinIDs with PDB hits.\")\n",
    "        else:\n",
    "             print(f\"  PDB results file '{pdb_results_file}' not found or is empty. Assuming no PDB hits.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDB results file {pdb_results_file}: {e}\")\n",
    "        pdb_hit_ids = set()\n",
    "\n",
    "    # --- 4. Prepare DataFrame for Output ---\n",
    "    print(\"\\nPreparing final DataFrame...\")\n",
    "    unique_protein_ids_in_parquet = df[protein_id_col_name].unique()\n",
    "    output_df = pd.DataFrame(index=unique_protein_ids_in_parquet)\n",
    "    output_df.index.name = 'ProteinID_Index' # Temporary index name\n",
    "\n",
    "    # --- Map and add columns from the parquet file ---\n",
    "    print(\"  Mapping columns from parquet to output DataFrame...\")\n",
    "    for parquet_col_name, output_col_name in required_cols.items():\n",
    "        if parquet_col_name == protein_id_col_name:\n",
    "            print(f\"  Skipping mapping for '{parquet_col_name}' as it's the primary ID.\")\n",
    "            continue\n",
    "\n",
    "        if parquet_col_name in df.columns:\n",
    "            try:\n",
    "                mapping_series = df.drop_duplicates(subset=[protein_id_col_name]).set_index(protein_id_col_name)[parquet_col_name]\n",
    "                output_df[output_col_name] = output_df.index.map(mapping_series)\n",
    "            except KeyError as e:\n",
    "                print(f\"!!! Internal KeyError during mapping for column '{parquet_col_name}': {e}\")\n",
    "                output_df[output_col_name] = pd.NA\n",
    "            except Exception as e:\n",
    "                print(f\"!!! Error during mapping for column '{parquet_col_name}': {e}\")\n",
    "                output_df[output_col_name] = pd.NA\n",
    "        else:\n",
    "            print(f\"  Info: Column '{parquet_col_name}' not found in input parquet. Adding empty column '{output_col_name}'.\")\n",
    "            output_df[output_col_name] = pd.NA\n",
    "\n",
    "    # Add Sequence column from the dictionary\n",
    "    output_df['Sequence'] = output_df.index.map(sequences)\n",
    "    missing_seq_count = output_df['Sequence'].isna().sum()\n",
    "    if missing_seq_count > 0:\n",
    "         print(f\"  Info: {missing_seq_count:,} proteins from parquet file did not have a sequence in the filtered FASTA file (expected).\")\n",
    "    print(\"  Added 'Sequence' column from FASTA.\")\n",
    "\n",
    "    # <<< CHANGE START >>>\n",
    "    # Calculate Length robustly, handling potential non-string values\n",
    "    print(\"  Calculating 'Length' column...\")\n",
    "    output_df['Length'] = output_df['Sequence'].apply(lambda x: len(x) if isinstance(x, str) else 0).astype(int)\n",
    "    # <<< CHANGE END >>>\n",
    "    print(\"  Calculated 'Length' column.\")\n",
    "\n",
    "\n",
    "    # Add Sequence Search Hit Flags\n",
    "    output_df['SeqSearch_PDB_Hit'] = output_df.index.isin(pdb_hit_ids)\n",
    "    output_df['SeqSearch_AFDB_Hit'] = False # Based on previous results\n",
    "    output_df['SeqSearch_MGnify_Hit'] = False # Search was skipped\n",
    "    print(\"  Added 'SeqSearch_*_Hit' flag columns.\")\n",
    "\n",
    "    # Reset index to make ProteinID a regular column again\n",
    "    output_df.reset_index(inplace=True)\n",
    "    output_df.rename(columns={'ProteinID_Index': 'ProteinID'}, inplace=True) # Rename index col\n",
    "\n",
    "    # --- Filter rows: Keep only proteins present in the filtered FASTA ---\n",
    "    initial_rows = len(output_df)\n",
    "    output_df = output_df[output_df['ProteinID'].isin(sequences.keys())].copy()\n",
    "    rows_after_fasta_filter = len(output_df)\n",
    "    print(f\"  Filtered DataFrame to keep only proteins present in '{filtered_fasta_file}'.\")\n",
    "    print(f\"  Rows before FASTA filter: {initial_rows:,}. Rows after: {rows_after_fasta_filter:,}\")\n",
    "\n",
    "    # --- Reorder columns ---\n",
    "    desired_order = [\n",
    "        'ProteinID', 'Sequence', 'Length', 'Source_Dataset', 'Dataset',\n",
    "        'Source_Genome_Assembly_Accession', 'GenomeID',\n",
    "        'Source_Protein_Annotation', 'OriginalName',\n",
    "        'NCBI_TaxID', 'Taxonomy_Supergroup', 'Taxonomy_Phylum', 'Phylum',\n",
    "        'Taxonomy_Class', 'Taxonomy_Species', 'Taxonomy',\n",
    "        'Orthogroup', 'OG_ID',\n",
    "        'IPR_Signatures', 'All_IPR_Hits',\n",
    "        'IPR_GO_Terms', 'All_GO_Terms',\n",
    "        'UniProtKB_AC', 'AFDB_Status',\n",
    "        'SeqSearch_PDB_Hit', 'SeqSearch_AFDB_Hit', 'SeqSearch_MGnify_Hit',\n",
    "        'Num_Domains', 'Domain_Architecture', 'Type', 'Is_Hypothetical', 'Has_Known_Structure'\n",
    "    ]\n",
    "    present_columns = [col for col in desired_order if col in output_df.columns]\n",
    "    present_columns.extend([col for col in output_df.columns if col not in present_columns])\n",
    "    final_columns = pd.Series(present_columns).drop_duplicates().tolist() # Ensure unique cols\n",
    "    output_df = output_df[final_columns]\n",
    "    print(f\"  Final DataFrame has {len(output_df.columns)} columns and {len(output_df):,} rows.\")\n",
    "\n",
    "    # --- 5. Write to CSV ---\n",
    "    try:\n",
    "        print(f\"\\nWriting final CSV to {output_csv_file}...\")\n",
    "        output_dir = os.path.dirname(output_csv_file)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "             os.makedirs(output_dir); print(f\"  Created output directory: {output_dir}\")\n",
    "        output_df.to_csv(output_csv_file, index=False, na_rep='NA')\n",
    "        print(f\"  Successfully wrote CSV file to {output_csv_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing CSV file {output_csv_file}: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(\"\\nCSV assembly script finished.\")\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Define your input and output file paths here ---\n",
    "    parquet_metadata_file = \"integrated_asgard_gv_ortho_interpro.parquet\"\n",
    "    pdb_search_results_file = \"results_vs_pdb_v2.txt\"\n",
    "    filtered_fasta_path = \"folding_candidates_final_filtered.fasta\"\n",
    "    output_database_csv = \"proteome_database_v0.1.csv\"\n",
    "\n",
    "    # --- Basic check if input files exist ---\n",
    "    if not os.path.exists(parquet_metadata_file):\n",
    "         print(f\"Error: Parquet metadata file '{parquet_metadata_file}' not found.\")\n",
    "         sys.exit(1)\n",
    "    if not os.path.exists(filtered_fasta_path):\n",
    "         print(f\"Error: Filtered FASTA file '{filtered_fasta_path}' not found.\")\n",
    "         sys.exit(1)\n",
    "\n",
    "    # --- Run the assembly function ---\n",
    "    assemble_core_csv_with_fasta(\n",
    "        parquet_file=parquet_metadata_file,\n",
    "        pdb_results_file=pdb_search_results_file,\n",
    "        filtered_fasta_file=filtered_fasta_path,\n",
    "        output_csv_file=output_database_csv\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3017e92f-96e2-43c1-bb4f-304e35e2e6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "parquet_metadata_file = \"integrated_asgard_gv_ortho_interpro.parquet\"\n",
    "try:\n",
    "    df_meta = pd.read_parquet(parquet_metadata_file, engine='auto', columns=['ProteinID'])\n",
    "    print(\"First 5 ProteinIDs from parquet file:\")\n",
    "    print(df_meta['ProteinID'].head().tolist())\n",
    "except Exception as e:\n",
    "    print(f\"Error reading ProteinID column: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75fe4fe-f327-4fc5-a50b-9fe4739d88e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def assemble_core_csv_with_fasta(\n",
    "    parquet_file: str,\n",
    "    pdb_results_file: str,\n",
    "    filtered_fasta_file: str, # Path to the FASTA with sequences\n",
    "    output_csv_file: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Assembles the initial core data CSV by combining metadata from a parquet file\n",
    "    with sequence search hit information and sequences from a FASTA file.\n",
    "    Handles FASTA headers containing '|' delimiters.\n",
    "\n",
    "    Args:\n",
    "        parquet_file: Path to the input parquet file containing integrated metadata.\n",
    "        pdb_results_file: Path to the MMseqs2 results file from the PDB search.\n",
    "        filtered_fasta_file: Path to the FASTA file containing sequences for filtering.\n",
    "        output_csv_file: Path where the assembled CSV file will be written.\n",
    "    \"\"\"\n",
    "    print(\"Starting CSV assembly...\")\n",
    "    print(f\"  Input Parquet: {parquet_file}\")\n",
    "    print(f\"  PDB Results: {pdb_results_file}\")\n",
    "    print(f\"  Input FASTA: {filtered_fasta_file}\")\n",
    "    print(f\"  Output CSV: {output_csv_file}\")\n",
    "\n",
    "    # --- 1. Load main metadata from Parquet ---\n",
    "    try:\n",
    "        print(f\"\\nReading metadata from {parquet_file}...\")\n",
    "        df = pd.read_parquet(parquet_file, engine='auto')\n",
    "        print(f\"  Loaded {len(df):,} records from parquet file.\")\n",
    "        print(f\"  Parquet Columns Found: {df.columns.tolist()}\")\n",
    "\n",
    "        # --- Column mapping based on user-provided list ---\n",
    "        required_cols = {\n",
    "            'ProteinID': 'ProteinID',\n",
    "            'GenomeID': 'Source_Genome_Assembly_Accession',\n",
    "            'OriginalName': 'Source_Protein_Annotation',\n",
    "            'Dataset': 'Source_Dataset',\n",
    "            'Phylum': 'Taxonomy_Phylum',\n",
    "            'Taxonomy': 'Taxonomy_Species',\n",
    "            'OG_ID': 'Orthogroup',\n",
    "            'All_IPR_Hits': 'IPR_Signatures',\n",
    "            'All_GO_Terms': 'IPR_GO_Terms',\n",
    "            'Num_Domains': 'Num_Domains',\n",
    "            'Domain_Architecture': 'Domain_Architecture',\n",
    "            'Type': 'Type',\n",
    "            'Is_Hypothetical': 'Is_Hypothetical',\n",
    "            'Has_Known_Structure': 'Has_Known_Structure',\n",
    "            'ncbi_taxid_placeholder': 'NCBI_TaxID',\n",
    "            'taxonomy_supergroup_placeholder': 'Taxonomy_Supergroup',\n",
    "            'taxonomy_class_placeholder': 'Taxonomy_Class',\n",
    "            'uniprot_ac_placeholder': 'UniProtKB_AC',\n",
    "            'afdb_status_placeholder': 'AFDB_Status'\n",
    "        }\n",
    "        protein_id_col_name = 'ProteinID' # Confirmed column name\n",
    "        if protein_id_col_name not in df.columns:\n",
    "             print(f\"Error: Crucial column for ProteinID ('{protein_id_col_name}') not found in {parquet_file}.\")\n",
    "             sys.exit(1)\n",
    "        print(f\"  Using '{protein_id_col_name}' as the ProteinID column from parquet.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Parquet file not found at {parquet_file}\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading or processing parquet file {parquet_file}: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- 2. Load Sequences from FASTA ---\n",
    "    sequences = {}\n",
    "    try:\n",
    "        print(f\"\\nReading sequences from {filtered_fasta_file}...\")\n",
    "        count_fasta = 0\n",
    "        count_parse_errors = 0\n",
    "        with open(filtered_fasta_file, 'r') as fastafile:\n",
    "             for record in SeqIO.parse(fastafile, \"fasta\"):\n",
    "                  # <<< CHANGE START >>>\n",
    "                  # Parse the ID: Take the part before the first '|' if present,\n",
    "                  # otherwise use the whole record.id (which handles headers without '|')\n",
    "                  fasta_id = record.description.split('|')[0] if '|' in record.description else record.id\n",
    "                  # Basic check if the extracted ID looks reasonable (optional)\n",
    "                  if not fasta_id:\n",
    "                      print(f\"  Warning: Could not extract valid ID from FASTA header: {record.description[:100]}...\")\n",
    "                      count_parse_errors += 1\n",
    "                      continue\n",
    "                  # <<< CHANGE END >>>\n",
    "\n",
    "                  sequences[fasta_id] = str(record.seq)\n",
    "                  count_fasta += 1\n",
    "\n",
    "        print(f\"  Loaded {len(sequences):,} sequences from FASTA file.\")\n",
    "        if count_parse_errors > 0:\n",
    "            print(f\"  Encountered {count_parse_errors} headers where ID parsing failed.\")\n",
    "        if count_fasta == 0: print(f\"  Warning: No sequences found in {filtered_fasta_file}.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Filtered FASTA file not found at {filtered_fasta_file}\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading FASTA file {filtered_fasta_file}: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # --- 3. Load PDB hit IDs ---\n",
    "    pdb_hit_ids = set()\n",
    "    try:\n",
    "        print(f\"\\nReading PDB hit IDs from {pdb_results_file}...\")\n",
    "        if os.path.exists(pdb_results_file) and os.path.getsize(pdb_results_file) > 0:\n",
    "             pdb_hits_df = pd.read_csv(\n",
    "                  pdb_results_file, sep='\\t', header=None, usecols=[0],\n",
    "                  names=['query'], comment='#', low_memory=False\n",
    "             )\n",
    "             # Also parse the query IDs from the PDB results file just in case they have '|'\n",
    "             pdb_hit_ids = set(pdb_hits_df['query'].dropna().astype(str).apply(lambda x: x.split('|')[0]))\n",
    "             print(f\"  Found {len(pdb_hit_ids):,} unique ProteinIDs (parsed) with PDB hits.\")\n",
    "        else:\n",
    "             print(f\"  PDB results file '{pdb_results_file}' not found or is empty. Assuming no PDB hits.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDB results file {pdb_results_file}: {e}\")\n",
    "        pdb_hit_ids = set()\n",
    "\n",
    "    # --- 4. Prepare DataFrame for Output ---\n",
    "    print(\"\\nPreparing final DataFrame...\")\n",
    "    unique_protein_ids_in_parquet = df[protein_id_col_name].unique()\n",
    "    output_df = pd.DataFrame(index=unique_protein_ids_in_parquet)\n",
    "    output_df.index.name = 'ProteinID_Index' # Temporary index name\n",
    "\n",
    "    # --- Map and add columns from the parquet file ---\n",
    "    print(\"  Mapping columns from parquet to output DataFrame...\")\n",
    "    for parquet_col_name, output_col_name in required_cols.items():\n",
    "        if parquet_col_name == protein_id_col_name:\n",
    "            print(f\"  Skipping mapping for '{parquet_col_name}' as it's the primary ID.\")\n",
    "            continue\n",
    "\n",
    "        if parquet_col_name in df.columns:\n",
    "            try:\n",
    "                mapping_series = df.drop_duplicates(subset=[protein_id_col_name]).set_index(protein_id_col_name)[parquet_col_name]\n",
    "                output_df[output_col_name] = output_df.index.map(mapping_series)\n",
    "            except KeyError as e:\n",
    "                print(f\"!!! Internal KeyError during mapping for column '{parquet_col_name}': {e}\")\n",
    "                output_df[output_col_name] = pd.NA\n",
    "            except Exception as e:\n",
    "                print(f\"!!! Error during mapping for column '{parquet_col_name}': {e}\")\n",
    "                output_df[output_col_name] = pd.NA\n",
    "        else:\n",
    "            print(f\"  Info: Column '{parquet_col_name}' not found in input parquet. Adding empty column '{output_col_name}'.\")\n",
    "            output_df[output_col_name] = pd.NA\n",
    "\n",
    "    # Add Sequence column from the dictionary\n",
    "    output_df['Sequence'] = output_df.index.map(sequences)\n",
    "    missing_seq_count = output_df['Sequence'].isna().sum()\n",
    "    if missing_seq_count > 0:\n",
    "         print(f\"  Info: {missing_seq_count:,} proteins from parquet file did not have a sequence in the filtered FASTA file (expected).\")\n",
    "    print(\"  Added 'Sequence' column from FASTA.\")\n",
    "\n",
    "    # Calculate Length robustly, handling potential non-string values\n",
    "    print(\"  Calculating 'Length' column...\")\n",
    "    output_df['Length'] = output_df['Sequence'].apply(lambda x: len(x) if isinstance(x, str) else 0).astype(int)\n",
    "    print(\"  Calculated 'Length' column.\")\n",
    "\n",
    "\n",
    "    # Add Sequence Search Hit Flags\n",
    "    output_df['SeqSearch_PDB_Hit'] = output_df.index.isin(pdb_hit_ids)\n",
    "    output_df['SeqSearch_AFDB_Hit'] = False # Based on previous results\n",
    "    output_df['SeqSearch_MGnify_Hit'] = False # Search was skipped\n",
    "    print(\"  Added 'SeqSearch_*_Hit' flag columns.\")\n",
    "\n",
    "    # Reset index to make ProteinID a regular column again\n",
    "    output_df.reset_index(inplace=True)\n",
    "    output_df.rename(columns={'ProteinID_Index': 'ProteinID'}, inplace=True) # Rename index col\n",
    "\n",
    "    # --- Filter rows: Keep only proteins present in the filtered FASTA ---\n",
    "    initial_rows = len(output_df)\n",
    "    # Ensure comparison is robust by checking against the keys from the sequences dict\n",
    "    output_df = output_df[output_df['ProteinID'].isin(sequences.keys())].copy()\n",
    "    rows_after_fasta_filter = len(output_df)\n",
    "    print(f\"  Filtered DataFrame to keep only proteins present in '{filtered_fasta_file}'.\")\n",
    "    print(f\"  Rows before FASTA filter: {initial_rows:,}. Rows after: {rows_after_fasta_filter:,}\")\n",
    "\n",
    "    # --- Reorder columns ---\n",
    "    desired_order = [\n",
    "        'ProteinID', 'Sequence', 'Length', 'Source_Dataset', 'Dataset',\n",
    "        'Source_Genome_Assembly_Accession', 'GenomeID',\n",
    "        'Source_Protein_Annotation', 'OriginalName',\n",
    "        'NCBI_TaxID', 'Taxonomy_Supergroup', 'Taxonomy_Phylum', 'Phylum',\n",
    "        'Taxonomy_Class', 'Taxonomy_Species', 'Taxonomy',\n",
    "        'Orthogroup', 'OG_ID',\n",
    "        'IPR_Signatures', 'All_IPR_Hits',\n",
    "        'IPR_GO_Terms', 'All_GO_Terms',\n",
    "        'UniProtKB_AC', 'AFDB_Status',\n",
    "        'SeqSearch_PDB_Hit', 'SeqSearch_AFDB_Hit', 'SeqSearch_MGnify_Hit',\n",
    "        'Num_Domains', 'Domain_Architecture', 'Type', 'Is_Hypothetical', 'Has_Known_Structure'\n",
    "    ]\n",
    "    present_columns = [col for col in desired_order if col in output_df.columns]\n",
    "    present_columns.extend([col for col in output_df.columns if col not in present_columns])\n",
    "    final_columns = pd.Series(present_columns).drop_duplicates().tolist() # Ensure unique cols\n",
    "    output_df = output_df[final_columns]\n",
    "    print(f\"  Final DataFrame has {len(output_df.columns)} columns and {len(output_df):,} rows.\")\n",
    "\n",
    "    # --- 5. Write to CSV ---\n",
    "    try:\n",
    "        print(f\"\\nWriting final CSV to {output_csv_file}...\")\n",
    "        output_dir = os.path.dirname(output_csv_file)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "             os.makedirs(output_dir); print(f\"  Created output directory: {output_dir}\")\n",
    "\n",
    "        # Check if DataFrame is empty before writing\n",
    "        if output_df.empty:\n",
    "            print(\"  Warning: Final DataFrame is empty. Writing only headers to CSV.\")\n",
    "            # Write only headers if empty\n",
    "            with open(output_csv_file, 'w') as f:\n",
    "                f.write(','.join(output_df.columns) + '\\n')\n",
    "        else:\n",
    "            output_df.to_csv(output_csv_file, index=False, na_rep='NA') # Use NA for missing values\n",
    "            print(f\"  Successfully wrote CSV file to {output_csv_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing CSV file {output_csv_file}: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(\"\\nCSV assembly script finished.\")\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Define your input and output file paths here ---\n",
    "    parquet_metadata_file = \"integrated_asgard_gv_ortho_interpro.parquet\"\n",
    "    pdb_search_results_file = \"results_vs_pdb_v2.txt\"\n",
    "    filtered_fasta_path = \"folding_candidates_final_filtered.fasta\"\n",
    "    output_database_csv = \"proteome_database_v0.1.csv\"\n",
    "\n",
    "    # --- Basic check if input files exist ---\n",
    "    if not os.path.exists(parquet_metadata_file):\n",
    "         print(f\"Error: Parquet metadata file '{parquet_metadata_file}' not found.\")\n",
    "         sys.exit(1)\n",
    "    if not os.path.exists(filtered_fasta_path):\n",
    "         print(f\"Error: Filtered FASTA file '{filtered_fasta_path}' not found.\")\n",
    "         sys.exit(1)\n",
    "\n",
    "    # --- Run the assembly function ---\n",
    "    assemble_core_csv_with_fasta(\n",
    "        parquet_file=parquet_metadata_file,\n",
    "        pdb_results_file=pdb_search_results_file,\n",
    "        filtered_fasta_file=filtered_fasta_path,\n",
    "        output_csv_file=output_database_csv\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4358d5af-d781-4ee4-a9c4-d50b0cc394fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys # Added for better error handling exit\n",
    "\n",
    "# --- Configuration: PLEASE UPDATE THESE PATHS ---\n",
    "unknowns_csv_path = 'your_unknowns_only.csv'  # Path to the CSV file generated by --output-unknowns\n",
    "interpro_list_path = 'interpro_entry.list' # Path to your InterPro entry list TSV file\n",
    "output_dir = '.' # Directory to save any output files/plots (optional)\n",
    "\n",
    "# --- Parameters ---\n",
    "top_n_ipr = 30  # How many top IPR IDs to display\n",
    "top_n_keywords = 50 # How many top annotation keywords to display\n",
    "\n",
    "# --- Ensure output directory exists ---\n",
    "try:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Output directory: {os.path.abspath(output_dir)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not create output directory '{output_dir}': {e}\")\n",
    "    output_dir = '.' # Default to current directory if creation fails\n",
    "\n",
    "print(f\"Unknowns CSV: {unknowns_csv_path}\")\n",
    "print(f\"InterPro List: {interpro_list_path}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Section 1: Load Data\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Section 1: Loading Data ---\")\n",
    "\n",
    "# --- Load Unknowns CSV ---\n",
    "try:\n",
    "    df_unknowns = pd.read_csv(unknowns_csv_path, low_memory=False)\n",
    "    print(f\"Successfully loaded {len(df_unknowns)} rows from {unknowns_csv_path}\")\n",
    "    # Fill NaN values in key columns to avoid errors later\n",
    "    df_unknowns['IPR_Signatures'] = df_unknowns['IPR_Signatures'].fillna('')\n",
    "    df_unknowns['Source_Protein_Annotation'] = df_unknowns['Source_Protein_Annotation'].fillna('')\n",
    "    # Display first few rows and info (optional, uncomment if needed)\n",
    "    # print(\"\\nUnknowns DataFrame Head:\")\n",
    "    # print(df_unknowns.head())\n",
    "    # print(\"\\nUnknowns DataFrame Info:\")\n",
    "    # df_unknowns.info()\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Unknowns CSV file not found at {unknowns_csv_path}. Please check the path.\")\n",
    "    sys.exit(1) # Stop execution if file not found\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not load unknowns CSV: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# --- Load InterPro List TSV ---\n",
    "def load_interpro_list_script(filepath):\n",
    "    \"\"\"Loads InterPro entry list into a dictionary for script use.\"\"\"\n",
    "    print(f\"Attempting to load InterPro list from: {filepath}\")\n",
    "    ipr_map = {}\n",
    "    abs_filepath = os.path.abspath(filepath)\n",
    "\n",
    "    if not os.path.exists(abs_filepath) or not os.path.isfile(abs_filepath):\n",
    "        print(f\"ERROR: InterPro list file not found or is not a file: {abs_filepath}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Use pandas for robust TSV reading\n",
    "        df_ipr = pd.read_csv(abs_filepath, sep='\\t', header=0,\n",
    "                             names=['ENTRY_AC', 'ENTRY_TYPE', 'ENTRY_NAME'], # Assign names in case header is missing\n",
    "                             usecols=[0, 1, 2], # Only read first 3 columns\n",
    "                             on_bad_lines='warn', # Report problematic lines\n",
    "                             encoding='utf-8', errors='ignore')\n",
    "\n",
    "        # Check if first row was mistakenly read as data instead of header\n",
    "        if not df_ipr.empty and df_ipr.iloc[0]['ENTRY_AC'] == 'ENTRY_AC':\n",
    "             df_ipr = df_ipr.iloc[1:]\n",
    "\n",
    "        # Filter for valid IPR IDs and convert to dictionary\n",
    "        df_ipr = df_ipr[df_ipr['ENTRY_AC'].astype(str).str.startswith('IPR')]\n",
    "        ipr_map = df_ipr.set_index('ENTRY_AC').to_dict('index')\n",
    "\n",
    "        print(f\"Successfully processed {len(ipr_map)} InterPro entries from {abs_filepath}.\")\n",
    "        if len(ipr_map) < 10: # Check if map seems very small\n",
    "             print(\"WARN: Very few InterPro entries loaded. Check file format and content.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to load or parse InterPro list file {abs_filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "    return ipr_map\n",
    "\n",
    "ipr_details_map = load_interpro_list_script(interpro_list_path)\n",
    "\n",
    "# Example lookup (if loaded successfully)\n",
    "if ipr_details_map and len(ipr_details_map) > 0:\n",
    "    example_id = list(ipr_details_map.keys())[0]\n",
    "    print(f\"Example InterPro entry: {example_id} -> {ipr_details_map.get(example_id)}\")\n",
    "elif not ipr_details_map:\n",
    "     print(\"WARN: InterPro map could not be loaded. Analysis requiring IPR details will be skipped.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Section 2: Count Proteins Lacking IPR Signatures\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Section 2: Counting Proteins Lacking IPR Signatures ---\")\n",
    "\n",
    "total_unknowns = len(df_unknowns)\n",
    "# Count rows where IPR_Signatures is null, empty string, or whitespace only\n",
    "unknowns_no_ipr = df_unknowns[df_unknowns['IPR_Signatures'].astype(str).str.strip() == ''].copy()\n",
    "count_no_ipr = len(unknowns_no_ipr)\n",
    "\n",
    "print(f\"Total 'Unknown/Unclassified' proteins: {total_unknowns}\")\n",
    "if total_unknowns > 0:\n",
    "    percentage_no_ipr = (count_no_ipr / total_unknowns) * 100\n",
    "    print(f\"Number of unknowns with NO IPR signatures: {count_no_ipr} ({percentage_no_ipr:.2f}%)\")\n",
    "else:\n",
    "    print(\"Number of unknowns with NO IPR signatures: 0\")\n",
    "\n",
    "# Display some examples of proteins without IPR signatures (optional)\n",
    "# if count_no_ipr > 0:\n",
    "#     print(\"\\nExample rows with no IPR Signatures (Annotation only):\")\n",
    "#     print(unknowns_no_ipr[['Source_Protein_Annotation']].head())\n",
    "\n",
    "print(\"\\nInsight: If this percentage is high, improving classification might heavily depend on\")\n",
    "print(\"analyzing and expanding the 'Annotation_Keywords' in the main script's CUSTOM_RULES.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Section 3: Find Most Frequent IPR IDs Among Unknowns\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Section 3: Finding Most Frequent IPR IDs Among Unknowns ---\")\n",
    "\n",
    "# --- Parse IPR Signatures and Count Frequencies ---\n",
    "all_ipr_ids_unknowns = []\n",
    "\n",
    "def extract_iprs(ipr_string):\n",
    "    # Split by common delimiters, strip whitespace, filter out empty strings\n",
    "    ids = [ipr_id.strip() for ipr_id in re.split(r'[,;|]', str(ipr_string)) if ipr_id.strip()]\n",
    "    return ids\n",
    "\n",
    "# Apply the function to the 'IPR_Signatures' column and flatten the list\n",
    "# Ensure the column exists before applying\n",
    "if 'IPR_Signatures' in df_unknowns.columns:\n",
    "    all_ipr_ids_unknowns = df_unknowns['IPR_Signatures'].apply(extract_iprs).sum()\n",
    "    print(f\"Total IPR signature occurrences found in unknowns: {len(all_ipr_ids_unknowns)}\")\n",
    "else:\n",
    "    print(\"WARN: 'IPR_Signatures' column not found in unknowns CSV. Skipping IPR frequency analysis.\")\n",
    "    all_ipr_ids_unknowns = [] # Ensure list exists but is empty\n",
    "\n",
    "# Count the frequency of each IPR ID\n",
    "ipr_counts = Counter(all_ipr_ids_unknowns)\n",
    "\n",
    "# Get the most common IPR IDs\n",
    "most_common_ipr = ipr_counts.most_common(top_n_ipr)\n",
    "\n",
    "print(f\"\\nTop {top_n_ipr} most frequent IPR IDs among unclassified proteins:\")\n",
    "if not most_common_ipr:\n",
    "    print(\"No IPR IDs found or processed in the unknowns file.\")\n",
    "else:\n",
    "    # Create a DataFrame for better display\n",
    "    df_top_ipr = pd.DataFrame(most_common_ipr, columns=['IPR_ID', 'Frequency'])\n",
    "    print(df_top_ipr.to_string()) # Print full dataframe\n",
    "\n",
    "    # --- Basic Plot ---\n",
    "    try:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.barh(df_top_ipr['IPR_ID'], df_top_ipr['Frequency'])\n",
    "        plt.xlabel('Frequency')\n",
    "        plt.ylabel('IPR ID')\n",
    "        plt.title(f'Top {top_n_ipr} Most Frequent IPR IDs in Unknowns')\n",
    "        plt.gca().invert_yaxis() # Display top ID at the top\n",
    "        plt.tight_layout()\n",
    "        # Save the plot (optional)\n",
    "        plot_path = os.path.join(output_dir, 'top_ipr_ids_unknowns.png')\n",
    "        plt.savefig(plot_path)\n",
    "        print(f\"\\nPlot saved to {plot_path}\")\n",
    "        # plt.show() # Uncomment to display plot if running interactively\n",
    "        plt.close() # Close plot to free memory\n",
    "    except Exception as e:\n",
    "        print(f\"\\nWARN: Could not generate IPR frequency plot: {e}\")\n",
    "\n",
    "\n",
    "print(\"\\nInsight: These IPR IDs are the most common signatures NOT being caught by your current rules.\")\n",
    "print(\"They are the primary candidates to investigate further.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Section 4: Analyze Details of Frequent Unknown IPR IDs\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Section 4: Analyzing Details of Frequent Unknown IPR IDs ---\")\n",
    "\n",
    "if not ipr_details_map:\n",
    "    print(\"Skipping IPR detail analysis because the InterPro map was not loaded.\")\n",
    "elif not most_common_ipr:\n",
    "    print(\"Skipping IPR detail analysis because no frequent IPR IDs were found.\")\n",
    "else:\n",
    "    print(f\"Looking up details for the top {top_n_ipr} IPR IDs...\")\n",
    "    top_ipr_data = []\n",
    "    for ipr_id, frequency in most_common_ipr:\n",
    "        details = ipr_details_map.get(ipr_id, {'type': 'Not Found', 'name': 'Not Found'})\n",
    "        top_ipr_data.append({\n",
    "            'IPR_ID': ipr_id,\n",
    "            'Frequency': frequency,\n",
    "            'IPR_Type': details.get('type', 'N/A'),\n",
    "            'IPR_Name': details.get('name', 'N/A')\n",
    "        })\n",
    "\n",
    "    df_top_ipr_details = pd.DataFrame(top_ipr_data)\n",
    "\n",
    "    print(\"\\nDetails of Top Frequent IPR IDs in Unknowns:\")\n",
    "    # Display the full table\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_colwidth', None):\n",
    "        print(df_top_ipr_details.to_string())\n",
    "\n",
    "print(\"\\nInsight:\")\n",
    "print(\"- Look at the IPR_Name: Does it suggest a function? Does it contain keywords you could add\")\n",
    "print(\"  to an existing category's 'IPR_Keywords' list (remember to use lowercase)?\")\n",
    "print(\"- Does the function represent a new category you need to create in CUSTOM_RULES?\")\n",
    "print(\"- Look at the IPR_Type: Is it 'Domain', 'Family', etc.? This helps understand the nature\")\n",
    "print(\"  of the unclassified signatures.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Section 5: Analyze Annotation Keywords\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Section 5: Analyzing Annotation Keywords ---\")\n",
    "\n",
    "if 'Source_Protein_Annotation' not in df_unknowns.columns:\n",
    "     print(\"WARN: 'Source_Protein_Annotation' column not found. Skipping annotation keyword analysis.\")\n",
    "else:\n",
    "    print(\"Analyzing keywords in 'Source_Protein_Annotation'...\")\n",
    "\n",
    "    # Define common words to ignore (customize this list as needed!)\n",
    "    stop_words = {\n",
    "        'protein', 'hypothetical', 'uncharacterized', 'predicted', 'putative',\n",
    "        'domain', 'family', 'containing', 'like', 'of', 'the', 'a', 'an', 'in', 'to', 'and',\n",
    "        'is', 'it', 'with', 'by', 'on', 'at', 'from', 'as', 'for', 'or', 'et', 'al',\n",
    "        'type', 'subunit', 'chain', 'region', 'motif', 'repeat', 'protein,', 'unknown', 'function'\n",
    "        # Add more domain-specific or common words if they obscure results\n",
    "    }\n",
    "\n",
    "    all_words = []\n",
    "    # Ensure the column is treated as string and lowercase\n",
    "    annotation_series = df_unknowns['Source_Protein_Annotation'].astype(str).str.lower()\n",
    "\n",
    "    # Simple word tokenization and filtering\n",
    "    for annotation in annotation_series:\n",
    "        # Remove punctuation (basic), split into words\n",
    "        words = re.findall(r'\\b\\w+\\b', annotation)\n",
    "        # Filter out stop words and very short words (e.g., <= 2 letters)\n",
    "        filtered_words = [word for word in words if word not in stop_words and len(word) > 2]\n",
    "        all_words.extend(filtered_words)\n",
    "\n",
    "    print(f\"Total potentially relevant words found in annotations: {len(all_words)}\")\n",
    "\n",
    "    # Count word frequencies\n",
    "    keyword_counts = Counter(all_words)\n",
    "    most_common_keywords = keyword_counts.most_common(top_n_keywords)\n",
    "\n",
    "    print(f\"\\nTop {top_n_keywords} most frequent keywords in annotations (excluding common words):\")\n",
    "    if not most_common_keywords:\n",
    "        print(\"No significant keywords found after filtering.\")\n",
    "    else:\n",
    "        df_top_keywords = pd.DataFrame(most_common_keywords, columns=['Keyword', 'Frequency'])\n",
    "        print(df_top_keywords.to_string())\n",
    "\n",
    "        # --- Basic Plot ---\n",
    "        try:\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.barh(df_top_keywords['Keyword'], df_top_keywords['Frequency'])\n",
    "            plt.xlabel('Frequency')\n",
    "            plt.ylabel('Keyword')\n",
    "            plt.title(f'Top {top_n_keywords} Annotation Keywords in Unknowns')\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.tight_layout()\n",
    "            # Save the plot (optional)\n",
    "            plot_path = os.path.join(output_dir, 'top_annotation_keywords_unknowns.png')\n",
    "            plt.savefig(plot_path)\n",
    "            print(f\"\\nPlot saved to {plot_path}\")\n",
    "            # plt.show() # Uncomment to display plot if running interactively\n",
    "            plt.close() # Close plot to free memory\n",
    "        except Exception as e:\n",
    "             print(f\"\\nWARN: Could not generate annotation keyword plot: {e}\")\n",
    "\n",
    "\n",
    "    print(\"\\nInsight:\")\n",
    "    print(\"- Are there recurring functional terms here that are missing from your\")\n",
    "    print(\"  'Annotation_Keywords' lists in CUSTOM_RULES (remember lowercase)?\")\n",
    "    print(\"- Do these keywords suggest functions that belong to existing categories or point\")\n",
    "    print(\"  towards new categories needed?\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Section 6: Conclusion\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Section 6: Conclusion & Next Steps ---\")\n",
    "print(\"\\nThis analysis provides several starting points for refining the `CUSTOM_RULES`\")\n",
    "print(\"in your `add_specific_category_IPR_v10.py` script:\")\n",
    "print(\"\\n1. Proteins without IPR: If many unknowns lack IPR IDs, focus on improving\")\n",
    "print(\"   the `Annotation_Keywords` rules.\")\n",
    "print(\"2. Frequent Unknown IPR IDs: Investigate the functions of the top IPR IDs\")\n",
    "print(\"   identified. Add relevant IDs or derived keywords (`IPR_Keywords`) to your\")\n",
    "print(\"   existing or new categories in `CUSTOM_RULES`.\")\n",
    "print(\"3. Frequent Annotation Keywords: Add relevant keywords found in the annotation\")\n",
    "print(\"   analysis to the `Annotation_Keywords` lists in `CUSTOM_RULES`.\")\n",
    "print(\"\\nRecommendation:\")\n",
    "print(\"* Iteratively update the `CUSTOM_RULES` dictionary in `add_specific_category_IPR_v10.py`\")\n",
    "print(\"  based on these findings.\")\n",
    "print(\"* Re-run the main script.\")\n",
    "print(\"* Re-run this analysis script on the *new* unknowns file to see if the\")\n",
    "print(\"  classification has improved and identify the next set of common unknowns.\")\n",
    "print(\"* Repeat this process until the number of 'Unknown/Unclassified' proteins\")\n",
    "print(\"  is acceptably low.\")\n",
    "\n",
    "print(\"\\nAnalysis complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9333b4-ac3a-4b17-9ede-1fcb3cd35ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "unknowns_csv_path = 'your_unknowns_only.csv' # Input CSV with unknown protein IDs\n",
    "original_fasta_files = [\n",
    "    'Fastas_filtered/Asgard_all_globular_proteins.fasta', # Path to first original FASTA\n",
    "    'Fastas_filtered/GV_all_globular_proteins.fasta'     # Path to second original FASTA\n",
    "    # Add more original FASTA files if needed\n",
    "]\n",
    "output_fasta_path = 'unknown_protein_sequences.fasta' # Output FASTA for InterProScan input\n",
    "\n",
    "# Column in the CSV containing the protein IDs (adjust if different)\n",
    "protein_id_column = 'ProteinID'\n",
    "\n",
    "# --- Script Logic ---\n",
    "\n",
    "print(f\"Loading unknown protein IDs from: {unknowns_csv_path}\")\n",
    "\n",
    "try:\n",
    "    df_unknowns = pd.read_csv(unknowns_csv_path)\n",
    "    if protein_id_column not in df_unknowns.columns:\n",
    "        print(f\"ERROR: Protein ID column '{protein_id_column}' not found in {unknowns_csv_path}\")\n",
    "        print(f\"Available columns: {df_unknowns.columns.tolist()}\")\n",
    "        sys.exit(1)\n",
    "    # Ensure IDs are strings and handle potential NaN values\n",
    "    unknown_ids = set(df_unknowns[protein_id_column].dropna().astype(str))\n",
    "    print(f\"Found {len(unknown_ids)} unique unknown protein IDs.\")\n",
    "    if not unknown_ids:\n",
    "        print(\"ERROR: No protein IDs found in the unknowns file. Exiting.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Unknowns CSV file not found at {unknowns_csv_path}\")\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to read or process {unknowns_csv_path}: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "print(f\"\\nExtracting sequences from:\")\n",
    "for f in original_fasta_files:\n",
    "    print(f\"- {f}\")\n",
    "\n",
    "sequences_found = 0\n",
    "sequences_written = 0\n",
    "ids_found = set()\n",
    "\n",
    "try:\n",
    "    with open(output_fasta_path, 'w') as outfile:\n",
    "        for fasta_file in original_fasta_files:\n",
    "            if not os.path.exists(fasta_file):\n",
    "                print(f\"WARNING: Original FASTA file not found, skipping: {fasta_file}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Processing {fasta_file}...\")\n",
    "            try:\n",
    "                for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "                    # --- MODIFIED LINE ---\n",
    "                    # Extract the ID part before the first pipe '|'\n",
    "                    # This assumes your CSV ID matches this part, e.g., 'RLI68853.1'\n",
    "                    current_id = record.id.split('|')[0]\n",
    "                    # --- END MODIFIED LINE ---\n",
    "\n",
    "                    if current_id in unknown_ids:\n",
    "                        sequences_found += 1\n",
    "                        ids_found.add(current_id)\n",
    "                        SeqIO.write(record, outfile, \"fasta\")\n",
    "                        sequences_written += 1\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR: Failed to parse {fasta_file}: {e}\")\n",
    "                # Decide if you want to continue or exit on parse error\n",
    "                # continue\n",
    "\n",
    "except IOError as e:\n",
    "     print(f\"ERROR: Could not write to output file {output_fasta_path}: {e}\")\n",
    "     sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during sequence extraction: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "print(f\"\\nExtraction complete.\")\n",
    "print(f\"Total sequences processed where ID matched an unknown ID: {sequences_found}\")\n",
    "print(f\"Total unique unknown IDs found in FASTA files: {len(ids_found)}\")\n",
    "print(f\"Total sequences written to {output_fasta_path}: {sequences_written}\")\n",
    "\n",
    "# --- Report missing IDs ---\n",
    "missing_ids = unknown_ids - ids_found\n",
    "if missing_ids:\n",
    "    print(f\"\\nWARNING: {len(missing_ids)} unknown IDs were not found in the provided FASTA files.\")\n",
    "    # Optionally print the first few missing IDs\n",
    "    # print(\"Example missing IDs:\", list(missing_ids)[:10])\n",
    "    # Consider writing missing IDs to a file\n",
    "    missing_ids_file = \"missing_unknown_ids.txt\"\n",
    "    try:\n",
    "        with open(missing_ids_file, \"w\") as f_missing:\n",
    "            for mid in sorted(list(missing_ids)):\n",
    "                f_missing.write(mid + \"\\n\")\n",
    "        print(f\"Full list of missing IDs written to {missing_ids_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not write missing IDs file: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nAll unknown IDs were found in the provided FASTA files.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8a3ce2-b615-429b-91b8-0ce1cc70e18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import numpy as np # For checking NaN\n",
    "\n",
    "# --- Configuration: PLEASE UPDATE THESE PATHS ---\n",
    "\n",
    "# Path to the main database CSV file (output from the last classification run)\n",
    "main_database_csv = 'proteome_database_v0.3.csv'\n",
    "\n",
    "# Path to the NEW InterProScan TSV output file (from the run on unknowns)\n",
    "# Adjust the filename if InterProScan generated a different one.\n",
    "new_ipr_tsv = 'InterProScan_Results/unknown_protein_sequences.fasta.tsv'\n",
    "\n",
    "# Path for the updated output CSV file\n",
    "updated_database_csv = 'proteome_database_v0.4.csv'\n",
    "\n",
    "# --- Column Names ---\n",
    "# Adjust these if your main CSV uses different names\n",
    "protein_id_col_main = 'ProteinID'\n",
    "ipr_col_main = 'IPR_Signatures'\n",
    "\n",
    "# --- Script Logic ---\n",
    "\n",
    "print(\"--- Starting Integration of New InterProScan Results ---\")\n",
    "\n",
    "# --- Step 1: Parse the new InterProScan TSV output ---\n",
    "print(f\"Reading new InterProScan results from: {new_ipr_tsv}\")\n",
    "\n",
    "if not os.path.exists(new_ipr_tsv):\n",
    "    print(f\"ERROR: New InterProScan TSV file not found: {new_ipr_tsv}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "try:\n",
    "    # Define column names based on standard InterProScan TSV format\n",
    "    # We only need Protein ID (col 0) and IPR ID (col 11)\n",
    "    col_names = [\n",
    "        'Protein_ID_raw', 'MD5', 'Length', 'Analysis', 'Sig_Acc', 'Sig_Desc',\n",
    "        'Start', 'Stop', 'Score', 'Status', 'Date', 'IPR_ID', 'IPR_Desc',\n",
    "        'GO', 'Pathway'\n",
    "    ]\n",
    "    # Read only necessary columns, specify separator and no header\n",
    "    df_new_ipr = pd.read_csv(\n",
    "        new_ipr_tsv,\n",
    "        sep='\\t',\n",
    "        header=None,\n",
    "        names=col_names,\n",
    "        usecols=['Protein_ID_raw', 'IPR_ID'], # Read raw ID first\n",
    "        dtype={'Protein_ID_raw': str, 'IPR_ID': str} # Read as string initially\n",
    "    )\n",
    "    print(f\"Read {len(df_new_ipr)} lines from TSV.\")\n",
    "\n",
    "    # --- *** ID PARSING FIX *** ---\n",
    "    # Extract the part before the first pipe '|' from the raw protein ID column\n",
    "    print(\"Parsing Protein IDs from TSV (removing extra info)...\")\n",
    "    df_new_ipr['Protein_ID'] = df_new_ipr['Protein_ID_raw'].str.split('|').str[0]\n",
    "    # --- *** END ID PARSING FIX *** ---\n",
    "\n",
    "\n",
    "    # Filter out rows without an IPR ID (often represented as '-')\n",
    "    df_new_ipr.dropna(subset=['IPR_ID'], inplace=True)\n",
    "    df_new_ipr = df_new_ipr[df_new_ipr['IPR_ID'] != '-']\n",
    "    print(f\"Found {len(df_new_ipr)} annotations with IPR IDs.\")\n",
    "\n",
    "    # --- Step 2: Aggregate IPR IDs per Protein ---\n",
    "    # Group by the *parsed* Protein ID and join unique IPR IDs with a semicolon\n",
    "    print(\"Aggregating new IPR IDs per protein...\")\n",
    "    # Ensure IDs are unique before joining\n",
    "    new_ipr_map_series = df_new_ipr.groupby('Protein_ID')['IPR_ID'].apply(lambda x: ';'.join(sorted(x.unique())))\n",
    "\n",
    "    if new_ipr_map_series.empty:\n",
    "         print(\"WARNING: No valid IPR signatures found in the new TSV file after processing.\")\n",
    "         # Decide whether to exit or continue (continuing will just rewrite the main CSV)\n",
    "         # sys.exit(1) # Uncomment to exit if no new IPRs found\n",
    "    else:\n",
    "        print(f\"Aggregated new IPR signatures for {len(new_ipr_map_series)} unique proteins.\")\n",
    "        # Display some examples (optional)\n",
    "        # print(\"\\nExample aggregated signatures:\")\n",
    "        # print(new_ipr_map_series.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: New InterProScan TSV file not found at {new_ipr_tsv}\")\n",
    "    sys.exit(1)\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(f\"ERROR: New InterProScan TSV file is empty: {new_ipr_tsv}\")\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to read or process {new_ipr_tsv}: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# --- Step 3: Load the main database CSV ---\n",
    "print(f\"\\nReading main database CSV: {main_database_csv}\")\n",
    "\n",
    "if not os.path.exists(main_database_csv):\n",
    "    print(f\"ERROR: Main database CSV file not found: {main_database_csv}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "try:\n",
    "    df_main = pd.read_csv(main_database_csv, low_memory=False)\n",
    "    print(f\"Read {len(df_main)} rows from main database.\")\n",
    "\n",
    "    # --- Validate required columns exist ---\n",
    "    if protein_id_col_main not in df_main.columns:\n",
    "        print(f\"ERROR: Protein ID column '{protein_id_col_main}' not found in {main_database_csv}\")\n",
    "        sys.exit(1)\n",
    "    if ipr_col_main not in df_main.columns:\n",
    "        print(f\"ERROR: IPR Signatures column '{ipr_col_main}' not found in {main_database_csv}\")\n",
    "        # If it doesn't exist, create it\n",
    "        print(f\"Creating column '{ipr_col_main}'...\")\n",
    "        df_main[ipr_col_main] = np.nan # Initialize with NaN\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Main database CSV file not found at {main_database_csv}\")\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to read main database CSV {main_database_csv}: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# --- Step 4: Merge/Update IPR Signatures ---\n",
    "print(\"\\nUpdating IPR signatures in the main database...\")\n",
    "\n",
    "# Ensure the IPR column is treated as string for checking emptiness\n",
    "# Use .fillna('') before checking to handle actual NaN values correctly\n",
    "df_main[ipr_col_main] = df_main[ipr_col_main].fillna('').astype(str)\n",
    "\n",
    "# Create a boolean mask for rows where IPR_Signatures is currently empty or effectively 'nan'\n",
    "is_empty_mask = (df_main[ipr_col_main].str.strip() == '') | (df_main[ipr_col_main].str.strip().str.lower() == 'nan')\n",
    "\n",
    "# Get the Protein IDs for the rows that need updating (using the mask)\n",
    "ids_to_update = df_main.loc[is_empty_mask, protein_id_col_main]\n",
    "\n",
    "# Map the new signatures onto these IDs\n",
    "# Use .get() on the series map to handle IDs that might be in the main DB but not have new IPRs\n",
    "# signatures_to_apply = ids_to_update.map(lambda pid: new_ipr_map_series.get(pid)) # Not directly needed for update\n",
    "\n",
    "# Update the main DataFrame only where the original was empty AND new data exists\n",
    "# Create a combined mask: original was empty AND new signature was found\n",
    "update_mask = is_empty_mask & df_main[protein_id_col_main].isin(new_ipr_map_series.index)\n",
    "\n",
    "# Get the number of rows that will actually be updated\n",
    "rows_to_be_updated_count = update_mask.sum()\n",
    "print(f\"Found {rows_to_be_updated_count} rows with previously empty IPR signatures that have new results.\")\n",
    "\n",
    "# Apply the update using .loc\n",
    "# We map directly from the series using the index alignment provided by Protein_ID\n",
    "# Ensure that only rows matching the update_mask are targeted\n",
    "if rows_to_be_updated_count > 0:\n",
    "    df_main.loc[update_mask, ipr_col_main] = df_main.loc[update_mask, protein_id_col_main].map(new_ipr_map_series)\n",
    "    print(\"IPR signature update applied.\")\n",
    "else:\n",
    "    print(\"No rows needed updating.\")\n",
    "\n",
    "\n",
    "# --- Step 5: Save the updated database ---\n",
    "print(f\"\\nSaving updated database to: {updated_database_csv}\")\n",
    "\n",
    "try:\n",
    "    df_main.to_csv(updated_database_csv, index=False)\n",
    "    print(f\"Successfully wrote {len(df_main)} rows to {updated_database_csv}.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to write updated database CSV: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"\\n--- Integration Finished ---\")\n",
    "print(f\"\\nNext Step: Re-run the classification script (add_specific_category_IPR_v10.py) using '{updated_database_csv}' as the input CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b096b81-51e6-452f-8134-b2962af3ab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Integrate USPNet Signal Peptide Predictions\n",
    "#\n",
    "# This cell reads the USPNet prediction results, aligns them with the main protein database using the original input FASTA order, and merges the predictions into the main dataframe.\n",
    "\n",
    "# %% [code]\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration: File Paths ---\n",
    "# Path to the main database CSV file (output from the last step, e.g., taxonomy refinement)\n",
    "main_csv_path = 'proteome_database_v0.6.csv'\n",
    "\n",
    "# Path to the USPNet output CSV file (generated by predict_fast.py)\n",
    "uspnet_results_path = 'USPNet_Intermediate/results.csv' # Default location based on run_uspnet.sh\n",
    "\n",
    "# Path to the FASTA file originally used as input for USPNet\n",
    "input_fasta_path = 'all_proteins_for_dtm.fasta'\n",
    "\n",
    "# Path for the updated output CSV file\n",
    "output_csv_path = 'proteome_database_v0.7.csv'\n",
    "\n",
    "# --- Configuration: Column Names ---\n",
    "# In main input CSV\n",
    "PROTEIN_ID_COL_MAIN = 'ProteinID'\n",
    "# In USPNet output CSV\n",
    "USP_SEQUENCE_COL = 'sequence' # We won't use this directly for merging\n",
    "USP_PRED_TYPE_COL = 'predicted_type'\n",
    "USP_PRED_CLEAVAGE_COL = 'predicted_cleavage'\n",
    "# New columns to create in the output CSV\n",
    "NEW_SP_TYPE_COL = 'Signal_Peptide_USPNet'\n",
    "NEW_SP_CLEAVAGE_COL = 'SP_Cleavage_Site_USPNet'\n",
    "\n",
    "# --- Script Logic ---\n",
    "\n",
    "print(\"--- Starting Integration of USPNet Results ---\")\n",
    "print(f\"Main database CSV: {main_csv_path}\")\n",
    "print(f\"USPNet results CSV: {uspnet_results_path}\")\n",
    "print(f\"Original FASTA input for USPNet: {input_fasta_path}\")\n",
    "print(f\"Output database CSV: {output_csv_path}\")\n",
    "\n",
    "# --- Validate input files ---\n",
    "if not os.path.exists(main_csv_path):\n",
    "    print(f\"ERROR: Main database CSV not found: {main_csv_path}\")\n",
    "    # sys.exit(1) # Use raise instead in a notebook or handle differently\n",
    "    raise FileNotFoundError(f\"Main database CSV not found: {main_csv_path}\")\n",
    "if not os.path.exists(uspnet_results_path):\n",
    "    print(f\"ERROR: USPNet results CSV not found: {uspnet_results_path}\")\n",
    "    raise FileNotFoundError(f\"USPNet results CSV not found: {uspnet_results_path}\")\n",
    "if not os.path.exists(input_fasta_path):\n",
    "    print(f\"ERROR: Input FASTA file not found: {input_fasta_path}\")\n",
    "    raise FileNotFoundError(f\"Input FASTA file not found: {input_fasta_path}\")\n",
    "\n",
    "# --- Step 1: Read Protein IDs from FASTA in order ---\n",
    "print(f\"\\nReading Protein IDs from FASTA: {input_fasta_path}...\")\n",
    "protein_ids_in_order = []\n",
    "try:\n",
    "    for record in SeqIO.parse(input_fasta_path, \"fasta\"):\n",
    "        # Assuming the ID used by USPNet corresponds to record.id\n",
    "        protein_ids_in_order.append(record.id)\n",
    "    print(f\"Read {len(protein_ids_in_order)} IDs from FASTA.\")\n",
    "    if not protein_ids_in_order:\n",
    "        print(\"ERROR: No sequences found in the FASTA file.\")\n",
    "        raise ValueError(\"No sequences found in FASTA file.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to read or parse FASTA file {input_fasta_path}: {e}\")\n",
    "    raise # Re-raise the exception\n",
    "\n",
    "# --- Step 2: Read USPNet results ---\n",
    "print(f\"Reading USPNet results: {uspnet_results_path}...\")\n",
    "try:\n",
    "    df_uspnet = pd.read_csv(uspnet_results_path)\n",
    "    print(f\"Read {len(df_uspnet)} rows from USPNet results.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to read USPNet results CSV {uspnet_results_path}: {e}\")\n",
    "    raise # Re-raise the exception\n",
    "\n",
    "# --- Step 3: Check length consistency and add ProteinID ---\n",
    "if len(protein_ids_in_order) != len(df_uspnet):\n",
    "    print(f\"ERROR: Mismatch in number of sequences!\")\n",
    "    print(f\"  FASTA file '{input_fasta_path}' has {len(protein_ids_in_order)} sequences.\")\n",
    "    print(f\"  USPNet results '{uspnet_results_path}' has {len(df_uspnet)} rows.\")\n",
    "    print(\"Cannot reliably merge results. Please check the inputs.\")\n",
    "    raise ValueError(\"Mismatch between FASTA sequence count and USPNet result count.\")\n",
    "\n",
    "print(\"Adding ProteinID column to USPNet results based on FASTA order...\")\n",
    "df_uspnet[PROTEIN_ID_COL_MAIN] = protein_ids_in_order\n",
    "# Select and rename columns for merging\n",
    "df_uspnet_to_merge = df_uspnet[[PROTEIN_ID_COL_MAIN, USP_PRED_TYPE_COL, USP_PRED_CLEAVAGE_COL]].copy()\n",
    "df_uspnet_to_merge.rename(columns={\n",
    "    USP_PRED_TYPE_COL: NEW_SP_TYPE_COL,\n",
    "    USP_PRED_CLEAVAGE_COL: NEW_SP_CLEAVAGE_COL\n",
    "}, inplace=True)\n",
    "\n",
    "# --- Step 4: Load main database ---\n",
    "print(f\"\\nLoading main database: {main_csv_path}...\")\n",
    "try:\n",
    "    df_main = pd.read_csv(main_csv_path, low_memory=False)\n",
    "    print(f\"Loaded {len(df_main)} rows.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to load main database CSV {main_csv_path}: {e}\")\n",
    "    raise # Re-raise the exception\n",
    "\n",
    "# --- Step 5: Merge USPNet results ---\n",
    "print(f\"Merging USPNet results into main database using '{PROTEIN_ID_COL_MAIN}'...\")\n",
    "\n",
    "# Check if merge columns already exist and remove them first if necessary\n",
    "if NEW_SP_TYPE_COL in df_main.columns:\n",
    "    print(f\"Warning: Column '{NEW_SP_TYPE_COL}' already exists. It will be overwritten.\")\n",
    "    df_main.drop(columns=[NEW_SP_TYPE_COL], inplace=True)\n",
    "if NEW_SP_CLEAVAGE_COL in df_main.columns:\n",
    "    print(f\"Warning: Column '{NEW_SP_CLEAVAGE_COL}' already exists. It will be overwritten.\")\n",
    "    df_main.drop(columns=[NEW_SP_CLEAVAGE_COL], inplace=True)\n",
    "\n",
    "# Perform left merge to keep all rows from the main database\n",
    "df_merged = pd.merge(\n",
    "    df_main,\n",
    "    df_uspnet_to_merge,\n",
    "    on=PROTEIN_ID_COL_MAIN,\n",
    "    how='left' # Keep all rows from df_main\n",
    ")\n",
    "\n",
    "# Check if merge resulted in the same number of rows\n",
    "if len(df_merged) != len(df_main):\n",
    "    print(\"ERROR: Merge resulted in an unexpected number of rows!\")\n",
    "    print(f\" Original rows: {len(df_main)}, Merged rows: {len(df_merged)}\")\n",
    "    raise ValueError(\"Merge changed the number of rows in the dataframe.\")\n",
    "\n",
    "# Report how many rows got USPNet data\n",
    "populated_uspnet = df_merged[NEW_SP_TYPE_COL].notna().sum()\n",
    "print(f\"Successfully merged USPNet data for {populated_uspnet} proteins.\")\n",
    "if populated_uspnet < len(df_uspnet_to_merge):\n",
    "     print(f\"Warning: Some proteins with USPNet results ({len(df_uspnet_to_merge) - populated_uspnet}) were not found in the main database '{main_csv_path}'.\")\n",
    "\n",
    "# --- Step 6: Save Updated CSV ---\n",
    "print(f\"\\nSaving updated database to: {output_csv_path}\")\n",
    "try:\n",
    "    # Ensure new columns are strings, fill potential merge NaNs with empty string\n",
    "    df_merged[NEW_SP_TYPE_COL] = df_merged[NEW_SP_TYPE_COL].fillna('').astype(str)\n",
    "    df_merged[NEW_SP_CLEAVAGE_COL] = df_merged[NEW_SP_CLEAVAGE_COL].fillna('').astype(str)\n",
    "\n",
    "    df_merged.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Successfully wrote {len(df_merged)} rows to {output_csv_path}.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to write updated CSV: {e}\")\n",
    "    raise # Re-raise the exception\n",
    "\n",
    "print(\"\\n--- USPNet Integration Finished ---\")\n",
    "print(f\"\\nNext Step: Integrate DeepTMHMM results into '{output_csv_path}'.\")\n",
    "\n",
    "# Display first few rows of the updated dataframe (optional, common in notebooks)\n",
    "print(\"\\nPreview of updated data:\")\n",
    "display(df_merged.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1335bd8-5a9b-465b-a24e-ce1dc1bb6778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# --- Configuration ---\n",
    "# Set the path to your CSV file\n",
    "csv_file_path = 'proteome_database_v0.7.csv'\n",
    "\n",
    "# Define columns used\n",
    "# Adjust these if your column names are different\n",
    "protein_id_col = 'ProteinID' # Assuming this is your protein identifier column\n",
    "sequence_col = 'Sequence'\n",
    "virus_family_col = 'Virus_Name'\n",
    "archaea_phylum_col = 'Asgard_Phylum' # Or use 'Source_Dataset' if more reliable\n",
    "uspnet_col = 'Signal_Peptide_USPNet'\n",
    "cleavage_site_col = 'SP_Cleavage_Site_USPNet' # Column with the sequence UP TO the cleavage site\n",
    "\n",
    "# Define output columns\n",
    "localization_output_col = 'Predicted_Subcellular_Localization'\n",
    "mature_seq_output_col = 'Mature_Protein_Sequence'\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    print(f\"Successfully loaded '{csv_file_path}'. Shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at '{csv_file_path}'. Please ensure the path is correct.\")\n",
    "    # Stop execution if file not found\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the CSV: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Data Validation ---\n",
    "# Check for essential columns for the entire process\n",
    "required_cols = [\n",
    "    protein_id_col, sequence_col, virus_family_col, archaea_phylum_col,\n",
    "    uspnet_col, cleavage_site_col\n",
    "]\n",
    "missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "if missing_cols:\n",
    "    print(f\"Error: The following required columns are missing from the CSV: {missing_cols}\")\n",
    "    raise KeyError(f\"Missing columns: {missing_cols}\")\n",
    "else:\n",
    "    print(\"All required columns found.\")\n",
    "\n",
    "# --- Define Organism Type ---\n",
    "is_virus = df[virus_family_col].notna()\n",
    "is_archaea = df[archaea_phylum_col].notna() & ~is_virus\n",
    "is_other = ~is_virus & ~is_archaea\n",
    "if is_other.sum() > 0:\n",
    "    warnings.warn(f\"{is_other.sum()} proteins did not clearly map to 'Virus' or 'Archaea'. Localization/Mature sequence might be less specific.\")\n",
    "\n",
    "# --- Define Localization Logic ---\n",
    "conditions = [\n",
    "    is_archaea & (df[uspnet_col] == 'NO_SP'),\n",
    "    is_archaea & (df[uspnet_col].isin(['SP', 'TAT'])),\n",
    "    is_archaea & (df[uspnet_col].isin(['LIPO', 'TATLIPO', 'PILIN'])),\n",
    "    is_virus & (df[uspnet_col] == 'NO_SP'),\n",
    "    is_virus & (df[uspnet_col].isin(['SP', 'TAT'])),\n",
    "    is_virus & (df[uspnet_col].isin(['LIPO', 'TATLIPO', 'PILIN'])),\n",
    "]\n",
    "outputs = [\n",
    "    'Archaea: Cytoplasmic/Membrane (non-SP)',\n",
    "    'Archaea: Secreted/Membrane (Sec/Tat pathway)',\n",
    "    'Archaea: Membrane-associated (Lipoprotein/Pilin)',\n",
    "    'Host: Cytoplasm/Nucleus/Virus Factory',\n",
    "    'Host: Secretory Pathway (Secreted/Membrane/Organelle)',\n",
    "    'Host: Membrane-associated (Lipoprotein/Pilin-like)',\n",
    "]\n",
    "default_output = 'Unknown/Other'\n",
    "\n",
    "# --- Apply Localization Logic ---\n",
    "df[localization_output_col] = np.select(conditions, outputs, default=default_output)\n",
    "print(f\"\\nAdded '{localization_output_col}' column.\")\n",
    "\n",
    "# --- Define Mature Sequence Logic ---\n",
    "def get_mature_sequence(row):\n",
    "    \"\"\"\n",
    "    Calculates the mature protein sequence based on USPNet prediction.\n",
    "    Removes the signal peptide sequence if predicted.\n",
    "    \"\"\"\n",
    "    full_sequence = row[sequence_col]\n",
    "    sp_type = row[uspnet_col]\n",
    "    cleavage_seq = row[cleavage_site_col] # Sequence up to and including cleavage site\n",
    "\n",
    "    # Ensure sequence is a string (handle potential NaNs)\n",
    "    if not isinstance(full_sequence, str):\n",
    "        return None # Or return '', or np.nan depending on desired handling\n",
    "\n",
    "    # Default to the full sequence\n",
    "    mature_sequence = full_sequence\n",
    "\n",
    "    # If a signal peptide is predicted (not NO_SP)\n",
    "    if sp_type != 'NO_SP':\n",
    "        # Check if cleavage site sequence is valid (string, not empty)\n",
    "        if isinstance(cleavage_seq, str) and cleavage_seq:\n",
    "            # Check if the full sequence starts with the cleavage site sequence\n",
    "            if full_sequence.startswith(cleavage_seq):\n",
    "                # Calculate the length of the cleavage site sequence\n",
    "                cleavage_len = len(cleavage_seq)\n",
    "                # Extract the sequence *after* the cleavage site\n",
    "                mature_sequence = full_sequence[cleavage_len:]\n",
    "            else:\n",
    "                # Warning: Predicted SP but cleavage site doesn't match start of sequence\n",
    "                warnings.warn(f\"ProteinID {row[protein_id_col]}: SP predicted ({sp_type}), but cleavage site '{cleavage_seq[:20]}...' not found at start of sequence. Using full sequence.\")\n",
    "                mature_sequence = full_sequence # Fallback to full sequence\n",
    "        else:\n",
    "            # Warning: Predicted SP but cleavage site data is missing/invalid\n",
    "            warnings.warn(f\"ProteinID {row[protein_id_col]}: SP predicted ({sp_type}), but cleavage site data is missing or invalid. Using full sequence.\")\n",
    "            mature_sequence = full_sequence # Fallback to full sequence\n",
    "\n",
    "    return mature_sequence\n",
    "\n",
    "# --- Apply Mature Sequence Logic ---\n",
    "print(\"\\nCalculating Mature Protein Sequences...\")\n",
    "df[mature_seq_output_col] = df.apply(get_mature_sequence, axis=1)\n",
    "print(f\"Added '{mature_seq_output_col}' column.\")\n",
    "\n",
    "# --- Display Results ---\n",
    "print(\"\\nValue Counts for Predicted Subcellular Localization:\")\n",
    "print(df[localization_output_col].value_counts())\n",
    "\n",
    "# Add sequence length columns for comparison\n",
    "df['Original_Seq_Length'] = df[sequence_col].str.len()\n",
    "df['Mature_Seq_Length'] = df[mature_seq_output_col].str.len()\n",
    "\n",
    "# Handle potential None values in Mature_Seq_Length if sequence was None\n",
    "df['Mature_Seq_Length'] = df['Mature_Seq_Length'].fillna(0).astype(int)\n",
    "\n",
    "print(f\"\\nFirst 5 rows showing sequence length changes:\")\n",
    "display_cols = [\n",
    "    protein_id_col,\n",
    "    uspnet_col,\n",
    "    localization_output_col,\n",
    "    'Original_Seq_Length',\n",
    "    'Mature_Seq_Length' # Show lengths instead of full sequences for brevity\n",
    "]\n",
    "# Ensure columns exist before trying to display them\n",
    "display_cols = [col for col in display_cols if col in df.columns]\n",
    "print(df[display_cols].head())\n",
    "\n",
    "# --- Optional: Save the updated DataFrame ---\n",
    "output_csv_path = 'proteome_database_v0.8.csv' # Consider incrementing version\n",
    "try:\n",
    "    # Select columns to save (optional, can save all)\n",
    "    # cols_to_save = [...]\n",
    "    # df.to_csv(output_csv_path, columns=cols_to_save, index=False)\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"\\nSuccessfully saved updated data to '{output_csv_path}'\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving updated data: {e}\")\n",
    "\n",
    "# Clean up temporary length columns if you don't want them saved\n",
    "# df = df.drop(columns=['Original_Seq_Length', 'Mature_Seq_Length'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb8a099-c1f2-4ffa-9cad-26f6d20c89d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44372534-2606-45cc-88e7-0c71aba8946c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration ---\n",
    "# Path to your main data file (output from the previous step)\n",
    "main_csv_path = 'proteome_database_v0.8.csv'\n",
    "# Path to the mapping file provided\n",
    "mapping_file_path = 'mapping_parquet_proteinid_to_uniprotkb_or_upi.tsv'\n",
    "# Column names used for merging and updating\n",
    "protein_id_col = 'ProteinID'\n",
    "uniprot_col = 'UniProtKB_AC'\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    # Load the main dataframe which should include the empty UniProtKB_AC column\n",
    "    df_main = pd.read_csv(main_csv_path)\n",
    "    print(f\"Successfully loaded main data '{main_csv_path}'. Shape: {df_main.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Main data file not found at '{main_csv_path}'. Make sure it was saved correctly in the previous step.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the main CSV: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    # Load the mapping file - assuming it's tab-separated (TSV)\n",
    "    df_mapping = pd.read_csv(mapping_file_path, sep='\\t')\n",
    "    print(f\"Successfully loaded mapping file '{mapping_file_path}'. Shape: {df_mapping.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Mapping file not found at '{mapping_file_path}'. Please ensure the path is correct.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the mapping TSV: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Data Validation ---\n",
    "# Ensure necessary columns exist in both dataframes\n",
    "if protein_id_col not in df_main.columns:\n",
    "    print(f\"Error: Column '{protein_id_col}' not found in main dataframe '{main_csv_path}'.\")\n",
    "    raise KeyError(f\"Missing column: {protein_id_col} in main dataframe\")\n",
    "if uniprot_col not in df_main.columns:\n",
    "    print(f\"Warning: Column '{uniprot_col}' not found in main dataframe '{main_csv_path}'. It will be created.\")\n",
    "    # Add the column if it's missing (though it should exist based on your header)\n",
    "    df_main[uniprot_col] = np.nan\n",
    "\n",
    "if protein_id_col not in df_mapping.columns:\n",
    "    print(f\"Error: Column '{protein_id_col}' not found in mapping dataframe '{mapping_file_path}'.\")\n",
    "    raise KeyError(f\"Missing column: {protein_id_col} in mapping dataframe\")\n",
    "if uniprot_col not in df_mapping.columns:\n",
    "    print(f\"Error: Column '{uniprot_col}' not found in mapping dataframe '{mapping_file_path}'.\")\n",
    "    raise KeyError(f\"Missing column: {uniprot_col} in mapping dataframe\")\n",
    "\n",
    "# --- Prepare for Merge ---\n",
    "# Check how many rows currently have a UniProt AC in the main dataframe\n",
    "# Convert potential empty strings or placeholders to NaN for accurate counting\n",
    "df_main[uniprot_col] = df_main[uniprot_col].replace('', np.nan)\n",
    "initial_filled_count = df_main[uniprot_col].notna().sum()\n",
    "print(f\"\\nInitial count of non-empty '{uniprot_col}' entries in main dataframe: {initial_filled_count}\")\n",
    "\n",
    "# Select only the necessary columns from the mapping file for efficiency\n",
    "df_mapping_subset = df_mapping[[protein_id_col, uniprot_col]].copy()\n",
    "\n",
    "# Handle potential duplicate ProteinIDs in the mapping file.\n",
    "# If a ProteinID maps to multiple UniProt ACs, this keeps the first one found.\n",
    "duplicates_in_mapping = df_mapping_subset[protein_id_col].duplicated().sum()\n",
    "if duplicates_in_mapping > 0:\n",
    "    print(f\"Warning: Found {duplicates_in_mapping} duplicate '{protein_id_col}' entries in the mapping file. Keeping the first occurrence for each.\")\n",
    "    df_mapping_subset = df_mapping_subset.drop_duplicates(subset=[protein_id_col], keep='first')\n",
    "\n",
    "# --- Perform Merge ---\n",
    "# Use a left merge: keep all rows from df_main, add UniProt ACs from df_mapping_subset.\n",
    "# 'suffixes' handles the case where the uniprot_col already exists in df_main.\n",
    "# The original column remains unchanged, the merged data goes into 'UniProtKB_AC_mapped'.\n",
    "df_merged = pd.merge(\n",
    "    df_main,\n",
    "    df_mapping_subset,\n",
    "    on=protein_id_col,\n",
    "    how='left',\n",
    "    suffixes=('', '_mapped') # Suffix for the column coming from the mapping file\n",
    ")\n",
    "\n",
    "# --- Update the UniProtKB_AC Column ---\n",
    "# Check if the merge created the new column with the suffix\n",
    "if uniprot_col + '_mapped' in df_merged.columns:\n",
    "    print(f\"Updating '{uniprot_col}' column with mapped values...\")\n",
    "    # Fill NaN values in the original UniProtKB_AC column using the mapped values.\n",
    "    # This preserves any existing values and only fills where it was originally NaN.\n",
    "    df_merged[uniprot_col] = df_merged[uniprot_col].fillna(df_merged[uniprot_col + '_mapped'])\n",
    "\n",
    "    # Clean up: Drop the temporary mapped column\n",
    "    df_merged = df_merged.drop(columns=[uniprot_col + '_mapped'])\n",
    "else:\n",
    "    # This case is unlikely with the suffixes parameter used correctly, but included as a fallback.\n",
    "    print(f\"Warning: Merge did not create a separate '{uniprot_col}_mapped' column. Check merge logic.\")\n",
    "\n",
    "# Ensure the updated column is treated as string (or object) to handle NaNs gracefully\n",
    "df_merged[uniprot_col] = df_merged[uniprot_col].astype(object)\n",
    "\n",
    "# --- Display Results ---\n",
    "final_filled_count = df_merged[uniprot_col].notna().sum()\n",
    "print(f\"\\nMerge complete.\")\n",
    "print(f\"Final count of non-empty '{uniprot_col}' entries: {final_filled_count}\")\n",
    "print(f\"Number of entries newly filled: {final_filled_count - initial_filled_count}\")\n",
    "\n",
    "# Show some rows where the UniProt AC was potentially filled\n",
    "print(f\"\\nExample rows with '{uniprot_col}' populated (showing first 5):\")\n",
    "print(df_merged[df_merged[uniprot_col].notna()][[protein_id_col, uniprot_col]].head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "# Show some rows where the UniProt AC might still be empty (no match in mapping file)\n",
    "print(f\"\\nExample rows where '{uniprot_col}' might still be empty (showing first 5):\")\n",
    "print(df_merged[df_merged[uniprot_col].isna()][[protein_id_col, uniprot_col]].head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "# --- Save the updated DataFrame ---\n",
    "# Overwrite the original file with the updated data\n",
    "output_csv_path = 'proteome_database_v0.9.csv' \n",
    "try:\n",
    "    df_merged.to_csv(output_csv_path, index=False)\n",
    "    print(f\"\\nSuccessfully saved updated data back to '{output_csv_path}'\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving updated data: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099ad2a1-11c0-45b8-a4ee-247bf17ed6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration ---\n",
    "# Path to your main data file (output from the previous step)\n",
    "main_csv_path = 'proteome_database_v0.8.csv' # Start with v0.8 again\n",
    "# Path to the mapping file provided (ensure this filename is correct)\n",
    "mapping_file_path = 'mapping_parquet_proteinid_to_uniprotkb_or_upi.tsv'\n",
    "# Column names used for merging and updating\n",
    "protein_id_col = 'ProteinID'\n",
    "uniprot_col = 'UniProtKB_AC'\n",
    "# Output file path\n",
    "output_csv_path = 'proteome_database_v0.9.csv'\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    # Load the main dataframe\n",
    "    df_main = pd.read_csv(main_csv_path)\n",
    "    print(f\"Successfully loaded main data '{main_csv_path}'. Shape: {df_main.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Main data file not found at '{main_csv_path}'.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the main CSV: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    # Load the mapping file - assuming it's tab-separated (TSV)\n",
    "    df_mapping = pd.read_csv(mapping_file_path, sep='\\t')\n",
    "    print(f\"Successfully loaded mapping file '{mapping_file_path}'. Shape: {df_mapping.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Mapping file not found at '{mapping_file_path}'.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the mapping TSV: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Data Validation ---\n",
    "# Ensure necessary columns exist in both dataframes\n",
    "required_cols_main = [protein_id_col, uniprot_col]\n",
    "required_cols_map = [protein_id_col, uniprot_col]\n",
    "\n",
    "if not all(col in df_main.columns for col in required_cols_main):\n",
    "    missing = [col for col in required_cols_main if col not in df_main.columns]\n",
    "    print(f\"Error: Columns {missing} not found in main dataframe '{main_csv_path}'.\")\n",
    "    raise KeyError(f\"Missing columns in main dataframe\")\n",
    "\n",
    "if not all(col in df_mapping.columns for col in required_cols_map):\n",
    "    missing = [col for col in required_cols_map if col not in df_mapping.columns]\n",
    "    print(f\"Error: Columns {missing} not found in mapping dataframe '{mapping_file_path}'.\")\n",
    "    raise KeyError(f\"Missing columns in mapping dataframe\")\n",
    "\n",
    "# --- Debugging: Inspect IDs ---\n",
    "print(\"\\n--- Debugging Info ---\")\n",
    "print(f\"First 5 '{protein_id_col}' values from main data:\")\n",
    "print(df_main[protein_id_col].head().to_list())\n",
    "print(f\"\\nFirst 5 '{protein_id_col}' values from mapping data:\")\n",
    "print(df_mapping[protein_id_col].head().to_list())\n",
    "\n",
    "# --- Debugging: Clean IDs (Remove Whitespace) ---\n",
    "# Convert to string and strip whitespace just in case\n",
    "df_main[protein_id_col] = df_main[protein_id_col].astype(str).str.strip()\n",
    "df_mapping[protein_id_col] = df_mapping[protein_id_col].astype(str).str.strip()\n",
    "print(\"\\nApplied .str.strip() to ProteinID columns.\")\n",
    "\n",
    "# --- Debugging: Check Overlap ---\n",
    "main_ids = set(df_main[protein_id_col])\n",
    "mapping_ids = set(df_mapping[protein_id_col])\n",
    "overlapping_ids = main_ids.intersection(mapping_ids)\n",
    "print(f\"\\nNumber of unique ProteinIDs in main data: {len(main_ids)}\")\n",
    "print(f\"Number of unique ProteinIDs in mapping data: {len(mapping_ids)}\")\n",
    "print(f\"Number of ProteinIDs overlapping between the two files: {len(overlapping_ids)}\")\n",
    "\n",
    "if len(overlapping_ids) == 0:\n",
    "    print(\">>> Critical Issue: No ProteinIDs match between the two files! Merge will result in no changes.\")\n",
    "    # Optional: Add logic to strip version numbers if that's suspected\n",
    "    # Example (use with caution):\n",
    "    # df_main[protein_id_col + '_base'] = df_main[protein_id_col].str.split('.').str[0]\n",
    "    # df_mapping[protein_id_col + '_base'] = df_mapping[protein_id_col].str.split('.').str[0]\n",
    "    # Then merge on protein_id_col + '_base'\n",
    "else:\n",
    "     print(\">>> Some overlapping IDs found. Proceeding with merge.\")\n",
    "print(\"--- End Debugging Info ---\")\n",
    "\n",
    "\n",
    "# --- Prepare for Merge ---\n",
    "# Check how many rows currently have a UniProt AC in the main dataframe\n",
    "df_main[uniprot_col] = df_main[uniprot_col].replace('', np.nan) # Ensure empty strings are NaN\n",
    "initial_filled_count = df_main[uniprot_col].notna().sum()\n",
    "print(f\"\\nInitial count of non-empty '{uniprot_col}' entries in main dataframe: {initial_filled_count}\")\n",
    "\n",
    "# Select only the necessary columns from the mapping file\n",
    "df_mapping_subset = df_mapping[[protein_id_col, uniprot_col]].copy()\n",
    "\n",
    "# Handle potential duplicate ProteinIDs in the mapping file.\n",
    "duplicates_in_mapping = df_mapping_subset[protein_id_col].duplicated().sum()\n",
    "if duplicates_in_mapping > 0:\n",
    "    print(f\"Warning: Found {duplicates_in_mapping} duplicate '{protein_id_col}' entries in the mapping file. Keeping the first occurrence for each.\")\n",
    "    df_mapping_subset = df_mapping_subset.drop_duplicates(subset=[protein_id_col], keep='first')\n",
    "\n",
    "# --- Perform Merge ---\n",
    "# Use a left merge\n",
    "df_merged = pd.merge(\n",
    "    df_main,\n",
    "    df_mapping_subset,\n",
    "    on=protein_id_col,\n",
    "    how='left',\n",
    "    suffixes=('', '_mapped') # Suffix for the column coming from the mapping file\n",
    ")\n",
    "\n",
    "# --- Update the UniProtKB_AC Column ---\n",
    "if uniprot_col + '_mapped' in df_merged.columns:\n",
    "    print(f\"Updating '{uniprot_col}' column with mapped values...\")\n",
    "    # Use .fillna() to update only where the original column is NaN\n",
    "    df_merged[uniprot_col] = df_merged[uniprot_col].fillna(df_merged[uniprot_col + '_mapped'])\n",
    "    # Clean up: Drop the temporary mapped column\n",
    "    df_merged = df_merged.drop(columns=[uniprot_col + '_mapped'])\n",
    "else:\n",
    "    print(f\"Warning: Merge did not create a separate '{uniprot_col}_mapped' column.\")\n",
    "\n",
    "# Ensure the updated column is treated as string (or object)\n",
    "df_merged[uniprot_col] = df_merged[uniprot_col].astype(object)\n",
    "\n",
    "# --- Display Results ---\n",
    "final_filled_count = df_merged[uniprot_col].notna().sum()\n",
    "print(f\"\\nMerge complete.\")\n",
    "print(f\"Final count of non-empty '{uniprot_col}' entries: {final_filled_count}\")\n",
    "print(f\"Number of entries newly filled: {final_filled_count - initial_filled_count}\")\n",
    "\n",
    "# Show some rows where the UniProt AC was potentially filled\n",
    "print(f\"\\nExample rows with '{uniprot_col}' populated (showing first 5):\")\n",
    "print(df_merged[df_merged[uniprot_col].notna()][[protein_id_col, uniprot_col]].head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "# Show some rows where the UniProt AC might still be empty\n",
    "print(f\"\\nExample rows where '{uniprot_col}' might still be empty (showing first 5):\")\n",
    "print(df_merged[df_merged[uniprot_col].isna()][[protein_id_col, uniprot_col]].head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "# --- Save the updated DataFrame ---\n",
    "try:\n",
    "    df_merged.to_csv(output_csv_path, index=False)\n",
    "    print(f\"\\nSuccessfully saved updated data back to '{output_csv_path}'\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving updated data: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f138a7-b9f2-42c0-b3b8-efb4aa29f19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration ---\n",
    "# Path to your main data file (output from the previous step)\n",
    "main_csv_path = 'proteome_database_v0.8.csv' # Start with v0.8 again\n",
    "# Path to the mapping file provided (ensure this filename is correct)\n",
    "mapping_file_path = 'mapping_parquet_proteinid_to_uniprotkb_or_upi.tsv'\n",
    "# Column names used for merging and updating\n",
    "protein_id_col = 'ProteinID'\n",
    "uniprot_col = 'UniProtKB_AC'\n",
    "# Temporary column for merging based on base ID (without version suffix)\n",
    "base_id_col = protein_id_col + '_base'\n",
    "# Output file path\n",
    "output_csv_path = 'proteome_database_v0.9.csv'\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    # Load the main dataframe\n",
    "    df_main = pd.read_csv(main_csv_path)\n",
    "    print(f\"Successfully loaded main data '{main_csv_path}'. Shape: {df_main.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Main data file not found at '{main_csv_path}'.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the main CSV: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    # Load the mapping file - assuming it's tab-separated (TSV)\n",
    "    df_mapping = pd.read_csv(mapping_file_path, sep='\\t')\n",
    "    print(f\"Successfully loaded mapping file '{mapping_file_path}'. Shape: {df_mapping.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Mapping file not found at '{mapping_file_path}'.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the mapping TSV: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Data Validation ---\n",
    "# Ensure necessary columns exist in both dataframes\n",
    "required_cols_main = [protein_id_col, uniprot_col]\n",
    "required_cols_map = [protein_id_col, uniprot_col]\n",
    "\n",
    "if not all(col in df_main.columns for col in required_cols_main):\n",
    "    missing = [col for col in required_cols_main if col not in df_main.columns]\n",
    "    print(f\"Error: Columns {missing} not found in main dataframe '{main_csv_path}'.\")\n",
    "    raise KeyError(f\"Missing columns in main dataframe\")\n",
    "\n",
    "if not all(col in df_mapping.columns for col in required_cols_map):\n",
    "    missing = [col for col in required_cols_map if col not in df_mapping.columns]\n",
    "    print(f\"Error: Columns {missing} not found in mapping dataframe '{mapping_file_path}'.\")\n",
    "    raise KeyError(f\"Missing columns in mapping dataframe\")\n",
    "\n",
    "# --- Create Base ID for Merging ---\n",
    "print(\"\\nCreating base ID columns (without version suffix) for merging...\")\n",
    "# Main data: Split ProteinID at '.' and take the first part\n",
    "df_main[base_id_col] = df_main[protein_id_col].astype(str).str.split('.', n=1).str[0]\n",
    "# Mapping data: Assume it already lacks suffix, just copy\n",
    "df_mapping[base_id_col] = df_mapping[protein_id_col].astype(str).str.strip() # Ensure clean\n",
    "\n",
    "# --- Debugging: Inspect IDs ---\n",
    "print(\"\\n--- Debugging Info ---\")\n",
    "print(f\"First 5 '{protein_id_col}' values from main data:\")\n",
    "print(df_main[protein_id_col].head().to_list())\n",
    "print(f\"First 5 '{base_id_col}' values from main data:\")\n",
    "print(df_main[base_id_col].head().to_list())\n",
    "\n",
    "print(f\"\\nFirst 5 '{protein_id_col}' values from mapping data:\")\n",
    "print(df_mapping[protein_id_col].head().to_list())\n",
    "print(f\"First 5 '{base_id_col}' values from mapping data:\")\n",
    "print(df_mapping[base_id_col].head().to_list())\n",
    "\n",
    "\n",
    "# --- Debugging: Check Overlap using Base ID ---\n",
    "main_base_ids = set(df_main[base_id_col])\n",
    "mapping_base_ids = set(df_mapping[base_id_col])\n",
    "overlapping_base_ids = main_base_ids.intersection(mapping_base_ids)\n",
    "print(f\"\\nNumber of unique Base IDs in main data: {len(main_base_ids)}\")\n",
    "print(f\"Number of unique Base IDs in mapping data: {len(mapping_base_ids)}\")\n",
    "print(f\"Number of Base IDs overlapping between the two files: {len(overlapping_base_ids)}\")\n",
    "\n",
    "if len(overlapping_base_ids) == 0:\n",
    "    print(\">>> Critical Issue: No Base IDs match between the two files! Merge will result in no changes.\")\n",
    "else:\n",
    "     print(\">>> Some overlapping Base IDs found. Proceeding with merge.\")\n",
    "print(\"--- End Debugging Info ---\")\n",
    "\n",
    "\n",
    "# --- Prepare for Merge ---\n",
    "# Check how many rows currently have a UniProt AC in the main dataframe\n",
    "df_main[uniprot_col] = df_main[uniprot_col].replace('', np.nan) # Ensure empty strings are NaN\n",
    "initial_filled_count = df_main[uniprot_col].notna().sum()\n",
    "print(f\"\\nInitial count of non-empty '{uniprot_col}' entries in main dataframe: {initial_filled_count}\")\n",
    "\n",
    "# Select only the necessary columns from the mapping file for the merge\n",
    "# Include the base_id_col and the uniprot_col\n",
    "df_mapping_subset = df_mapping[[base_id_col, uniprot_col]].copy()\n",
    "\n",
    "# Handle potential duplicate Base IDs in the mapping file.\n",
    "# If a Base ID maps to multiple UniProt ACs, this keeps the first one found.\n",
    "duplicates_in_mapping = df_mapping_subset[base_id_col].duplicated().sum()\n",
    "if duplicates_in_mapping > 0:\n",
    "    print(f\"Warning: Found {duplicates_in_mapping} duplicate '{base_id_col}' entries in the mapping file. Keeping the first occurrence for each.\")\n",
    "    df_mapping_subset = df_mapping_subset.drop_duplicates(subset=[base_id_col], keep='first')\n",
    "\n",
    "# --- Perform Merge on Base ID ---\n",
    "# Use a left merge: keep all rows from df_main, add UniProt ACs based on base_id_col match.\n",
    "print(f\"\\nPerforming merge on '{base_id_col}'...\")\n",
    "df_merged = pd.merge(\n",
    "    df_main,\n",
    "    df_mapping_subset,\n",
    "    on=base_id_col, # Merge using the base ID column\n",
    "    how='left',\n",
    "    suffixes=('', '_mapped') # Suffix for the UniProtKB_AC column coming from mapping\n",
    ")\n",
    "\n",
    "# --- Update the UniProtKB_AC Column ---\n",
    "if uniprot_col + '_mapped' in df_merged.columns:\n",
    "    print(f\"Updating '{uniprot_col}' column with mapped values...\")\n",
    "    # Use .fillna() to update only where the original column is NaN\n",
    "    # This prevents overwriting any existing values if df_main already had some\n",
    "    df_merged[uniprot_col] = df_merged[uniprot_col].fillna(df_merged[uniprot_col + '_mapped'])\n",
    "    # Clean up: Drop the temporary mapped column\n",
    "    df_merged = df_merged.drop(columns=[uniprot_col + '_mapped'])\n",
    "else:\n",
    "    print(f\"Warning: Merge did not create a separate '{uniprot_col}_mapped' column.\")\n",
    "\n",
    "# Clean up the temporary base ID column from the final dataframe\n",
    "if base_id_col in df_merged.columns:\n",
    "    df_merged = df_merged.drop(columns=[base_id_col])\n",
    "    print(f\"Removed temporary column '{base_id_col}'.\")\n",
    "\n",
    "\n",
    "# Ensure the updated column is treated as string (or object)\n",
    "df_merged[uniprot_col] = df_merged[uniprot_col].astype(object)\n",
    "\n",
    "# --- Display Results ---\n",
    "final_filled_count = df_merged[uniprot_col].notna().sum()\n",
    "print(f\"\\nMerge complete.\")\n",
    "print(f\"Final count of non-empty '{uniprot_col}' entries: {final_filled_count}\")\n",
    "print(f\"Number of entries newly filled: {final_filled_count - initial_filled_count}\")\n",
    "\n",
    "# Show some rows where the UniProt AC was potentially filled\n",
    "print(f\"\\nExample rows with '{uniprot_col}' populated (showing first 5):\")\n",
    "print(df_merged[df_merged[uniprot_col].notna()][[protein_id_col, uniprot_col]].head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "# Show some rows where the UniProt AC might still be empty\n",
    "print(f\"\\nExample rows where '{uniprot_col}' might still be empty (showing first 5):\")\n",
    "print(df_merged[df_merged[uniprot_col].isna()][[protein_id_col, uniprot_col]].head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "# --- Save the updated DataFrame ---\n",
    "try:\n",
    "    df_merged.to_csv(output_csv_path, index=False)\n",
    "    print(f\"\\nSuccessfully saved updated data back to '{output_csv_path}'\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving updated data: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63131f2-96a6-4ccd-b748-2beed0fa6f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration ---\n",
    "# Path to your updated main data file\n",
    "main_csv_path = 'proteome_database_v0.9.csv'\n",
    "# Path to the file containing UniProt ACs found in AFDB\n",
    "afdb_uniprot_list_path = 'PDB_AFDB_screening/afdb_processing_output/afdb_found_uniprot_acs_or_upi.csv'\n",
    "# Column names\n",
    "uniprot_col = 'UniProtKB_AC'\n",
    "protein_id_col = 'ProteinID' # Used for displaying examples\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    # Load the main dataframe (which now has UniProtKB_AC populated)\n",
    "    df_main = pd.read_csv(main_csv_path)\n",
    "    print(f\"Successfully loaded main data '{main_csv_path}'. Shape: {df_main.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Main data file not found at '{main_csv_path}'.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the main CSV: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    # Load the AFDB UniProt list file\n",
    "    # Assuming it's a CSV and the UniProt IDs are in the first column (index 0)\n",
    "    # Adjust 'header=None' and 'usecols=[0]' if the file has headers or a different structure\n",
    "    df_afdb_uniprot = pd.read_csv(afdb_uniprot_list_path, header=None, usecols=[0], names=[uniprot_col])\n",
    "    print(f\"Successfully loaded AFDB UniProt list '{afdb_uniprot_list_path}'. Shape: {df_afdb_uniprot.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: AFDB UniProt list file not found at '{afdb_uniprot_list_path}'.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the AFDB UniProt list CSV: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Process and Count ---\n",
    "# Get the set of unique UniProt ACs from the AFDB list\n",
    "# Drop any potential NaN values and ensure they are strings\n",
    "afdb_uniprot_set = set(df_afdb_uniprot[uniprot_col].dropna().astype(str))\n",
    "print(f\"\\nFound {len(afdb_uniprot_set)} unique UniProt ACs in the AFDB list file.\")\n",
    "\n",
    "# Filter the main dataframe:\n",
    "# 1. Keep rows where UniProtKB_AC is not NaN/null\n",
    "# 2. Keep rows where the UniProtKB_AC is present in the afdb_uniprot_set\n",
    "df_main_filtered = df_main[\n",
    "    df_main[uniprot_col].notna() &\n",
    "    df_main[uniprot_col].astype(str).isin(afdb_uniprot_set)\n",
    "].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "# Count the number of rows in the filtered dataframe\n",
    "overlap_count = len(df_main_filtered)\n",
    "total_with_uniprot = df_main[uniprot_col].notna().sum()\n",
    "\n",
    "print(f\"\\nTotal entries in main database with a UniProtKB_AC: {total_with_uniprot}\")\n",
    "print(f\"Number of entries whose UniProtKB_AC was found in the AFDB list: {overlap_count}\")\n",
    "\n",
    "if total_with_uniprot > 0:\n",
    "    percentage = (overlap_count / total_with_uniprot) * 100\n",
    "    print(f\"Percentage of mapped entries found in AFDB list: {percentage:.2f}%\")\n",
    "\n",
    "# Display a few examples of overlapping proteins\n",
    "print(\"\\nExample ProteinIDs whose UniProtKB_AC matched the AFDB list (first 5):\")\n",
    "print(df_main_filtered[[protein_id_col, uniprot_col]].head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8755ffa4-4fea-43bb-876e-abe3e8a8dc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re # Import regular expressions for UniProt ID check\n",
    "\n",
    "# --- Configuration ---\n",
    "# Path to the AFESM cluster metadata file (unzipped)\n",
    "cluster_meta_path = '2-repID_isOnlyESM_nMem_nAllMem_repPlddt_avgPlddt_avgAllPlddt_repLen_avgLen_avgAllLen_LCAtaxID_nBiome_LCBID.tsv'\n",
    "# Path to the AFESM cluster membership file (unzipped)\n",
    "cluster_members_path = '1-AFESMClusters-repId_memId_cluFlag_taxId_biomeId.tsv' # File 1 from description, assuming entryId is memId based on docs\n",
    "# Output file for the filtered UniProt IDs\n",
    "output_ids_path = 'afesm_esm_only_uniprot_ids.txt'\n",
    "\n",
    "# Define expected column names for AFESM files based on documentation\n",
    "# File 2: Cluster Metadata\n",
    "meta_cols = [\n",
    "    'repID', 'isOnlyESM', 'nMem', 'nAllMem', 'repPlddt', 'avgPlddt',\n",
    "    'avgAllPlddt', 'repLen', 'avgLen', 'avgAllLen', 'LCAtaxID',\n",
    "    'nBiome', 'LCBID'\n",
    "]\n",
    "# File 1: Cluster Membership - Assuming entryId is the member ID based on file name\n",
    "members_cols = ['repId', 'memId', 'cluFlag', 'taxId', 'biomeID'] # Using File 1 description, mapping entryId to memId\n",
    "\n",
    "# Variables to hold specific column names used in the script\n",
    "afesm_rep_id_col = 'repID' # Use 'repID' from meta_cols for consistency\n",
    "afesm_member_id_col = 'memId' # From members_cols\n",
    "afesm_is_esm_only_col = 'isOnlyESM' # From meta_cols\n",
    "\n",
    "# --- Load Cluster Metadata ---\n",
    "try:\n",
    "    # Load cluster metadata, specifying NO header and assigning names.\n",
    "    df_meta = pd.read_csv(cluster_meta_path, sep='\\t', header=None, names=meta_cols)\n",
    "    print(f\"Successfully loaded cluster metadata '{cluster_meta_path}' (no header, unzipped). Shape: {df_meta.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Cluster metadata file not found at '{cluster_meta_path}'.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading cluster metadata: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Identify ESM-only Cluster Representatives ---\n",
    "# Ensure the isOnlyESM column is numeric\n",
    "df_meta[afesm_is_esm_only_col] = pd.to_numeric(df_meta[afesm_is_esm_only_col], errors='coerce')\n",
    "esm_only_reps = set(df_meta[df_meta[afesm_is_esm_only_col] == 1][afesm_rep_id_col].unique())\n",
    "print(f\"\\nIdentified {len(esm_only_reps)} representative IDs for ESM-only clusters.\")\n",
    "\n",
    "# Free up memory from df_meta as it's no longer needed\n",
    "del df_meta\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"Freed memory from metadata dataframe.\")\n",
    "\n",
    "if not esm_only_reps:\n",
    "    print(\"Warning: No ESM-only cluster representatives found. Stopping.\")\n",
    "    # Exit if no reps found, otherwise the next step processes everything\n",
    "    exit()\n",
    "\n",
    "# --- Process Cluster Members in Chunks (to save memory) ---\n",
    "print(f\"\\nProcessing cluster members file '{cluster_members_path}' in chunks...\")\n",
    "chunk_size = 10_000_000 # Process 10 million rows at a time, adjust if needed\n",
    "esm_only_uniprot_ids_set = set()\n",
    "processed_rows = 0\n",
    "\n",
    "try:\n",
    "    # Use iterator=True and get_chunk to read the large file piece by piece\n",
    "    chunk_iterator = pd.read_csv(\n",
    "        cluster_members_path,\n",
    "        sep='\\t',\n",
    "        header=None,\n",
    "        names=members_cols,\n",
    "        low_memory=False,\n",
    "        iterator=True,\n",
    "        chunksize=chunk_size\n",
    "    )\n",
    "\n",
    "    for i, chunk in enumerate(chunk_iterator):\n",
    "        processed_rows += len(chunk)\n",
    "        print(f\"  Processing chunk {i+1} (Rows {processed_rows - chunk_size + 1} - {processed_rows})...\")\n",
    "\n",
    "        # Ensure correct data types for filtering\n",
    "        chunk['repId'] = chunk['repId'].astype(str)\n",
    "        chunk[afesm_member_id_col] = chunk[afesm_member_id_col].astype(str)\n",
    "\n",
    "        # Filter the chunk for members belonging to ESM-only clusters\n",
    "        esm_only_members_chunk = chunk[chunk['repId'].isin(esm_only_reps)]\n",
    "\n",
    "        # Extract member IDs from this chunk\n",
    "        member_ids_chunk = set(esm_only_members_chunk[afesm_member_id_col].dropna())\n",
    "\n",
    "        # Filter for potential UniProt IDs and add to the main set\n",
    "        for mem_id in member_ids_chunk:\n",
    "             if not mem_id.startswith('MGYP') and not mem_id.startswith('pdb|'):\n",
    "                 esm_only_uniprot_ids_set.add(mem_id)\n",
    "\n",
    "        print(f\"    Found {len(member_ids_chunk)} members in ESM-only clusters in this chunk.\")\n",
    "        print(f\"    Current total unique potential UniProt IDs found: {len(esm_only_uniprot_ids_set)}\")\n",
    "\n",
    "    print(f\"\\nFinished processing {processed_rows} rows from cluster members file.\")\n",
    "    print(f\"Total unique potential UniProt IDs found in ESM-only clusters: {len(esm_only_uniprot_ids_set)}\")\n",
    "\n",
    "    # --- Save the Filtered IDs to a File ---\n",
    "    print(f\"\\nSaving filtered UniProt IDs to '{output_ids_path}'...\")\n",
    "    with open(output_ids_path, 'w') as f_out:\n",
    "        for uniprot_id in sorted(list(esm_only_uniprot_ids_set)): # Sort for consistency\n",
    "            f_out.write(f\"{uniprot_id}\\n\")\n",
    "    print(\"Save complete.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Cluster members file not found at '{cluster_members_path}'.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while processing cluster members: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab15e532-6eca-4dc1-8bbb-5fa587f50b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration ---\n",
    "# Path to your main data file\n",
    "main_csv_path = 'proteome_database_v0.9.csv'\n",
    "# Path to the filtered list of ESM-only UniProt IDs created in the previous step\n",
    "afesm_ids_path = 'afesm_esm_only_uniprot_ids.txt'\n",
    "# Column names in your main file\n",
    "protein_id_col = 'ProteinID'\n",
    "uniprot_col = 'UniProtKB_AC'\n",
    "# Optional: Output file for your proteins that matched\n",
    "output_match_path = 'proteins_hitting_afesm_esm_only_uniprot.csv'\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    df_main = pd.read_csv(main_csv_path)\n",
    "    print(f\"Successfully loaded main data '{main_csv_path}'. Shape: {df_main.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Main data file not found at '{main_csv_path}'.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the main CSV: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    # Load the filtered AFESM UniProt IDs into a set for efficient lookup\n",
    "    print(f\"Loading filtered AFESM UniProt IDs from '{afesm_ids_path}'...\")\n",
    "    with open(afesm_ids_path, 'r') as f_in:\n",
    "        # Read lines, strip whitespace/newlines, and add to set\n",
    "        afesm_esm_only_uniprot_ids_set = {line.strip() for line in f_in if line.strip()}\n",
    "    print(f\"Successfully loaded {len(afesm_esm_only_uniprot_ids_set)} unique IDs into a set.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: AFESM ID list file not found at '{afesm_ids_path}'. Make sure the previous step ran correctly.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the AFESM ID list: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Compare Your Database UniProt IDs to the AFESM Set ---\n",
    "# Get the set of non-null UniProt ACs from your main database\n",
    "main_uniprot_ids = set(df_main[uniprot_col].dropna().astype(str))\n",
    "print(f\"\\nFound {len(main_uniprot_ids)} unique UniProt ACs in your main database.\")\n",
    "\n",
    "# Find the intersection\n",
    "overlapping_uniprot_ids = main_uniprot_ids.intersection(afesm_esm_only_uniprot_ids_set)\n",
    "overlap_count = len(overlapping_uniprot_ids)\n",
    "\n",
    "print(f\"\\nNumber of your UniProtKB_ACs found within the AFESM ESM-only UniProt ID set: {overlap_count}\")\n",
    "\n",
    "if len(main_uniprot_ids) > 0:\n",
    "    percentage = (overlap_count / len(main_uniprot_ids)) * 100\n",
    "    print(f\"Percentage of your mapped UniProt IDs found in the AFESM ESM-only set: {percentage:.2f}%\")\n",
    "else:\n",
    "    print(\"No UniProt IDs found in your main database to compare.\")\n",
    "\n",
    "# --- Display Results & Save ---\n",
    "# Filter the main dataframe to show the proteins that matched\n",
    "df_main_matched = df_main[df_main[uniprot_col].astype(str).isin(overlapping_uniprot_ids)].copy()\n",
    "\n",
    "print(\"\\nExample ProteinIDs whose UniProtKB_AC matched the AFESM ESM-only set (first 10):\")\n",
    "print(df_main_matched[[protein_id_col, uniprot_col]].head(10).to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "# Save the list of your ProteinIDs that hit the ESM-only set\n",
    "if not df_main_matched.empty:\n",
    "    try:\n",
    "        df_main_matched[[protein_id_col, uniprot_col]].to_csv(output_match_path, index=False)\n",
    "        print(f\"\\nSaved list of matching ProteinIDs ({overlap_count} entries) to '{output_match_path}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving matching ProteinIDs: {e}\")\n",
    "else:\n",
    "    print(\"\\nNo matching proteins found to save.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f02d50-d57d-4e6f-905b-b36e4be206f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration ---\n",
    "# Path to your main data file\n",
    "main_csv_path = 'proteome_database_v0.9.csv'\n",
    "# Path to the filtered list of ESM-only UniProt IDs created in the previous step\n",
    "afesm_ids_path = 'afesm_esm_only_uniprot_ids.txt'\n",
    "# Column names in your main file\n",
    "protein_id_col = 'ProteinID'\n",
    "uniprot_col = 'UniProtKB_AC'\n",
    "# Optional: Output file for your proteins that matched\n",
    "output_match_path = 'proteins_hitting_afesm_esm_only_uniprot.csv'\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    df_main = pd.read_csv(main_csv_path)\n",
    "    print(f\"Successfully loaded main data '{main_csv_path}'. Shape: {df_main.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Main data file not found at '{main_csv_path}'.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the main CSV: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    # Load the filtered AFESM UniProt IDs into a set for efficient lookup\n",
    "    print(f\"Loading filtered AFESM UniProt IDs from '{afesm_ids_path}'...\")\n",
    "    afesm_esm_only_uniprot_ids_list = [] # Load into list first for inspection\n",
    "    with open(afesm_ids_path, 'r') as f_in:\n",
    "        for line in f_in:\n",
    "            stripped_line = line.strip()\n",
    "            if stripped_line: # Ensure non-empty lines\n",
    "                afesm_esm_only_uniprot_ids_list.append(stripped_line)\n",
    "\n",
    "    afesm_esm_only_uniprot_ids_set = set(afesm_esm_only_uniprot_ids_list)\n",
    "    print(f\"Successfully loaded {len(afesm_esm_only_uniprot_ids_set)} unique IDs into a set.\")\n",
    "\n",
    "    # --- Debugging: Print first few loaded IDs ---\n",
    "    print(\"\\n--- Debugging Info ---\")\n",
    "    print(\"First 10 UniProt IDs loaded from AFESM list file:\")\n",
    "    print(afesm_esm_only_uniprot_ids_list[:10])\n",
    "    print(\"--- End Debugging Info ---\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: AFESM ID list file not found at '{afesm_ids_path}'. Make sure the previous step ran correctly.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the AFESM ID list: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Compare Your Database UniProt IDs to the AFESM Set ---\n",
    "# Get the set of non-null UniProt ACs from your main database\n",
    "main_uniprot_list = df_main[uniprot_col].dropna().astype(str).tolist() # Get as list first\n",
    "main_uniprot_ids = set(main_uniprot_list)\n",
    "print(f\"\\nFound {len(main_uniprot_ids)} unique UniProt ACs in your main database.\")\n",
    "\n",
    "# --- Debugging: Print first few loaded IDs from main DB ---\n",
    "print(\"\\n--- Debugging Info ---\")\n",
    "print(\"First 10 non-null UniProt IDs loaded from main database:\")\n",
    "# Ensure list isn't empty before slicing\n",
    "print(main_uniprot_list[:10] if main_uniprot_list else \"No UniProt IDs found in main DB\")\n",
    "print(\"--- End Debugging Info ---\")\n",
    "\n",
    "# Find the intersection\n",
    "print(\"\\nPerforming intersection...\")\n",
    "overlapping_uniprot_ids = main_uniprot_ids.intersection(afesm_esm_only_uniprot_ids_set)\n",
    "overlap_count = len(overlapping_uniprot_ids)\n",
    "\n",
    "print(f\"\\nNumber of your UniProtKB_ACs found within the AFESM ESM-only UniProt ID set: {overlap_count}\")\n",
    "\n",
    "if len(main_uniprot_ids) > 0:\n",
    "    percentage = (overlap_count / len(main_uniprot_ids)) * 100\n",
    "    print(f\"Percentage of your mapped UniProt IDs found in the AFESM ESM-only set: {percentage:.2f}%\")\n",
    "else:\n",
    "    print(\"No UniProt IDs found in your main database to compare.\")\n",
    "\n",
    "# --- Display Results & Save ---\n",
    "# Filter the main dataframe to show the proteins that matched\n",
    "df_main_matched = df_main[df_main[uniprot_col].astype(str).isin(overlapping_uniprot_ids)].copy()\n",
    "\n",
    "print(\"\\nExample ProteinIDs whose UniProtKB_AC matched the AFESM ESM-only set (first 10):\")\n",
    "print(df_main_matched[[protein_id_col, uniprot_col]].head(10).to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "# Save the list of your ProteinIDs that hit the ESM-only set\n",
    "if not df_main_matched.empty:\n",
    "    try:\n",
    "        df_main_matched[[protein_id_col, uniprot_col]].to_csv(output_match_path, index=False)\n",
    "        print(f\"\\nSaved list of matching ProteinIDs ({overlap_count} entries) to '{output_match_path}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving matching ProteinIDs: {e}\")\n",
    "else:\n",
    "    print(\"\\nNo matching proteins found to save.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e3dcde-9460-4737-b18a-b95394019a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration ---\n",
    "# Path to your main data file\n",
    "main_csv_path = 'proteome_database_v0.9.csv'\n",
    "# Path to the filtered list of ESM-only UniProt IDs created in the previous step\n",
    "afesm_ids_path = 'afesm_esm_only_uniprot_ids.txt'\n",
    "# Column names in your main file\n",
    "protein_id_col = 'ProteinID'\n",
    "uniprot_col = 'UniProtKB_AC'\n",
    "# Optional: Output file for your proteins that matched\n",
    "output_match_path = 'proteins_hitting_afesm_esm_only_uniprot.csv'\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    df_main = pd.read_csv(main_csv_path)\n",
    "    print(f\"Successfully loaded main data '{main_csv_path}'. Shape: {df_main.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Main data file not found at '{main_csv_path}'.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the main CSV: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    # Load the filtered AFESM UniProt IDs into a set for efficient lookup\n",
    "    print(f\"Loading filtered AFESM UniProt IDs from '{afesm_ids_path}'...\")\n",
    "    afesm_esm_only_uniprot_ids_list = [] # Load into list first for inspection\n",
    "    with open(afesm_ids_path, 'r') as f_in:\n",
    "        for line in f_in:\n",
    "            stripped_line = line.strip()\n",
    "            if stripped_line: # Ensure non-empty lines\n",
    "                afesm_esm_only_uniprot_ids_list.append(stripped_line)\n",
    "\n",
    "    # Convert to uppercase when creating the set\n",
    "    afesm_esm_only_uniprot_ids_set = {item.upper() for item in afesm_esm_only_uniprot_ids_list}\n",
    "    print(f\"Successfully loaded {len(afesm_esm_only_uniprot_ids_set)} unique UPPERCASE IDs into a set.\")\n",
    "\n",
    "    # --- Debugging: Print first few loaded IDs ---\n",
    "    print(\"\\n--- Debugging Info ---\")\n",
    "    print(\"First 10 UniProt IDs loaded from AFESM list file (as read):\")\n",
    "    print(afesm_esm_only_uniprot_ids_list[:10])\n",
    "    print(\"First 10 UniProt IDs from AFESM list file (UPPERCASE set sample):\")\n",
    "    # Convert set back to list temporarily for slicing sample\n",
    "    print(list(afesm_esm_only_uniprot_ids_set)[:10])\n",
    "    print(\"--- End Debugging Info ---\")\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: AFESM ID list file not found at '{afesm_ids_path}'. Make sure the previous step ran correctly.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the AFESM ID list: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Compare Your Database UniProt IDs to the AFESM Set ---\n",
    "# Get the set of non-null UniProt ACs from your main database, convert to uppercase\n",
    "main_uniprot_list = df_main[uniprot_col].dropna().astype(str).tolist() # Get as list first\n",
    "main_uniprot_ids = {item.upper() for item in main_uniprot_list} # Convert to uppercase set\n",
    "print(f\"\\nFound {len(main_uniprot_ids)} unique UPPERCASE UniProt ACs in your main database.\")\n",
    "\n",
    "# --- Debugging: Print first few loaded IDs from main DB ---\n",
    "print(\"\\n--- Debugging Info ---\")\n",
    "print(\"First 10 non-null UniProt IDs loaded from main database (as read):\")\n",
    "print(main_uniprot_list[:10] if main_uniprot_list else \"No UniProt IDs found in main DB\")\n",
    "print(\"First 10 non-null UniProt IDs from main database (UPPERCASE set sample):\")\n",
    "print(list(main_uniprot_ids)[:10] if main_uniprot_ids else \"No UniProt IDs found in main DB\")\n",
    "print(\"--- End Debugging Info ---\")\n",
    "\n",
    "\n",
    "# Find the intersection\n",
    "print(\"\\nPerforming intersection...\")\n",
    "overlapping_uniprot_ids = main_uniprot_ids.intersection(afesm_esm_only_uniprot_ids_set)\n",
    "overlap_count = len(overlapping_uniprot_ids)\n",
    "\n",
    "print(f\"\\nNumber of your UniProtKB_ACs found within the AFESM ESM-only UniProt ID set: {overlap_count}\")\n",
    "\n",
    "if len(main_uniprot_ids) > 0:\n",
    "    percentage = (overlap_count / len(main_uniprot_ids)) * 100\n",
    "    print(f\"Percentage of your mapped UniProt IDs found in the AFESM ESM-only set: {percentage:.2f}%\")\n",
    "else:\n",
    "    print(\"No UniProt IDs found in your main database to compare.\")\n",
    "\n",
    "# --- Display Results & Save ---\n",
    "# Filter the main dataframe to show the proteins that matched\n",
    "# Need to compare the uppercase version of the column to the overlapping set\n",
    "df_main_matched = df_main[df_main[uniprot_col].astype(str).str.upper().isin(overlapping_uniprot_ids)].copy()\n",
    "\n",
    "\n",
    "print(\"\\nExample ProteinIDs whose UniProtKB_AC matched the AFESM ESM-only set (first 10):\")\n",
    "print(df_main_matched[[protein_id_col, uniprot_col]].head(10).to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "# Save the list of your ProteinIDs that hit the ESM-only set\n",
    "if not df_main_matched.empty:\n",
    "    try:\n",
    "        df_main_matched[[protein_id_col, uniprot_col]].to_csv(output_match_path, index=False)\n",
    "        print(f\"\\nSaved list of matching ProteinIDs ({overlap_count} entries) to '{output_match_path}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving matching ProteinIDs: {e}\")\n",
    "else:\n",
    "    print(\"\\nNo matching proteins found to save.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a416945-e530-4a3d-bf05-e4b8cfc40528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Configuration ---\n",
    "# Input file path\n",
    "input_csv_path = 'proteome_database_v0.9.csv'\n",
    "# Output file path\n",
    "output_csv_path = 'proteome_database_v10.csv'\n",
    "# Columns to drop\n",
    "columns_to_drop = [\n",
    "    'AFDB_Status',\n",
    "    'SeqSearch_PDB_Hit',\n",
    "    'SeqSearch_AFDB_Hit',\n",
    "    'SeqSearch_MGnify_Hit'\n",
    "]\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    print(f\"Successfully loaded '{input_csv_path}'. Shape: {df.shape}\")\n",
    "    print(f\"Original columns ({len(df.columns)}): {df.columns.tolist()}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Input file not found at '{input_csv_path}'.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the CSV: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Drop Columns ---\n",
    "# Check which of the specified columns actually exist in the DataFrame\n",
    "existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "missing_columns = [col for col in columns_to_drop if col not in df.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    print(f\"\\nWarning: The following columns specified for dropping were not found: {missing_columns}\")\n",
    "\n",
    "if not existing_columns_to_drop:\n",
    "    print(\"\\nNo columns to drop were found in the DataFrame. Saving original data.\")\n",
    "    df_dropped = df.copy() # Create a copy to proceed with saving\n",
    "else:\n",
    "    print(f\"\\nDropping columns: {existing_columns_to_drop}\")\n",
    "    try:\n",
    "        df_dropped = df.drop(columns=existing_columns_to_drop)\n",
    "        print(f\"Columns dropped successfully.\")\n",
    "        print(f\"New columns ({len(df_dropped.columns)}): {df_dropped.columns.tolist()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while dropping columns: {e}\")\n",
    "        raise\n",
    "\n",
    "# --- Save Updated Data ---\n",
    "try:\n",
    "    df_dropped.to_csv(output_csv_path, index=False)\n",
    "    print(f\"\\nSuccessfully saved updated data to '{output_csv_path}'. Shape: {df_dropped.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving updated data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c53e10-6b03-4227-9664-b0cce71c6fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration ---\n",
    "# Input file path (output from the previous step)\n",
    "input_csv_path = 'proteome_database_v10.csv'\n",
    "# Output file path for the unique virus names\n",
    "output_txt_path = 'unique_virus_names.txt'\n",
    "# Column containing the virus names\n",
    "virus_name_col = 'Virus_Name'\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    print(f\"Successfully loaded '{input_csv_path}'. Shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Input file not found at '{input_csv_path}'. Make sure the previous step ran correctly.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the CSV: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Extract Unique Virus Names ---\n",
    "# Check if the column exists\n",
    "if virus_name_col not in df.columns:\n",
    "    print(f\"Error: Column '{virus_name_col}' not found in the DataFrame.\")\n",
    "    raise KeyError(f\"Column '{virus_name_col}' not found.\")\n",
    "\n",
    "# Select the column, drop missing values (NaN, None, etc.), and get unique values\n",
    "unique_names = df[virus_name_col].dropna().unique()\n",
    "\n",
    "# Convert to a list and sort alphabetically\n",
    "unique_names_list = sorted(list(unique_names))\n",
    "\n",
    "print(f\"\\nFound {len(unique_names_list)} unique virus names.\")\n",
    "\n",
    "# --- Save to Text File ---\n",
    "try:\n",
    "    print(f\"Saving unique virus names to '{output_txt_path}'...\")\n",
    "    with open(output_txt_path, 'w') as f_out:\n",
    "        for name in unique_names_list:\n",
    "            f_out.write(f\"{name}\\n\")\n",
    "    print(\"Save complete.\")\n",
    "    print(f\"\\nFirst 10 unique names saved:\")\n",
    "    print(unique_names_list[:10])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred while saving the text file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335ae3f7-a250-40eb-9afd-7af96d3e62a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# --- Configuration ---\n",
    "# Input file path (output from the previous step)\n",
    "input_csv_path = 'proteome_database_v10.csv'\n",
    "# Output file path\n",
    "output_csv_path = 'proteome_database_v11.csv'\n",
    "# Column containing the virus names\n",
    "virus_name_col = 'Virus_Name'\n",
    "# Column to create/update with the family assignment\n",
    "virus_family_col = 'Virus_Family'\n",
    "\n",
    "# --- Keyword to Family Mapping ---\n",
    "# Dictionary mapping lowercase keywords to family names.\n",
    "# Order matters: More specific keywords should come before less specific ones.\n",
    "# (e.g., 'moumouvirus' before 'virus')\n",
    "# Based on unique_virus_names.txt and known NCLDV families/groups.\n",
    "FAMILY_MAP = {\n",
    "    # Mimiviridae and related groups (Imitervirales)\n",
    "    'mimivirus': 'Mimiviridae',\n",
    "    'mamavirus': 'Mimiviridae',\n",
    "    'moumouvirus': 'Mimiviridae',\n",
    "    'megavirus': 'Mimiviridae', # Proposed Megaviridae, often grouped\n",
    "    'tupanvirus': 'Mimiviridae', # Proposed Kheliviricetes\n",
    "    'cedratvirus': 'Mimiviridae', # Proposed Pithoviricetes\n",
    "    'faustovirus': 'Mimiviridae', # Proposed Duplodnaviria\n",
    "    'pacmanvirus': 'Mimiviridae', # Proposed Duplodnaviria\n",
    "    'orpheovirus': 'Mimiviridae', # Proposed Pithoviricetes\n",
    "    'klosneuvirus': 'Mimiviridae', # Proposed Klosneuvirinae\n",
    "    'catovirus': 'Mimiviridae', # Proposed Klosneuvirinae\n",
    "    'hokovirus': 'Mimiviridae', # Proposed Klosneuvirinae\n",
    "    'indivirus': 'Mimiviridae', # Proposed Klosneuvirinae\n",
    "    'samba_virus': 'Mimiviridae',\n",
    "    'bandra_megavirus': 'Mimiviridae', # Likely related\n",
    "    'niemeyer_virus': 'Mimiviridae',\n",
    "    'terrestrivirus': 'Mimiviridae',\n",
    "    'harvfovirus': 'Mimiviridae',\n",
    "    'barrevirus': 'Mimiviridae',\n",
    "    'dasosvirus': 'Mimiviridae',\n",
    "    'gaeavirus': 'Mimiviridae',\n",
    "    'satyrvirus': 'Mimiviridae',\n",
    "    'hirudovirus': 'Mimiviridae',\n",
    "    'edafosvirus': 'Mimiviridae',\n",
    "    'homavirus': 'Mimiviridae',\n",
    "    'acanthamoeba_polyphaga_lentillevirus': 'Mimiviridae', # Added back\n",
    "    'cotonvirus': 'Mimiviridae',\n",
    "    'hyperionvirus': 'Mimiviridae',\n",
    "    'powai_lake_megavirus': 'Mimiviridae', # Likely related\n",
    "    # Marseilleviridae\n",
    "    'marseillevirus': 'Marseilleviridae',\n",
    "    'lausannevirus': 'Marseilleviridae',\n",
    "    'tokyovirus': 'Marseilleviridae',\n",
    "    'noumeavirus': 'Marseilleviridae',\n",
    "    'kurlavirus': 'Marseilleviridae',\n",
    "    'port-miou_virus': 'Marseilleviridae',\n",
    "    'golden marseillevirus': 'Marseilleviridae', # Handle space\n",
    "    # Phycodnaviridae\n",
    "    'phycodnavirus': 'Phycodnaviridae',\n",
    "    'chlorella_virus': 'Phycodnaviridae',\n",
    "    'ostreococcus_virus': 'Phycodnaviridae',\n",
    "    'micromonas_pusilla_virus': 'Phycodnaviridae',\n",
    "    'bathycoccus_virus': 'Phycodnaviridae',\n",
    "    'phaeocystis_globosa_virus': 'Phycodnaviridae',\n",
    "    'emiliania_huxleyi_virus': 'Phycodnaviridae',\n",
    "    'chrysochromulina_virus': 'Phycodnaviridae',\n",
    "    'feldmannia_virus': 'Phycodnaviridae',\n",
    "    'ectocarpus_siliculosus_virus': 'Phycodnaviridae',\n",
    "    'prasinovirus': 'Phycodnaviridae', # Often grouped here\n",
    "    # Iridoviridae\n",
    "    'ranavirus': 'Iridoviridae', # Specific genus first\n",
    "    'lymphocystis_disease_virus': 'Iridoviridae', # Specific genus first\n",
    "    'chloriridovirus': 'Iridoviridae', # Specific genus first\n",
    "    'iridovirus': 'Iridoviridae', # General term last for this family\n",
    "    # Ascoviridae\n",
    "    'ascovirus': 'Ascoviridae',\n",
    "    # Pithoviridae\n",
    "    'pithovirus': 'Pithoviridae',\n",
    "    # Pandoraviridae\n",
    "    'pandoravirus': 'Pandoraviridae',\n",
    "    # Yaraviridae\n",
    "    'yaravirus': 'Yaraviridae',\n",
    "    # Specific Archaeal Virus Families\n",
    "    'bicaudavirus': 'Bicaudaviridae',\n",
    "    'acidianus_two-tailed_virus': 'Bicaudaviridae',\n",
    "    'sulfolobus_virus_stsv': 'Rudiviridae',\n",
    "    'acidianus_tailed_spindle_virus': 'Fuselloviridae',\n",
    "    'sulfolobus_monocaudavirus': 'Fuselloviridae', # Or proposed Thaspiviridae\n",
    "    # Grouping for other proposed NCLDV families/genera\n",
    "    'faunusvirus': 'Chaseviridae',\n",
    "    'solivirus': 'Pithoviridae-like',\n",
    "    'solumvirus': 'Pithoviridae-like',\n",
    "    # Catch-all for 'virus' if no other keyword matched\n",
    "    'virus': 'Unclassified Virus' # General catch-all if name contains 'virus'\n",
    "}\n",
    "\n",
    "# --- Helper Function ---\n",
    "def assign_family(name):\n",
    "    \"\"\"Assigns a virus family based on keywords in the name.\"\"\"\n",
    "    if pd.isna(name):\n",
    "        return np.nan # Return NaN if input is NaN\n",
    "\n",
    "    name_lower = str(name).lower() # Convert to string and lower case\n",
    "\n",
    "    for keyword, family in FAMILY_MAP.items():\n",
    "        # Use regex to find keyword as a whole word or part of compound name\n",
    "        # \\b matches word boundaries, allowing matches like 'ranavirus' but not 'miranavirus'\n",
    "        # We also allow matches if the keyword is followed by '_' or '-' or ends the string\n",
    "        # Or if it starts the string and is followed by '_' or '-'\n",
    "        # This handles cases like 'Megavirus_lba' or 'Frog_virus_3'\n",
    "        pattern = r'(?:^|\\b|_|-)' + re.escape(keyword) + r'(?:$|\\b|_|-)'\n",
    "        if re.search(pattern, name_lower):\n",
    "            return family\n",
    "\n",
    "    # If no keyword matches, return Unknown\n",
    "    return 'Unknown'\n",
    "\n",
    "# --- Load Data ---\n",
    "try:\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "    print(f\"Successfully loaded '{input_csv_path}'. Shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Input file not found at '{input_csv_path}'. Make sure the previous step ran correctly.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the CSV: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Apply Mapping ---\n",
    "print(f\"\\nAssigning families based on '{virus_name_col}'...\")\n",
    "# Apply the function to the Virus_Name column\n",
    "# Ensure the target column exists, create if not\n",
    "if virus_family_col not in df.columns:\n",
    "    df[virus_family_col] = np.nan\n",
    "\n",
    "# Apply the function only where Virus_Name is not null\n",
    "df[virus_family_col] = df[virus_name_col].apply(assign_family)\n",
    "print(\"Family assignment complete.\")\n",
    "\n",
    "# --- Display Results ---\n",
    "print(\"\\nValue counts for assigned Virus Families:\")\n",
    "print(df[virus_family_col].value_counts(dropna=False)) # Include NaNs in count\n",
    "\n",
    "print(f\"\\nExample rows with assigned '{virus_family_col}' (showing first 15 where Virus_Name is not NaN):\")\n",
    "print(df[df[virus_name_col].notna()][[virus_name_col, virus_family_col]].head(15).to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "\n",
    "# --- Save Updated Data ---\n",
    "try:\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"\\nSuccessfully saved updated data with '{virus_family_col}' to '{output_csv_path}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving updated data: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153a318d-1042-4145-85f9-5b556a582817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# Input file path (the scrambled combined file)\n",
    "input_csv_path = 'proteome_database_combined_final_v1.3.csv'\n",
    "# Output file path (overwrite the input or save as new version)\n",
    "output_csv_path = 'proteome_database_combined_final_v1.3.csv' # Overwriting\n",
    "\n",
    "# Define the DESIRED column order\n",
    "# Based on the schema of proteome_database_v1.1.csv plus added columns\n",
    "desired_column_order = [\n",
    "    'ProteinID', 'Sequence', 'Length', 'Source_Dataset',\n",
    "    'Source_Genome_Assembly_Accession', 'Source_Protein_Annotation',\n",
    "    'NCBI_TaxID', 'Asgard_Phylum', 'Virus_Family', 'Virus_Name',\n",
    "    'Orthogroup', # Suffixed version\n",
    "    'IPR_Signatures', 'IPR_GO_Terms', 'UniProtKB_AC',\n",
    "    'Num_Domains', 'Domain_Architecture', 'Type', 'Is_Hypothetical',\n",
    "    'Has_Known_Structure', # Original flag\n",
    "    'Percent_Disorder',\n",
    "    'Specific_Functional_Category', 'Category_Trigger',\n",
    "    'Signal_Peptide_USPNet', 'SP_Cleavage_Site_USPNet',\n",
    "    'Predicted_Subcellular_Localization', 'Mature_Protein_Sequence',\n",
    "    'Original_Seq_Length', 'Mature_Seq_Length',\n",
    "    'Group', # Asgard/GV/Other\n",
    "    'SeqSearch_PDB_Hit', # Added back\n",
    "    'SeqSearch_AFDB_Hit', # Added back\n",
    "    'Has_Reference_Structure' # Derived flag\n",
    "    # Add any other columns that should be present\n",
    "]\n",
    "\n",
    "# --- Load Data ---\n",
    "print(f\"--- Loading Scrambled Database ---\")\n",
    "try:\n",
    "    df = pd.read_csv(input_csv_path, low_memory=False)\n",
    "    print(f\"Loaded '{input_csv_path}'. Shape: {df.shape}\")\n",
    "    print(f\"Original (scrambled) columns: {df.columns.tolist()}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Input file not found at '{input_csv_path}'.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the CSV: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Reorder Columns ---\n",
    "print(\"\\n--- Reordering Columns ---\")\n",
    "# Check if all desired columns are actually present in the loaded dataframe\n",
    "current_cols = df.columns.tolist()\n",
    "final_cols = []\n",
    "missing_cols = []\n",
    "extra_cols = [col for col in current_cols if col not in desired_column_order]\n",
    "\n",
    "for col in desired_column_order:\n",
    "    if col in current_cols:\n",
    "        final_cols.append(col)\n",
    "    else:\n",
    "        missing_cols.append(col)\n",
    "\n",
    "# Add any extra columns found in the df but not in the desired list to the end\n",
    "final_cols.extend(extra_cols)\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"Warning: The following columns defined in 'desired_column_order' were NOT found in the loaded CSV: {missing_cols}\")\n",
    "if extra_cols:\n",
    "     print(f\"Warning: The following columns were found in the CSV but not in 'desired_column_order' (will be appended to the end): {extra_cols}\")\n",
    "\n",
    "try:\n",
    "    df_reordered = df[final_cols]\n",
    "    print(\"Columns successfully reordered.\")\n",
    "    print(f\"New column order: {df_reordered.columns.tolist()}\")\n",
    "except KeyError as e:\n",
    "     print(f\"Error during reordering - a column in final_cols was unexpectedly missing: {e}\")\n",
    "     # Fallback: Don't reorder if something went wrong\n",
    "     df_reordered = df\n",
    "except Exception as e:\n",
    "     print(f\"An unexpected error occurred during reordering: {e}\")\n",
    "     df_reordered = df\n",
    "\n",
    "\n",
    "# --- Save Reordered Data ---\n",
    "try:\n",
    "    df_reordered.to_csv(output_csv_path, index=False)\n",
    "    print(f\"\\nSuccessfully saved reordered data back to '{output_csv_path}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving reordered data: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2758cd62-75ef-4af3-8c0f-74b141bf7862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# Input file path (the reordered combined file from Cell 7)\n",
    "input_csv_path = 'proteome_database_combined_final_v1.3.csv'\n",
    "# Input path for MAIN DB *after* USPNet integration (contains correct derived cols)\n",
    "# !! CHECK THIS FILENAME !! Example: v0.8 if USPNet was added to v0.7\n",
    "main_db_with_uspnet_path = 'proteome_database_v1.1.csv'\n",
    "# Input path for FILTERED-OUT DB *after* USPNet integration (contains correct derived cols)\n",
    "# !! CHECK THIS FILENAME !! Example: v0.7 if USPNet was added to v0.6\n",
    "filtered_db_with_uspnet_path = 'all_filtered_out_proteins_v0.8.csv'\n",
    "# Output file path\n",
    "output_csv_path = 'proteome_database_combined_final_v1.4_complete.csv' # Final analysis-ready version\n",
    "\n",
    "# Columns to add/populate\n",
    "protein_id_col = 'ProteinID'\n",
    "cols_to_add = [\n",
    "    'Predicted_Subcellular_Localization',\n",
    "    'Mature_Protein_Sequence',\n",
    "    'Mature_Seq_Length'\n",
    "]\n",
    "# Also include ProteinID for merging\n",
    "cols_to_extract = [protein_id_col] + cols_to_add\n",
    "\n",
    "# --- Load Target Combined Data ---\n",
    "print(f\"--- Loading Reordered Combined Database ---\")\n",
    "try:\n",
    "    df_combined = pd.read_csv(input_csv_path, low_memory=False)\n",
    "    print(f\"Loaded '{input_csv_path}'. Shape: {df_combined.shape}\")\n",
    "    # Drop existing (likely empty or incomplete) versions of the columns if they exist\n",
    "    existing_cols_to_drop = [col for col in cols_to_add if col in df_combined.columns]\n",
    "    if existing_cols_to_drop:\n",
    "        print(f\"Dropping existing columns before merge: {existing_cols_to_drop}\")\n",
    "        df_combined = df_combined.drop(columns=existing_cols_to_drop)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Input file not found at '{input_csv_path}'.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the combined CSV: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- Load Source Dataframes (with correct derived columns) ---\n",
    "print(\"\\n--- Loading Source Databases with USPNet Data ---\")\n",
    "df_source_data_list = []\n",
    "loaded_source = False\n",
    "\n",
    "# Load Main DB with USPNet\n",
    "try:\n",
    "    df_main_source = pd.read_csv(main_db_with_uspnet_path, usecols=cols_to_extract, low_memory=False)\n",
    "    print(f\"Loaded main source DB '{main_db_with_uspnet_path}'. Shape: {df_main_source.shape}\")\n",
    "    # Check if all needed columns were loaded\n",
    "    if all(col in df_main_source.columns for col in cols_to_extract):\n",
    "        df_source_data_list.append(df_main_source)\n",
    "        loaded_source = True\n",
    "    else:\n",
    "        print(f\"Warning: Main source DB missing one or more required columns: {cols_to_extract}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: Main source DB file not found at '{main_db_with_uspnet_path}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Error loading main source DB: {e}\")\n",
    "\n",
    "# Load Filtered-Out DB with USPNet\n",
    "try:\n",
    "    df_filtered_source = pd.read_csv(filtered_db_with_uspnet_path, usecols=cols_to_extract, low_memory=False)\n",
    "    print(f\"Loaded filtered-out source DB '{filtered_db_with_uspnet_path}'. Shape: {df_filtered_source.shape}\")\n",
    "     # Check if all needed columns were loaded\n",
    "    if all(col in df_filtered_source.columns for col in cols_to_extract):\n",
    "        df_source_data_list.append(df_filtered_source)\n",
    "        loaded_source = True\n",
    "    else:\n",
    "        print(f\"Warning: Filtered-out source DB missing one or more required columns: {cols_to_extract}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: Filtered-out source DB file not found at '{filtered_db_with_uspnet_path}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Error loading filtered-out source DB: {e}\")\n",
    "\n",
    "\n",
    "# --- Combine and Merge Source Data ---\n",
    "if not loaded_source:\n",
    "    print(\"\\nError: Could not load necessary source data with USPNet columns. Cannot proceed.\")\n",
    "    # Handle error - maybe raise exception\n",
    "    raise RuntimeError(\"Failed to load source data with USPNet columns.\")\n",
    "else:\n",
    "    print(\"\\nCombining source data for merge...\")\n",
    "    df_source_combined = pd.concat(df_source_data_list, ignore_index=True)\n",
    "    # Drop duplicates based on ProteinID, keeping the first instance found\n",
    "    df_source_combined = df_source_combined.drop_duplicates(subset=[protein_id_col], keep='first')\n",
    "    print(f\"Combined source data shape (unique ProteinIDs): {df_source_combined.shape}\")\n",
    "\n",
    "    print(\"\\nMerging missing columns into the main combined dataframe...\")\n",
    "    # Ensure ProteinID types match for merge\n",
    "    df_combined[protein_id_col] = df_combined[protein_id_col].astype(str)\n",
    "    df_source_combined[protein_id_col] = df_source_combined[protein_id_col].astype(str)\n",
    "\n",
    "    # Perform left merge\n",
    "    df_final = pd.merge(\n",
    "        df_combined,\n",
    "        df_source_combined,\n",
    "        on=protein_id_col,\n",
    "        how='left' # Keep all rows from df_combined\n",
    "    )\n",
    "    print(\"Merge complete.\")\n",
    "\n",
    "    # --- Final Checks and Type Conversions ---\n",
    "    print(\"\\nPerforming final checks and type conversions...\")\n",
    "    # Check if merge changed row count (shouldn't happen with left merge on unique IDs)\n",
    "    if len(df_final) != len(df_combined):\n",
    "        print(f\"Warning: Merge changed row count from {len(df_combined)} to {len(df_final)}!\")\n",
    "\n",
    "    # Fill NaNs and set types for the newly merged columns\n",
    "    df_final['Predicted_Subcellular_Localization'] = df_final['Predicted_Subcellular_Localization'].fillna('Unknown').astype(str)\n",
    "    df_final['Mature_Protein_Sequence'] = df_final['Mature_Protein_Sequence'].fillna('').astype(str) # Fill with empty string? Or keep NaN?\n",
    "    df_final['Mature_Seq_Length'] = pd.to_numeric(df_final['Mature_Seq_Length'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "    print(\"Final checks complete.\")\n",
    "    print(f\"Columns in final dataframe: {df_final.columns.tolist()}\")\n",
    "    print(f\"Non-null counts for added columns:\")\n",
    "    print(f\"  Predicted_Subcellular_Localization: {df_final['Predicted_Subcellular_Localization'].notna().sum()}\")\n",
    "    print(f\"  Mature_Protein_Sequence: {df_final['Mature_Protein_Sequence'].notna().sum()}\")\n",
    "    print(f\"  Mature_Seq_Length: {df_final['Mature_Seq_Length'].notna().sum()}\")\n",
    "\n",
    "\n",
    "    # --- Save Final Data ---\n",
    "    try:\n",
    "        # Optional: Reorder columns one last time if needed\n",
    "        # desired_final_order = [...]\n",
    "        # df_final = df_final[desired_final_order]\n",
    "        df_final.to_csv(output_csv_path, index=False)\n",
    "        print(f\"\\nSuccessfully saved final combined and corrected data to '{output_csv_path}'. Shape: {df_final.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError saving final data: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc945911-c082-4366-bb06-553debb9b040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "# --- Configuration ---\n",
    "# Input file paths (adjust if necessary)\n",
    "main_db_path = 'proteome_database_combined_v1.4.csv'\n",
    "mgnify_results_path = 'results_vs_mgnify.m8' # Your MMseqs2 output file\n",
    "\n",
    "# Output file path\n",
    "output_db_path = 'proteome_database_combined_v1.5_mgnify.csv'\n",
    "\n",
    "# Column names\n",
    "protein_id_col = 'ProteinID' # Column in your main database\n",
    "mgnify_query_col = 'query' # Column in the MMseqs2 .m8 file (first column)\n",
    "new_hit_col = 'SeqSearch_MGnify_Hit' # Name of the new column to add\n",
    "\n",
    "# --- Load Main Database ---\n",
    "print(f\"--- Loading Main Database ---\")\n",
    "start_time = time.time()\n",
    "try:\n",
    "    # Use low_memory=False for large files, consistent with your analysis notebook\n",
    "    df_main = pd.read_csv(main_db_path, low_memory=False)\n",
    "    print(f\"Loaded '{main_db_path}'. Shape: {df_main.shape}\")\n",
    "    # Ensure ProteinID column is string type for reliable matching\n",
    "    if protein_id_col in df_main.columns:\n",
    "        df_main[protein_id_col] = df_main[protein_id_col].astype(str)\n",
    "    else:\n",
    "        raise KeyError(f\"Required column '{protein_id_col}' not found in main database.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Main database file not found at '{main_db_path}'.\")\n",
    "    raise\n",
    "except KeyError as e:\n",
    "     print(f\"Error loading main database: {e}\")\n",
    "     raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the main database CSV: {e}\")\n",
    "    raise\n",
    "print(f\"Main database loaded in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "# --- Load MGnify MMseqs2 Results ---\n",
    "print(f\"\\n--- Loading MGnify Search Results ---\")\n",
    "start_time = time.time()\n",
    "try:\n",
    "    # Define column names for the .m8 file based on the format used in the search plan\n",
    "    # We only really need the 'query' column\n",
    "    m8_cols = ['query', 'target', 'pident', 'qcov', 'tcov', 'evalue']\n",
    "    # Load only the query column to save memory, use tab separator\n",
    "    df_mgnify_hits = pd.read_csv(\n",
    "        mgnify_results_path,\n",
    "        sep='\\t',\n",
    "        header=None, # .m8 files typically don't have headers\n",
    "        names=m8_cols,\n",
    "        usecols=[mgnify_query_col] # Only load the first column ('query')\n",
    "    )\n",
    "    print(f\"Loaded '{mgnify_results_path}'. Found {len(df_mgnify_hits)} total hits.\")\n",
    "    # Ensure query column is string type\n",
    "    df_mgnify_hits[mgnify_query_col] = df_mgnify_hits[mgnify_query_col].astype(str)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: MGnify results file not found at '{mgnify_results_path}'.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the MGnify results CSV: {e}\")\n",
    "    raise\n",
    "print(f\"MGnify results loaded in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "# --- Identify Proteins with Hits ---\n",
    "print(f\"\\n--- Identifying Proteins with MGnify Hits ---\")\n",
    "# Get the set of unique ProteinIDs that had at least one hit in the MGnify search\n",
    "proteins_with_mgnify_hits = set(df_mgnify_hits[mgnify_query_col].unique())\n",
    "print(f\"Found {len(proteins_with_mgnify_hits)} unique proteins with hits in MGnify.\")\n",
    "\n",
    "# --- Add New Column to Main Database ---\n",
    "print(f\"\\n--- Adding '{new_hit_col}' column ---\")\n",
    "# Initialize the new column to False\n",
    "df_main[new_hit_col] = False\n",
    "\n",
    "# Find the rows in the main dataframe where the ProteinID is in our set of hits\n",
    "hit_mask = df_main[protein_id_col].isin(proteins_with_mgnify_hits)\n",
    "\n",
    "# Set the new column to True for those rows\n",
    "df_main.loc[hit_mask, new_hit_col] = True\n",
    "\n",
    "# --- Verify ---\n",
    "print(f\"\\nValue counts for the new '{new_hit_col}' column:\")\n",
    "print(df_main[new_hit_col].value_counts())\n",
    "num_true = df_main[new_hit_col].sum()\n",
    "if num_true == len(proteins_with_mgnify_hits):\n",
    "    print(\"Verification successful: Number of True values matches number of unique hitting proteins.\")\n",
    "else:\n",
    "    print(f\"Warning: Mismatch detected! Number of True values ({num_true}) does not match unique hitting proteins ({len(proteins_with_mgnify_hits)}). Check ProteinID matching.\")\n",
    "\n",
    "# --- Save Updated Database ---\n",
    "print(f\"\\n--- Saving Updated Database ---\")\n",
    "start_time = time.time()\n",
    "try:\n",
    "    df_main.to_csv(output_db_path, index=False)\n",
    "    print(f\"Successfully saved updated database with MGnify hits to '{output_db_path}'.\")\n",
    "    print(f\"Final shape: {df_main.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving updated database: {e}\")\n",
    "    raise\n",
    "print(f\"Database saved in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "print(\"\\n--- Integration Complete ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c588d498-2680-4385-9dfe-f17722caaa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "# --- Configuration ---\n",
    "# Input file paths (adjust if necessary)\n",
    "# Use the output from the MGnify integration step or v1.4 if starting fresh\n",
    "main_db_path = 'proteome_database_combined_v1.5_mgnify.csv' # Or v1.4\n",
    "mgnify_results_path = 'results_vs_mgnify.m8' # Your MMseqs2 output file\n",
    "\n",
    "# *** USER ACTION NEEDED: Path to your downloaded AFESM metadata file ***\n",
    "afesm_metadata_path = '2-repID_isOnlyESM_nMem_nAllMem_repPlddt_avgPlddt_avgAllPlddt_repLen_avgLen_avgAllLen_LCAtaxID_nBiome_LCBID.tsv' # <-- UPDATE THIS PATH\n",
    "\n",
    "# Output file path\n",
    "output_db_path = 'proteome_database_combined_v1.6_esm_dark.csv'\n",
    "\n",
    "# Column names\n",
    "protein_id_col = 'ProteinID' # Column in your main database\n",
    "mgnify_query_col = 'query' # Column in the MMseqs2 .m8 file (first column)\n",
    "mgnify_target_col = 'target' # Column in the MMseqs2 .m8 file (second column)\n",
    "afesm_repid_col = 'repID' # Column name in the AFESM metadata file\n",
    "\n",
    "# Hit columns (ensure these exist in your input main_db_path)\n",
    "pdb_hit_col = 'SeqSearch_PDB_Hit'\n",
    "afdb_hit_col = 'SeqSearch_AFDB_Hit'\n",
    "mgnify_hit_col = 'SeqSearch_MGnify_Hit' # Added previously\n",
    "esma_hit_col = 'SeqSearch_ESMA_Hit' # New column for ESMA hits via MGnify/AFESM\n",
    "dark_col = 'Is_Structurally_Dark' # New triple-negative flag\n",
    "\n",
    "# --- Load Main Database ---\n",
    "print(f\"--- Loading Main Database ---\")\n",
    "start_time = time.time()\n",
    "try:\n",
    "    df_main = pd.read_csv(main_db_path, low_memory=False)\n",
    "    print(f\"Loaded '{main_db_path}'. Shape: {df_main.shape}\")\n",
    "    # Ensure ProteinID column is string type for reliable matching\n",
    "    if protein_id_col in df_main.columns:\n",
    "        df_main[protein_id_col] = df_main[protein_id_col].astype(str)\n",
    "    else:\n",
    "        raise KeyError(f\"Required column '{protein_id_col}' not found in main database.\")\n",
    "    # Ensure previous hit columns exist and are boolean\n",
    "    for col in [pdb_hit_col, afdb_hit_col, mgnify_hit_col]:\n",
    "        if col not in df_main.columns:\n",
    "             print(f\"Warning: Column '{col}' not found. Adding as False.\")\n",
    "             df_main[col] = False\n",
    "        else:\n",
    "             # Fill potential NAs from previous steps and ensure boolean\n",
    "             df_main[col] = df_main[col].fillna(False).astype(bool)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Main database file not found at '{main_db_path}'.\")\n",
    "    raise\n",
    "except KeyError as e:\n",
    "     print(f\"Error loading main database: {e}\")\n",
    "     raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the main database CSV: {e}\")\n",
    "    raise\n",
    "print(f\"Main database loaded in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "# --- Load AFESM Metadata (Representative IDs with Structures) ---\n",
    "print(f\"\\n--- Loading AFESM Metadata ---\")\n",
    "start_time = time.time()\n",
    "afesm_repids_with_structure = set()\n",
    "try:\n",
    "    # Read the gzipped TSV file, using only the first column ('repID')\n",
    "    # Assuming the first column is indeed 'repID' based on the file name and description\n",
    "    df_afesm_meta = pd.read_csv(\n",
    "        afesm_metadata_path,\n",
    "        sep='\\t',\n",
    "        usecols=[0], # Load only the first column\n",
    "        header=0, # Use the first row as header to get the column name\n",
    "    )\n",
    "    # Check the actual column name loaded (should be 'repID')\n",
    "    loaded_col_name = df_afesm_meta.columns[0]\n",
    "    if loaded_col_name != afesm_repid_col:\n",
    "        print(f\"Warning: Expected column name '{afesm_repid_col}' but found '{loaded_col_name}'. Using '{loaded_col_name}'.\")\n",
    "        afesm_repid_col = loaded_col_name # Use the actual loaded name\n",
    "\n",
    "    # Extract unique IDs into the set, ensuring they are strings\n",
    "    afesm_repids_with_structure = set(df_afesm_meta[afesm_repid_col].dropna().astype(str))\n",
    "\n",
    "    if not afesm_repids_with_structure:\n",
    "         print(f\"Warning: No IDs loaded from '{afesm_metadata_path}'. Check file content and path.\")\n",
    "    else:\n",
    "         print(f\"Loaded {len(afesm_repids_with_structure)} AFESM representative IDs with structures from '{afesm_metadata_path}'.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: AFESM metadata file not found at '{afesm_metadata_path}'. Cannot determine ESMA hits.\")\n",
    "    print(\"Proceeding without ESMA hit information. The 'SeqSearch_ESMA_Hit' and 'Is_Structurally_Dark' columns will reflect this.\")\n",
    "    afesm_repids_with_structure = set() # Ensure empty set\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the AFESM metadata: {e}\")\n",
    "    afesm_repids_with_structure = set() # Ensure empty set\n",
    "print(f\"AFESM metadata loaded in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "# --- Load MGnify MMseqs2 Results (Query and Target) ---\n",
    "# Only proceed if we have AFESM IDs to check against\n",
    "proteins_hitting_esma_target = set()\n",
    "if afesm_repids_with_structure:\n",
    "    print(f\"\\n--- Loading MGnify Search Results (Query & Target) ---\")\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        # Define column names for the .m8 file\n",
    "        m8_cols = ['query', 'target', 'pident', 'qcov', 'tcov', 'evalue']\n",
    "        # Load only the query and target columns\n",
    "        df_mgnify_hits = pd.read_csv(\n",
    "            mgnify_results_path,\n",
    "            sep='\\t',\n",
    "            header=None,\n",
    "            names=m8_cols,\n",
    "            usecols=[mgnify_query_col, mgnify_target_col] # Load query and target\n",
    "        )\n",
    "        print(f\"Loaded '{mgnify_results_path}'. Found {len(df_mgnify_hits)} total hits.\")\n",
    "        # Ensure columns are string type\n",
    "        df_mgnify_hits[mgnify_query_col] = df_mgnify_hits[mgnify_query_col].astype(str)\n",
    "        df_mgnify_hits[mgnify_target_col] = df_mgnify_hits[mgnify_target_col].astype(str)\n",
    "\n",
    "        # --- Identify Your Proteins Hitting MGnify Targets that ARE AFESM Representatives ---\n",
    "        print(f\"\\n--- Filtering MGnify hits for targets present in AFESM metadata ---\")\n",
    "        # Filter the hits dataframe where the target is in our set of AFESM repIDs\n",
    "        df_esma_relevant_hits = df_mgnify_hits[df_mgnify_hits[mgnify_target_col].isin(afesm_repids_with_structure)]\n",
    "        print(f\"Found {len(df_esma_relevant_hits)} hits where the MGnify target is an AFESM representative with a structure.\")\n",
    "\n",
    "        # Get the set of unique ProteinIDs (query) from these relevant hits\n",
    "        proteins_hitting_esma_target = set(df_esma_relevant_hits[mgnify_query_col].unique())\n",
    "        print(f\"Found {len(proteins_hitting_esma_target)} unique query proteins hitting an AFESM representative target.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: MGnify results file not found at '{mgnify_results_path}'. Cannot determine ESMA hits.\")\n",
    "        proteins_hitting_esma_target = set() # Ensure empty set\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading or processing the MGnify results CSV: {e}\")\n",
    "        proteins_hitting_esma_target = set() # Ensure empty set\n",
    "    print(f\"MGnify results loaded and processed in {time.time() - start_time:.2f} seconds.\")\n",
    "else:\n",
    "     print(\"\\nSkipping MGnify results loading/processing as no AFESM metadata was loaded.\")\n",
    "\n",
    "\n",
    "# --- Add New ESMA Hit Column to Main Database ---\n",
    "print(f\"\\n--- Adding '{esma_hit_col}' column ---\")\n",
    "# Initialize the new column to False\n",
    "df_main[esma_hit_col] = False\n",
    "# Find rows where the ProteinID is in our set of proteins hitting an AFESM target\n",
    "if proteins_hitting_esma_target: # Only update if we have hits\n",
    "    esma_hit_mask = df_main[protein_id_col].isin(proteins_hitting_esma_target)\n",
    "    # Set the new column to True for those rows\n",
    "    df_main.loc[esma_hit_mask, esma_hit_col] = True\n",
    "\n",
    "print(f\"\\nValue counts for the new '{esma_hit_col}' column:\")\n",
    "print(df_main[esma_hit_col].value_counts())\n",
    "num_esma_true = df_main[esma_hit_col].sum()\n",
    "if not afesm_repids_with_structure:\n",
    "     print(\"(Note: ESMA hits could not be determined due to missing AFESM metadata.)\")\n",
    "elif num_esma_true == len(proteins_hitting_esma_target):\n",
    "    print(\"Verification successful: Number of True values matches number of unique hitting proteins.\")\n",
    "else:\n",
    "    print(f\"Warning: Mismatch detected! Number of True values ({num_esma_true}) does not match unique hitting proteins ({len(proteins_hitting_esma_target)}). Check ProteinID matching.\")\n",
    "\n",
    "\n",
    "# --- Add 'Is_Structurally_Dark' Column ---\n",
    "print(f\"\\n--- Adding '{dark_col}' column ---\")\n",
    "# True if PDB hit is False AND AFDB hit is False AND ESMA hit is False\n",
    "df_main[dark_col] = (\n",
    "    (~df_main[pdb_hit_col]) &\n",
    "    (~df_main[afdb_hit_col]) &\n",
    "    (~df_main[esma_hit_col])\n",
    ")\n",
    "\n",
    "print(f\"\\nValue counts for the new '{dark_col}' column:\")\n",
    "print(df_main[dark_col].value_counts())\n",
    "print(f\"Identified {df_main[dark_col].sum()} 'structurally dark' proteins (Triple Negative).\")\n",
    "\n",
    "\n",
    "# --- Save Updated Database ---\n",
    "print(f\"\\n--- Saving Updated Database ---\")\n",
    "start_time = time.time()\n",
    "try:\n",
    "    # Reorder columns slightly? Optional. Put new columns near other hit columns.\n",
    "    cols = df_main.columns.tolist()\n",
    "    # Find insertion point (e.g., after mgnify_hit_col)\n",
    "    try:\n",
    "        insert_idx = cols.index(mgnify_hit_col) + 1\n",
    "        # Ensure new columns actually exist before trying to reorder\n",
    "        if esma_hit_col in cols:\n",
    "             cols.insert(insert_idx, cols.pop(cols.index(esma_hit_col)))\n",
    "             insert_idx += 1 # Increment index for the next column\n",
    "        if dark_col in cols:\n",
    "             cols.insert(insert_idx, cols.pop(cols.index(dark_col)))\n",
    "        df_main = df_main[cols]\n",
    "    except ValueError:\n",
    "         print(\"Could not reorder columns, saving with new columns at the end.\")\n",
    "\n",
    "    df_main.to_csv(output_db_path, index=False)\n",
    "    print(f\"Successfully saved updated database to '{output_db_path}'.\")\n",
    "    print(f\"Final shape: {df_main.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving updated database: {e}\")\n",
    "    raise\n",
    "print(f\"Database saved in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "print(\"\\n--- Integration Complete ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae13a83-01c6-4a12-81ac-db091b9e50d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard Library Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from collections import Counter\n",
    "import re # For keyword searching\n",
    "\n",
    "# --- Configuration ---\n",
    "# Assumes setup cell (jupyter_arcadia_analysis_setup) ran for plot settings\n",
    "# Required variables from setup: summary_data_dir\n",
    "\n",
    "# *** ADDED: Define the summary data directory if not defined earlier ***\n",
    "if 'summary_data_dir' not in locals():\n",
    "    summary_data_dir = 'output_summary_data'\n",
    "    print(f\"Defined summary_data_dir = '{summary_data_dir}'\")\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(summary_data_dir):\n",
    "        try:\n",
    "            os.makedirs(summary_data_dir)\n",
    "            print(f\"Created directory: {summary_data_dir}\")\n",
    "        except OSError as e:\n",
    "            print(f\"Warning: Could not create summary data directory '{summary_data_dir}'. Error: {e}\")\n",
    "            summary_data_dir = None # Disable saving if creation fails\n",
    "\n",
    "# Input file path for the LATEST combined database\n",
    "# This will be BOTH input and the base for the output\n",
    "combined_db_path = 'proteome_database_combined_v1.6_esm_dark.csv' # <-- UPDATE if needed\n",
    "# Path to InterPro entry list file (ensure this is correct)\n",
    "interpro_entry_path = 'interpro_entry.txt' # Adjust path if needed\n",
    "\n",
    "# *** MODIFIED: Output file path is now for the UPDATED main database ***\n",
    "if summary_data_dir:\n",
    "     # Save updated DB in the main directory, not summary_data_dir? Or keep consistent?\n",
    "     # Let's save it in the main directory for now.\n",
    "     output_db_with_cat_path = 'proteome_database_combined_v1.7_broad_cat.csv'\n",
    "else:\n",
    "    output_db_with_cat_path = 'proteome_database_combined_v1.7_broad_cat.csv' # Save locally if dir failed\n",
    "    print(f\"Warning: Saving updated database locally as '{output_db_with_cat_path}'\")\n",
    "\n",
    "\n",
    "# Columns from main db\n",
    "protein_id_col = 'ProteinID'\n",
    "og_col = 'Orthogroup'\n",
    "source_annot_col = 'Source_Protein_Annotation'\n",
    "ipr_col = 'IPR_Signatures'\n",
    "# Optional: Original specific category column for fallback\n",
    "func_cat_col = 'Specific_Functional_Category'\n",
    "# Name for the new column\n",
    "broad_cat_col = 'Broad_Functional_Category'\n",
    "\n",
    "# --- Define Broad Categories and Associated Keywords/Patterns ---\n",
    "# (Using the same definitions as v5)\n",
    "CATEGORY_DEFINITIONS = [\n",
    "    ('ESCRT/Endosomal Sorting', [\n",
    "        'escrt', 'vps\\d*', 'snf7', 'vps2[\\\\b_]', 'vps4[\\\\b_]', 'vps20', 'vps22', 'vps24', 'vps25', 'vps28', 'vps32', 'vps36', 'vps60',\n",
    "        'retromer', 'sortilin', 'vps5', 'vps10', 'vps17', 'vps26', 'vps29', 'vps35', # Retromer\n",
    "        'corvet', 'hops', 'vps11', 'vps16', 'vps18', 'vps33', 'vps39', 'vps41', # HOPS/CORVET\n",
    "        'garp', 'vps51', 'vps52', 'vps53', 'vps54', # GARP\n",
    "        'steadinessbox', 'glue domain', 'ist1', 'brot1', 'mvb12', # Other ESCRT/Sorting related\n",
    "    ]),\n",
    "    ('Cytoskeleton', [\n",
    "        'actin', 'arp2/3', 'profilin', 'cofilin', 'formin', 'gelsolin', # Actin\n",
    "        'tubulin', 'kinesin', 'dynein', # Tubulin\n",
    "        'septin', # Other cytoskeleton\n",
    "        'lokiactin', # Specific Asgard term\n",
    "    ]),\n",
    "     ('Membrane Trafficking/Vesicles', [\n",
    "        'dynamin', # Membrane remodeling\n",
    "        'rab', 'rho', 'rac', 'ran', 'cdc42', 'arl', 'arf', 'yipt', # Small GTPases\n",
    "        'sec\\d*', 'sec59', 'sec61', 'sec65', 'signal recognition particle', 'srp', # Secretion/Translocation (Sec)\n",
    "        'coatomer', 'copi', 'copii', 'clathrin', # Vesicular transport coats\n",
    "        'adaptin', 'ap complex', 'adaptor protein complex', # Adaptor complexes\n",
    "        'snare', # SNAREs\n",
    "        'longin', 'mon1', 'roadblock', 'lc7', 'hook domain', 'arftrapp', # Other trafficking\n",
    "        'tralphin', # Specific Asgard term? (Check spelling/context)\n",
    "    ]),\n",
    "    ('Ubiquitin System', [\n",
    "        'ubiquitin', 'sumo', 'nedd8', 'ubl', 'ubiquitin-like', # Ubiquitin and UBLs\n",
    "        'proteasome', 'proteasomal', # Proteasome\n",
    "        'ubiquitinating', 'ubiquitin conjugating', 'ubiquitin ligase', 'ubiquitin activating', # Enzymes E1/E2/E3\n",
    "        'ubiquitin domain', 'ubx domain', # Domains\n",
    "    ]),\n",
    "    ('N-glycosylation', [\n",
    "        'glycosylation', 'ost\\d*', 'stt3', 'wbp1', 'ddost', 'ribophorin', # OST complex\n",
    "        'trap alpha', 'trap beta', 'trap gamma', 'trap delta', # TRAP complex (often associated)\n",
    "    ]),\n",
    "    ('Nuclear Transport/Pore', [\n",
    "        'nuclear pore', 'nucleoporin', 'nup\\d*', 'importin', 'exportin', 'karyopherin', 'ranbp', # Nuclear transport\n",
    "    ]),\n",
    "    ('DNA Info Processing', [\n",
    "        'dna polymerase', 'dna replication', 'dna repair', 'recombination', # DNA processing\n",
    "        'topoisomerase', 'helicase', 'primase', 'ligase', # Core DNA enzymes\n",
    "        'histone', 'nucleosome', # Chromatin\n",
    "        'e2f', 'dp transcription factor', # Specific factors if known ESPs\n",
    "        'mcm\\d*', 'orc\\d*', 'cdc\\d*', # Cell cycle related DNA factors\n",
    "    ]),\n",
    "    ('RNA Info Processing', [\n",
    "        'rna polymerase', 'tbp', 'tata-binding', 'tfii[a-z]', 'transcription factor', # Transcription machinery\n",
    "        'spliceosome', 'snrna', 'snrnp', 'splicing factor', # Splicing\n",
    "        'rna processing', 'rna modification', 'rrp\\d*', 'exosome', # RNA processing/turnover\n",
    "    ]),\n",
    "    ('Translation', [\n",
    "        'ribosome', 'ribosomal protein', 'rrna', # Ribosome components\n",
    "        'eif[0-9]', 'eef[0-9]', 'translation initiation', 'translation elongation', 'translation termination', # Translation factors\n",
    "        'trna synthetase', 'aminoacyl-trna', # tRNA related\n",
    "        'pelota', # Translation related\n",
    "    ]),\n",
    "     ('Signal Transduction', [\n",
    "         'kinase', 'phosphatase', 'receptor', 'gtpase', 'signal transduction',\n",
    "     ]),\n",
    "     ('Metabolism', [\n",
    "         'metabolism', 'synthase', 'transferase', 'oxidoreductase', 'hydrolase',\n",
    "     ]),\n",
    "]\n",
    "\n",
    "# Compile regex patterns for faster matching\n",
    "CATEGORY_PATTERNS = []\n",
    "for category, keywords in CATEGORY_DEFINITIONS:\n",
    "    patterns = [re.compile(r'\\b' + kw + r'\\b', re.IGNORECASE) for kw in keywords]\n",
    "    CATEGORY_PATTERNS.append({'category': category, 'patterns': patterns})\n",
    "\n",
    "# Define generic patterns/terms to exclude from source/IPR names during fallback\n",
    "generic_source_patterns = [\n",
    "    re.compile(r'hypothetical', re.IGNORECASE), re.compile(r'uncharacterized', re.IGNORECASE),\n",
    "    re.compile(r'unknown function', re.IGNORECASE), re.compile(r'predicted protein', re.IGNORECASE),\n",
    "    re.compile(r'^na$', re.IGNORECASE), re.compile(r'^protein$', re.IGNORECASE),\n",
    "    re.compile(r'domain-containing protein', re.IGNORECASE), re.compile(r'conserved protein', re.IGNORECASE),\n",
    "]\n",
    "generic_ipr_types = {'Domain', 'Family', 'Repeat', 'Conserved_site', 'PTM'}\n",
    "generic_ipr_names = {'Domain of unknown function', 'Uncharacterised protein family'}\n",
    "\n",
    "\n",
    "# --- Load InterPro Entry Data ---\n",
    "print(f\"\\n--- Loading InterPro Entry Data ---\")\n",
    "ipr_lookup = {}\n",
    "start_time = time.time()\n",
    "try:\n",
    "    ipr_info = pd.read_csv(\n",
    "        interpro_entry_path, sep='\\t', usecols=[0, 1, 2],\n",
    "        names=['IPR_ID', 'Type', 'Name'], header=None, comment='#', on_bad_lines='warn'\n",
    "    )\n",
    "    ipr_info['IPR_ID'] = ipr_info['IPR_ID'].astype(str).str.strip()\n",
    "    ipr_lookup = ipr_info.set_index('IPR_ID')[['Type', 'Name']].to_dict('index')\n",
    "    print(f\"Loaded InterPro entry data for {len(ipr_lookup)} entries in {time.time() - start_time:.2f} seconds.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: InterPro entry file not found at '{interpro_entry_path}'. Cannot use IPR names for categorization.\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: An error occurred loading or processing '{interpro_entry_path}': {e}\")\n",
    "if 'ipr_lookup' not in globals(): ipr_lookup = {}\n",
    "\n",
    "\n",
    "# --- Load Main Database ---\n",
    "print(f\"\\n--- Loading Main Database ---\")\n",
    "start_time = time.time()\n",
    "try:\n",
    "    # Load ALL columns this time, as we will save the whole thing\n",
    "    df_main = pd.read_csv(combined_db_path, low_memory=False)\n",
    "    print(f\"Loaded '{combined_db_path}'. Shape: {df_main.shape}\")\n",
    "\n",
    "    # --- Data Cleaning and Type Checks ---\n",
    "    # Ensure OG column is string for matching\n",
    "    if og_col not in df_main.columns: raise KeyError(f\"'{og_col}' missing from main DB.\")\n",
    "    df_main[og_col] = df_main[og_col].astype(str)\n",
    "\n",
    "    # Ensure annotation columns exist and fill NaNs\n",
    "    if source_annot_col not in df_main.columns:\n",
    "        print(f\"Warning: '{source_annot_col}' not found, adding empty column.\")\n",
    "        df_main[source_annot_col] = ''\n",
    "    else:\n",
    "        df_main[source_annot_col] = df_main[source_annot_col].fillna('')\n",
    "\n",
    "    if ipr_col not in df_main.columns:\n",
    "        print(f\"Warning: '{ipr_col}' not found, adding empty column.\")\n",
    "        df_main[ipr_col] = ''\n",
    "    else:\n",
    "        df_main[ipr_col] = df_main[ipr_col].fillna('')\n",
    "\n",
    "    if func_cat_col not in df_main.columns:\n",
    "        print(f\"Warning: '{func_cat_col}' not found, adding 'Unknown/Unclassified'.\")\n",
    "        df_main[func_cat_col] = 'Unknown/Unclassified'\n",
    "    else:\n",
    "        df_main[func_cat_col] = df_main[func_cat_col].fillna('Unknown/Unclassified')\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Combined database file not found at '{combined_db_path}'. Cannot map functions.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the main database CSV: {e}\")\n",
    "    raise\n",
    "print(f\"Main DB loaded in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "# --- Function to Assign Broad Functional Category ---\n",
    "# (Using the same hierarchical function 'assign_broad_category' from v5)\n",
    "def assign_broad_category(series_group):\n",
    "    \"\"\"Assigns a broad functional category based on hierarchical rules.\"\"\"\n",
    "    # --- 1. Check Source Annotations against Keyword Patterns (Priority Order) ---\n",
    "    source_annots = series_group[source_annot_col].dropna().astype(str)\n",
    "    if not source_annots.empty:\n",
    "        for category_info in CATEGORY_PATTERNS:\n",
    "            category_name = category_info['category']\n",
    "            patterns = category_info['patterns']\n",
    "            for annot in source_annots:\n",
    "                for pattern in patterns:\n",
    "                    if pattern.search(annot):\n",
    "                        return category_name # Return first category match based on priority\n",
    "\n",
    "    # --- 2. Check Informative IPR Signatures ---\n",
    "    if ipr_col in series_group.columns and ipr_lookup:\n",
    "        ipr_lists = series_group.dropna(subset=[ipr_col])[ipr_col].astype(str).str.split('|')\n",
    "        all_ipr_ids = [ipr.strip() for sublist in ipr_lists for ipr in sublist if ipr.strip()]\n",
    "        if all_ipr_ids:\n",
    "            ipr_counts = Counter(all_ipr_ids)\n",
    "            # Check most common IPRs against category keywords/patterns\n",
    "            for ipr_id, count in ipr_counts.most_common():\n",
    "                ipr_info = ipr_lookup.get(ipr_id)\n",
    "                if ipr_info:\n",
    "                    ipr_name = ipr_info.get('Name', '')\n",
    "                    # Check IPR name against category patterns\n",
    "                    for category_info in CATEGORY_PATTERNS:\n",
    "                         category_name = category_info['category']\n",
    "                         patterns = category_info['patterns']\n",
    "                         for pattern in patterns:\n",
    "                              if pattern.search(ipr_name):\n",
    "                                   return category_name # Return category if IPR name matches\n",
    "                    # If no keyword match, check if IPR itself is informative (but not keyworded)\n",
    "                    ipr_type = ipr_info.get('Type', '')\n",
    "                    if ipr_name and ipr_type not in generic_ipr_types and ipr_name not in generic_ipr_names:\n",
    "                         pass # Fall through\n",
    "\n",
    "    # --- 3. Check Other Specific Source Annotations (Fallback) ---\n",
    "    if not source_annots.empty:\n",
    "        specific_annots = []\n",
    "        processed_annots = set()\n",
    "        for annot in source_annots:\n",
    "             annot_clean = annot.strip()\n",
    "             if annot_clean and annot_clean not in processed_annots and not any(p.search(annot_clean) for p in generic_source_patterns):\n",
    "                  specific_annots.append(annot_clean)\n",
    "                  processed_annots.add(annot_clean)\n",
    "        if specific_annots:\n",
    "             annot_counts = Counter(specific_annots)\n",
    "             most_common_annots = annot_counts.most_common()\n",
    "             return \"Other Specific Annotation\" # Placeholder category\n",
    "\n",
    "    # --- 4. Fallback Original Specific_Functional_Category ---\n",
    "    if func_cat_col in series_group.columns:\n",
    "        func_cats = series_group[func_cat_col].dropna()\n",
    "        filtered_cats = func_cats[~func_cats.isin(['Unknown/Unclassified', 'Domain', 'Family', 'Repeat', 'Conserved_site', 'PTM'])]\n",
    "        if not filtered_cats.empty:\n",
    "            mode_val = filtered_cats.mode()\n",
    "            if not mode_val.empty:\n",
    "                 return sorted(mode_val.tolist())[0]\n",
    "\n",
    "    # --- 5. Lowest Priority: Unknown ---\n",
    "    return 'Unknown/Unclassified'\n",
    "\n",
    "\n",
    "# --- Apply Categorization Function ---\n",
    "print(\"\\nApplying hierarchical categorization function to all OGs...\")\n",
    "start_time = time.time()\n",
    "# Create the mapping DataFrame (OG -> Broad_Functional_Category)\n",
    "og_broad_categories_map = df_main.groupby(og_col).apply(assign_broad_category).reset_index()\n",
    "og_broad_categories_map.columns = [og_col, broad_cat_col] # Rename columns\n",
    "print(f\"Categorization mapping complete in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "# --- Merge Category Mapping into Main DataFrame ---\n",
    "print(f\"\\nMerging '{broad_cat_col}' into the main database...\")\n",
    "start_merge_time = time.time()\n",
    "# Ensure OG columns have same type before merge\n",
    "df_main[og_col] = df_main[og_col].astype(str)\n",
    "og_broad_categories_map[og_col] = og_broad_categories_map[og_col].astype(str)\n",
    "\n",
    "# Remove the new column if it already exists from a previous run\n",
    "if broad_cat_col in df_main.columns:\n",
    "    print(f\"Column '{broad_cat_col}' already exists. Dropping before merge.\")\n",
    "    df_main = df_main.drop(columns=[broad_cat_col])\n",
    "\n",
    "df_main_updated = pd.merge(\n",
    "    df_main,\n",
    "    og_broad_categories_map,\n",
    "    on=og_col,\n",
    "    how='left' # Keep all proteins, even if OG somehow missing from map\n",
    ")\n",
    "# Fill any NaNs created by the merge (shouldn't happen if map is complete)\n",
    "df_main_updated[broad_cat_col] = df_main_updated[broad_cat_col].fillna('Unknown/Unclassified')\n",
    "print(f\"Merge complete in {time.time() - start_merge_time:.2f} seconds.\")\n",
    "print(f\"Shape after merge: {df_main_updated.shape}\")\n",
    "\n",
    "\n",
    "# --- Display Results ---\n",
    "print(f\"\\nValue Counts for new '{broad_cat_col}' column:\")\n",
    "print(df_main_updated[broad_cat_col].value_counts().to_markdown())\n",
    "\n",
    "# --- Save the Updated Main Database ---\n",
    "print(f\"\\n--- Saving Updated Main Database ---\")\n",
    "start_save_time = time.time()\n",
    "# Check if output path is defined (depends on summary_data_dir)\n",
    "if output_db_with_cat_path:\n",
    "    try:\n",
    "        # Optional: Reorder columns to place new category near other functional columns\n",
    "        cols = df_main_updated.columns.tolist()\n",
    "        try:\n",
    "            # Find insertion point (e.g., after Specific_Functional_Category)\n",
    "            insert_idx = cols.index(func_cat_col) + 1\n",
    "            cols.insert(insert_idx, cols.pop(cols.index(broad_cat_col)))\n",
    "            df_main_updated = df_main_updated[cols]\n",
    "        except ValueError:\n",
    "             print(\"Could not reorder columns, saving with new column at the end.\")\n",
    "\n",
    "        df_main_updated.to_csv(output_db_with_cat_path, index=False)\n",
    "        print(f\"\\nSuccessfully saved updated database with '{broad_cat_col}' to '{output_db_with_cat_path}'\")\n",
    "    except Exception as e: print(f\"Error saving updated database: {e}\")\n",
    "else:\n",
    "    print(\"\\nSkipping save of updated database (output path not defined).\")\n",
    "print(f\"Database saved in {time.time() - start_save_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Broad Functional Categorization and DB Update Complete (v5.1) ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbcbcfb-2226-4e4c-beee-021819939046",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob # For finding files in a folder\n",
    "import time\n",
    "import re # Added for parsing organism names\n",
    "\n",
    "# --- Configuration ---\n",
    "# Main database file\n",
    "db_path = 'proteome_database_combined_v1.7_broad_cat.csv'\n",
    "# Folder containing DIAMOND search results files (tab-separated)\n",
    "diamond_results_folder = 'euk_diamond_search_results' # User provided this folder name\n",
    "# ESP Orthogroup list file\n",
    "esp_og_list_path = 'output_summary_data/all_esp_orthogroup_list_v4.txt'\n",
    "\n",
    "# Output file for the database with DIAMOND hits and ESP column integrated\n",
    "output_db_updated_path = 'proteome_database_combined_v1.8_euk_hits_esps.csv' # Example new version\n",
    "\n",
    "# DIAMOND parsing parameters\n",
    "e_value_threshold = 1e-10 # Adjust as needed, 1e-5 is also common\n",
    "# DIAMOND output columns (standard 12 fields for `outfmt 6`):\n",
    "# qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore\n",
    "# User example also had qlen and slen at the end, so we'll use 14 columns\n",
    "diamond_col_names = [\n",
    "    'qseqid', 'sseqid_full', 'pident', 'length', 'mismatch', 'gapopen',\n",
    "    'qstart', 'qend', 'sstart', 'send', 'evalue', 'bitscore', 'qlen', 'slen'\n",
    "]\n",
    "\n",
    "# Columns to be added to the main DataFrame\n",
    "hit_flag_col = 'Has_Euk_DIAMOND_Hit'\n",
    "best_hit_sseqid_col = 'Euk_Hit_SSEQID'\n",
    "best_hit_organism_col = 'Euk_Hit_Organism'\n",
    "best_hit_pident_col = 'Euk_Hit_PIDENT'\n",
    "best_hit_evalue_col = 'Euk_Hit_EVALUE'\n",
    "esp_col = 'Is_ESP' # Name of the new ESP column we will create\n",
    "\n",
    "# Define relevant column names from your main database\n",
    "protein_id_col = 'ProteinID'\n",
    "group_col = 'Group' # 'Asgard' or 'Virus'\n",
    "orthogroup_col = 'Orthogroup' # Needed for ESP identification\n",
    "structurally_dark_col = 'Is_Structurally_Dark'\n",
    "num_domains_col = 'Num_Domains'\n",
    "\n",
    "\n",
    "print(\"--- Starting DIAMOND Hit Integration, ESP Definition, and Analysis ---\")\n",
    "\n",
    "# --- 1. Load Main Database ---\n",
    "print(f\"\\nLoading main database from: {db_path}\")\n",
    "try:\n",
    "    df_main = pd.read_csv(db_path, low_memory=False)\n",
    "    print(f\"Successfully loaded main database. Shape: {df_main.shape}\")\n",
    "    # Diagnostic: Print some ProteinIDs from Asgard group in df_main\n",
    "    if protein_id_col in df_main.columns and group_col in df_main.columns:\n",
    "        print(\"  Example Asgard ProteinIDs from main DB (first 5):\")\n",
    "        print(df_main[df_main[group_col] == 'Asgard'][protein_id_col].head().tolist())\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Main database file not found at '{db_path}'. Please check the path.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the main database: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- 2. Define ESPs based on Orthogroup List ---\n",
    "print(f\"\\nLoading ESP Orthogroup list from: {esp_og_list_path}\")\n",
    "esp_og_set = set()\n",
    "try:\n",
    "    esp_og_df = pd.read_csv(esp_og_list_path, header=None, names=[orthogroup_col])\n",
    "    esp_og_set = set(esp_og_df[orthogroup_col])\n",
    "    print(f\"Successfully loaded {len(esp_og_set)} ESP Orthogroups.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: ESP Orthogroup list file not found at '{esp_og_list_path}'. '{esp_col}' will be False for all.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the ESP Orthogroup list: {e}. '{esp_col}' will be False for all.\")\n",
    "\n",
    "# Add 'Is_ESP' column to the main DataFrame\n",
    "if orthogroup_col in df_main.columns and group_col in df_main.columns and esp_og_set:\n",
    "    df_main[esp_col] = df_main.apply(\n",
    "        lambda row: row[orthogroup_col] in esp_og_set if row[group_col] == 'Asgard' else False,\n",
    "        axis=1\n",
    "    )\n",
    "    print(f\"Added '{esp_col}' column. Found {df_main[esp_col].sum()} ESPs in Asgard.\")\n",
    "else:\n",
    "    df_main[esp_col] = False # Default to False if OGs not loaded or columns missing\n",
    "    if not esp_og_set:\n",
    "        print(f\"ESP Orthogroup list was not loaded or is empty. '{esp_col}' column set to False for all proteins.\")\n",
    "    else:\n",
    "        print(f\"Could not define ESPs due to missing '{orthogroup_col}' or '{group_col}' columns. '{esp_col}' column set to False.\")\n",
    "\n",
    "\n",
    "# --- 3. Parse DIAMOND Results from Folder ---\n",
    "print(f\"\\nParsing DIAMOND results from folder: {diamond_results_folder}\")\n",
    "best_hits_data = {} # To store best hit info for each query protein\n",
    "raw_diamond_files = glob.glob(os.path.join(diamond_results_folder, '*_diamond_hits.tsv'))\n",
    "\n",
    "if not raw_diamond_files:\n",
    "    print(f\"WARNING: No '*_diamond_hits.tsv' files found in '{diamond_results_folder}'.\")\n",
    "    df_diamond_raw_list = []\n",
    "else:\n",
    "    print(f\"Found {len(raw_diamond_files)} DIAMOND result files to process.\")\n",
    "    df_diamond_raw_list = []\n",
    "    for f_path in raw_diamond_files:\n",
    "        try:\n",
    "            df_temp = pd.read_csv(f_path, sep='\\t', header=None, names=diamond_col_names)\n",
    "            # CRITICAL CHANGE: Do NOT add .ASG suffix here.\n",
    "            # The qseqid from DIAMOND output should match ProteinID in df_main directly.\n",
    "            # df_temp['qseqid'] = df_temp['qseqid'].astype(str) + \".ASG\" # REMOVED THIS LINE\n",
    "            df_temp['qseqid'] = df_temp['qseqid'].astype(str) # Ensure it's string\n",
    "            df_diamond_raw_list.append(df_temp)\n",
    "        except Exception as e:\n",
    "            print(f\"  Error reading or processing file {f_path}: {e}\")\n",
    "            continue # Skip problematic files\n",
    "\n",
    "if df_diamond_raw_list:\n",
    "    df_diamond_raw = pd.concat(df_diamond_raw_list, ignore_index=True)\n",
    "    print(f\"  Read {len(df_diamond_raw)} total DIAMOND alignments from all files.\")\n",
    "\n",
    "    # Filter by e-value\n",
    "    df_diamond_filtered = df_diamond_raw[df_diamond_raw['evalue'] <= e_value_threshold].copy()\n",
    "    print(f\"  Found {len(df_diamond_filtered)} alignments passing e-value threshold <= {e_value_threshold}.\")\n",
    "\n",
    "    if not df_diamond_filtered.empty:\n",
    "        # Sort by qseqid and then by e-value (ascending), then bitscore (descending) to easily get the best hit\n",
    "        df_diamond_filtered.sort_values(by=['qseqid', 'evalue', 'bitscore'], ascending=[True, True, False], inplace=True)\n",
    "        \n",
    "        # Keep only the best hit per qseqid\n",
    "        df_best_hits = df_diamond_filtered.drop_duplicates(subset=['qseqid'], keep='first').copy()\n",
    "        print(f\"  Identified {len(df_best_hits)} unique query proteins with significant eukaryotic hits.\")\n",
    "        \n",
    "        # Diagnostic: Print some qseqids from DIAMOND best hits\n",
    "        print(\"  Example qseqids from DIAMOND best hits (first 5):\")\n",
    "        print(df_best_hits['qseqid'].head().tolist())\n",
    "\n",
    "\n",
    "        # Prepare data for merging\n",
    "        for _, row in df_best_hits.iterrows():\n",
    "            query_id_for_db = row['qseqid'] # This is now the original ID from DIAMOND file\n",
    "            subject_full = str(row['sseqid_full'])\n",
    "            sseqid = subject_full\n",
    "            organism = \"Unknown\" # Default\n",
    "            \n",
    "            if '|' in subject_full: # Standard parsing for NCBI-like headers\n",
    "                parts = subject_full.split('|', 1)\n",
    "                sseqid = parts[0]\n",
    "                if len(parts) > 1 and parts[1]: # Check if there's anything after the first pipe\n",
    "                    desc_text = parts[1]\n",
    "                    # Try to extract organism from [Organism Name]\n",
    "                    match = re.search(r\"\\[(.*?)\\]\", desc_text)\n",
    "                    if match:\n",
    "                        organism = match.group(1)\n",
    "                    else:\n",
    "                        # If no brackets, try to get it from common patterns like XP_id|Genus species\n",
    "                        # This part might need refinement based on the exact format of sseqid_full\n",
    "                        sub_parts = desc_text.split(' ')\n",
    "                        if len(sub_parts) > 0:\n",
    "                             # Check if the first part after pipe is just an accession (e.g. XP_12345)\n",
    "                            if not (sub_parts[0].startswith(\"XP_\") or sub_parts[0].startswith(\"NP_\") or sub_parts[0].startswith(\"WP_\")):\n",
    "                                organism = sub_parts[0] # Often the first word is genus\n",
    "                                if len(sub_parts) > 1 and not (sub_parts[1].startswith(\"XP_\") or sub_parts[1].startswith(\"NP_\") or sub_parts[1].startswith(\"WP_\")):\n",
    "                                     organism = f\"{sub_parts[0]} {sub_parts[1]}\" # Genus species\n",
    "                            elif len(sub_parts) > 1: # If first part is an ID, try the next\n",
    "                                organism = sub_parts[1]\n",
    "\n",
    "\n",
    "            best_hits_data[query_id_for_db] = {\n",
    "                hit_flag_col: True,\n",
    "                best_hit_sseqid_col: sseqid,\n",
    "                best_hit_organism_col: organism,\n",
    "                best_hit_pident_col: row['pident'],\n",
    "                best_hit_evalue_col: row['evalue']\n",
    "            }\n",
    "    else:\n",
    "        print(\"  No DIAMOND hits passed the e-value threshold after processing all files.\")\n",
    "else:\n",
    "    print(\"  No valid DIAMOND alignments loaded. Skipping hit processing.\")\n",
    "\n",
    "\n",
    "# --- 4. Merge DIAMOND Hit Information with Main Database ---\n",
    "print(f\"\\nMerging DIAMOND hit information into the main database...\")\n",
    "df_main_updated = df_main.copy() # Start with the df_main that now includes Is_ESP\n",
    "\n",
    "if best_hits_data:\n",
    "    df_hits_to_merge = pd.DataFrame.from_dict(best_hits_data, orient='index')\n",
    "    df_hits_to_merge.index.name = protein_id_col \n",
    "    \n",
    "    # Diagnostic: Check dtypes before merge\n",
    "    print(f\"  dtype of df_main_updated['{protein_id_col}']: {df_main_updated[protein_id_col].dtype}\")\n",
    "    print(f\"  dtype of df_hits_to_merge.index: {df_hits_to_merge.index.dtype}\")\n",
    "    \n",
    "    # Ensure ProteinID is string in both for robust merging\n",
    "    df_main_updated[protein_id_col] = df_main_updated[protein_id_col].astype(str)\n",
    "    df_hits_to_merge.index = df_hits_to_merge.index.astype(str)\n",
    "\n",
    "    df_main_updated = df_main_updated.merge(df_hits_to_merge, on=protein_id_col, how='left')\n",
    "    \n",
    "    df_main_updated[hit_flag_col] = df_main_updated[hit_flag_col].fillna(False)\n",
    "else:\n",
    "    print(\"  No best hits data to merge. Adding empty hit flag column.\")\n",
    "    df_main_updated[hit_flag_col] = False\n",
    "    for col in [best_hit_sseqid_col, best_hit_organism_col, best_hit_pident_col, best_hit_evalue_col]:\n",
    "        if col not in df_main_updated.columns: \n",
    "            df_main_updated[col] = np.nan\n",
    "\n",
    "\n",
    "print(f\"  Merge complete. Updated database shape: {df_main_updated.shape}\")\n",
    "print(f\"  Total proteins flagged with eukaryotic hits: {df_main_updated[hit_flag_col].sum()}\")\n",
    "\n",
    "# --- 5. Save Updated Database ---\n",
    "print(f\"\\nSaving updated database (with ESPs and DIAMOND hits) to: {output_db_updated_path}\")\n",
    "try:\n",
    "    df_main_updated.to_csv(output_db_updated_path, index=False)\n",
    "    print(f\"Successfully saved updated database.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not save the updated database. Error: {e}\")\n",
    "\n",
    "# --- 6. Initial Summary Analyses ---\n",
    "print(\"\\n\\n--- Initial Summary of Eukaryotic DIAMOND Hits ---\")\n",
    "\n",
    "if group_col not in df_main_updated.columns:\n",
    "    print(f\"ERROR: Group column '{group_col}' not found. Cannot perform group-specific summary.\")\n",
    "else:\n",
    "    # Overall counts\n",
    "    total_proteins = len(df_main_updated)\n",
    "    total_hits = df_main_updated[hit_flag_col].sum()\n",
    "    print(f\"\\nOverall:\")\n",
    "    print(f\"  Total proteins in database: {total_proteins}\")\n",
    "    print(f\"  Total proteins with eukaryotic DIAMOND hits: {total_hits} ({(total_hits/total_proteins*100 if total_proteins > 0 else 0.0):.1f}%)\")\n",
    "\n",
    "    # Asgard proteins\n",
    "    df_asgard = df_main_updated[df_main_updated[group_col] == 'Asgard']\n",
    "    if not df_asgard.empty:\n",
    "        total_asgard = len(df_asgard)\n",
    "        asgard_hits = df_asgard[hit_flag_col].sum()\n",
    "        print(f\"\\nAsgard Archaea:\")\n",
    "        print(f\"  Total Asgard proteins: {total_asgard}\")\n",
    "        print(f\"  Asgard proteins with eukaryotic hits: {asgard_hits} ({(asgard_hits/total_asgard*100 if total_asgard > 0 else 0.0):.1f}%)\")\n",
    "\n",
    "        # Asgard ESPs\n",
    "        if esp_col in df_asgard.columns: \n",
    "            df_asgard_esps = df_asgard[df_asgard[esp_col] == True]\n",
    "            if not df_asgard_esps.empty:\n",
    "                total_asgard_esps = len(df_asgard_esps)\n",
    "                asgard_esps_hits = df_asgard_esps[hit_flag_col].sum()\n",
    "                print(f\"  Asgard ESPs with eukaryotic hits: {asgard_esps_hits} ({(asgard_esps_hits/total_asgard_esps*100 if total_asgard_esps > 0 else 0.0):.1f}%)\")\n",
    "            else:\n",
    "                print(f\"  No Asgard ESPs identified in the Asgard subset (column '{esp_col}' has no True values).\")\n",
    "        else:\n",
    "             print(f\"  ESP column ('{esp_col}') was not successfully added. Skipping ESP hit analysis.\")\n",
    "\n",
    "\n",
    "        # Structurally Dark Asgard proteins\n",
    "        if structurally_dark_col in df_asgard.columns:\n",
    "            df_asgard_dark = df_asgard[df_asgard[structurally_dark_col] == True]\n",
    "            if not df_asgard_dark.empty:\n",
    "                total_asgard_dark = len(df_asgard_dark)\n",
    "                asgard_dark_hits = df_asgard_dark[hit_flag_col].sum()\n",
    "                print(f\"  Structurally Dark Asgard proteins with eukaryotic hits: {asgard_dark_hits} ({(asgard_dark_hits/total_asgard_dark*100 if total_asgard_dark > 0 else 0.0):.1f}%)\")\n",
    "            else:\n",
    "                print(f\"  No Structurally Dark Asgard proteins identified.\")\n",
    "        else:\n",
    "            print(f\"  Structurally Dark column ('{structurally_dark_col}') not found. Skipping dark protein hit analysis.\")\n",
    "\n",
    "        # Domain-less Asgard proteins\n",
    "        if num_domains_col in df_asgard.columns:\n",
    "            df_asgard_domainless = df_asgard[df_asgard[num_domains_col].isna()]\n",
    "            if not df_asgard_domainless.empty:\n",
    "                total_asgard_domainless = len(df_asgard_domainless)\n",
    "                asgard_domainless_hits = df_asgard_domainless[hit_flag_col].sum()\n",
    "                print(f\"  Domain-less Asgard proteins with eukaryotic hits: {asgard_domainless_hits} ({(asgard_domainless_hits/total_asgard_domainless*100 if total_asgard_domainless > 0 else 0.0):.1f}%)\")\n",
    "            else:\n",
    "                print(f\"  No Domain-less Asgard proteins identified.\")\n",
    "        else:\n",
    "            print(f\"  Num_Domains column ('{num_domains_col}') not found. Skipping domain-less protein hit analysis.\")\n",
    "    else:\n",
    "        print(\"\\nNo Asgard proteins found in the database.\")\n",
    "\n",
    "    # Giant Virus (GV) proteins\n",
    "    df_gv_subset = df_main_updated[df_main_updated[group_col] == 'Virus'] \n",
    "    if not df_gv_subset.empty:\n",
    "        total_gv = len(df_gv_subset)\n",
    "        gv_hits = df_gv_subset[hit_flag_col].sum()\n",
    "        print(f\"\\nGiant Viruses (GV):\")\n",
    "        print(f\"  Total GV proteins: {total_gv}\")\n",
    "        print(f\"  GV proteins with eukaryotic hits: {gv_hits} ({(gv_hits/total_gv*100 if total_gv > 0 else 0.0):.1f}%)\")\n",
    "    else:\n",
    "        print(\"\\nNo Giant Virus proteins found in the database.\")\n",
    "\n",
    "print(\"\\n\\n--- DIAMOND Hit Integration, ESP Definition, and Initial Analysis Complete ---\")\n",
    "print(f\"Next steps could include:\")\n",
    "print(f\"  - Detailed analysis of the '{best_hit_organism_col}' distribution for hits.\")\n",
    "print(f\"  - Comparing domain architectures between Asgard ESPs and their eukaryotic hits (as per plans).\")\n",
    "print(f\"  - Focusing on hits for specific proteins of interest (e.g., Ig-like domain proteins).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9f7615-7fb7-4949-91dd-3155bb16ca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "\"\"\"\n",
    "Updates the Euk_Hit_Organism column and adds Euk_Hit_Protein_Name information \n",
    "to the main protein database by parsing headers from a eukaryotic proteome FASTA file.\n",
    "Version 1.2: Removed per-line debug logging to prevent performance issues.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "import sys\n",
    "import logging\n",
    "import re # Added for stripping version in fallback lookup\n",
    "# Ensure Biopython is installed: pip install biopython\n",
    "try:\n",
    "    from Bio import SeqIO \n",
    "except ImportError:\n",
    "    print(\"ERROR: Biopython is required for this script. Please install it: pip install biopython\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# --- Setup Logging ---\n",
    "log_formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "logger = logging.getLogger()\n",
    "# Prevent adding handlers multiple times if script is re-run in interactive session\n",
    "if not logger.hasHandlers():\n",
    "    # Set level back to INFO to avoid excessive output\n",
    "    logger.setLevel(logging.INFO) \n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setFormatter(log_formatter)\n",
    "    logger.addHandler(console_handler)\n",
    "else:\n",
    "    logger.setLevel(logging.INFO) # Ensure level is INFO\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "# INPUT: Main database CSV (should contain Euk_Hit_SSEQID and Euk_Hit_Organism columns)\n",
    "db_with_hits_path = Path('proteome_database_combined_v1.8_euk_hits_esps.csv')\n",
    "\n",
    "# INPUT: Eukaryotic proteome FASTA file with informative headers\n",
    "euk_fasta_path = Path('euk63_proteomes_final.fasta') \n",
    "\n",
    "# OUTPUT: Updated database CSV file\n",
    "output_db_updated_path = Path('proteome_database_combined_v1.9_euk_fasta_annots_updated_org.csv')\n",
    "\n",
    "# --- Define Key Column Names ---\n",
    "protein_id_col = 'ProteinID'\n",
    "group_col = 'Group'\n",
    "hit_flag_col = 'Has_Euk_DIAMOND_Hit'\n",
    "euk_hit_sseqid_col = 'Euk_Hit_SSEQID' # The NCBI ID (e.g., XP_...) from DIAMOND\n",
    "euk_hit_organism_col = 'Euk_Hit_Organism' # Existing column to UPDATE\n",
    "\n",
    "# New column to add\n",
    "euk_hit_protein_name_col = 'Euk_Hit_Protein_Name'\n",
    "\n",
    "# --- Helper Function to Parse FASTA Headers (Improved ID Extraction) ---\n",
    "def parse_euk_fasta_headers(fasta_file: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Parses a FASTA file to extract information from headers.\n",
    "    Extracts ID before the first pipe '|' or space ' '.\n",
    "    Assumes header format like: >ID|Genus species|Protein Name\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary mapping the extracted sequence ID (e.g., 'XP_020145405.1') to \n",
    "        {'Genus_Species': 'Genus species', 'Protein_Name': 'Protein Name'}\n",
    "    \"\"\"\n",
    "    logging.info(f\"Parsing FASTA headers from: {fasta_file}...\")\n",
    "    header_info = {}\n",
    "    count = 0\n",
    "    skipped_count = 0\n",
    "    \n",
    "    if not fasta_file.is_file():\n",
    "        logging.error(f\"FASTA file not found: {fasta_file}\")\n",
    "        return header_info\n",
    "\n",
    "    try:\n",
    "        for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "            count += 1\n",
    "            # Use record.description (full header line after '>') for parsing\n",
    "            description = record.description \n",
    "            \n",
    "            # --- Improved ID Extraction ---\n",
    "            # Find the first pipe or space\n",
    "            first_pipe = description.find('|')\n",
    "            first_space = description.find(' ')\n",
    "            \n",
    "            end_of_id = -1\n",
    "            if first_pipe != -1 and first_space != -1:\n",
    "                end_of_id = min(first_pipe, first_space)\n",
    "            elif first_pipe != -1:\n",
    "                end_of_id = first_pipe\n",
    "            elif first_space != -1:\n",
    "                end_of_id = first_space\n",
    "                \n",
    "            if end_of_id != -1:\n",
    "                fasta_id_key = description[:end_of_id].strip()\n",
    "            else:\n",
    "                # If no pipe or space, the whole description might be the ID (unlikely for NCBI)\n",
    "                fasta_id_key = description.strip() \n",
    "            # --- End Improved ID Extraction ---\n",
    "\n",
    "            if not fasta_id_key:\n",
    "                 skipped_count += 1\n",
    "                 logging.warning(f\"Could not extract valid ID Key from header: {description[:100]}...\")\n",
    "                 continue\n",
    "\n",
    "            # --- Removed per-line debug logging ---\n",
    "            # logging.debug(f\"Record {count}: Extracted FASTA Key = '{fasta_id_key}' from header = '{description[:60]}...'\")\n",
    "\n",
    "\n",
    "            # Default values\n",
    "            genus_species = \"Unknown Species\"\n",
    "            protein_name = \"Unknown Protein\"\n",
    "\n",
    "            # Attempt to parse based on pipe '|' delimiter using the full description\n",
    "            parts = description.split('|')\n",
    "            \n",
    "            # We already extracted the ID, now parse the rest based on pipe count\n",
    "            if len(parts) >= 3: \n",
    "                # Assumes ID|Genus species|Protein Name\n",
    "                genus_species = parts[1].strip()\n",
    "                protein_name = \" | \".join(p.strip() for p in parts[2:]) # Join remaining parts\n",
    "                    \n",
    "            elif len(parts) == 2:\n",
    "                # Might be ID|Protein Name or ID|Genus species\n",
    "                second_part = parts[1].strip()\n",
    "                # Heuristic: If it contains space and isn't all caps, assume species\n",
    "                if ' ' in second_part and not second_part.isupper(): \n",
    "                    genus_species = second_part\n",
    "                    protein_name = \"Unknown Protein (Genus/Species only in header)\"\n",
    "                else:\n",
    "                    genus_species = \"Unknown Species (Name only in header)\"\n",
    "                    protein_name = second_part\n",
    "\n",
    "            elif len(parts) == 1:\n",
    "                 # No pipes. Try splitting by space after the extracted ID key\n",
    "                 id_len = len(fasta_id_key)\n",
    "                 # Find the first space *after* the ID key in the original description\n",
    "                 first_space_after_id = description.find(' ', id_len)\n",
    "                 if first_space_after_id != -1:\n",
    "                     protein_name = description[first_space_after_id:].strip()\n",
    "                     genus_species = \"Unknown Species (Space delimited header)\"\n",
    "                 else:\n",
    "                     protein_name = \"Unknown Protein (ID only header)\"\n",
    "                     genus_species = \"Unknown Species (ID only header)\"\n",
    "\n",
    "            # Store using the explicitly extracted ID key\n",
    "            # Basic cleanup: replace underscores with spaces in genus_species if needed\n",
    "            if genus_species != \"Unknown Species\":\n",
    "                 genus_species = genus_species.replace('_', ' ')\n",
    "                 \n",
    "            header_info[fasta_id_key] = {\n",
    "                'Genus_Species': genus_species,\n",
    "                'Protein_Name': protein_name\n",
    "            }\n",
    "\n",
    "            if count % 50000 == 0:\n",
    "                logging.info(f\"  Processed {count:,} FASTA records...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error parsing FASTA file {fasta_file}: {e}\", exc_info=True) # Log traceback\n",
    "        \n",
    "    logging.info(f\"Finished parsing. Extracted info for {len(header_info):,} sequences.\")\n",
    "    if skipped_count > 0:\n",
    "        logging.warning(f\"Skipped {skipped_count} records due to missing/unparseable IDs.\")\n",
    "        \n",
    "    # Diagnostic: Print first 5 keys from the map\n",
    "    if header_info:\n",
    "        logging.info(\"First 5 keys extracted from FASTA headers (for verification):\")\n",
    "        keys_sample = list(header_info.keys())[:5]\n",
    "        logging.info(keys_sample)\n",
    "        \n",
    "    return header_info\n",
    "\n",
    "# --- Main Script Logic ---\n",
    "def main():\n",
    "    logging.info(\"--- Starting Eukaryotic Annotation Addition Script (v1.2) ---\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 1. Load Main Database\n",
    "    logging.info(f\"Loading main database: {db_with_hits_path}\")\n",
    "    if not db_with_hits_path.is_file():\n",
    "        logging.error(f\"Database file not found: {db_with_hits_path}\")\n",
    "        sys.exit(1)\n",
    "    try:\n",
    "        df_main = pd.read_csv(db_with_hits_path, low_memory=False)\n",
    "        logging.info(f\"Loaded database shape: {df_main.shape}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load database: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Ensure key columns exist\n",
    "    required_cols = [protein_id_col, group_col, hit_flag_col, euk_hit_sseqid_col, euk_hit_organism_col]\n",
    "    if not all(col in df_main.columns for col in required_cols):\n",
    "        missing = [col for col in required_cols if col not in df_main.columns]\n",
    "        logging.error(f\"Database is missing required columns: {missing}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # 2. Parse Eukaryotic FASTA Headers\n",
    "    euk_header_map = parse_euk_fasta_headers(euk_fasta_path)\n",
    "    if not euk_header_map:\n",
    "        logging.warning(\"FASTA header map is empty. Cannot add annotations.\")\n",
    "        # Decide whether to exit or continue and save just the original data\n",
    "        # For now, let's save the original data if no map is generated\n",
    "        # sys.exit(1) \n",
    "\n",
    "    # 3. Add New Columns and Populate\n",
    "    logging.info(f\"Preparing to update '{euk_hit_organism_col}' and add '{euk_hit_protein_name_col}'\")\n",
    "    \n",
    "    # Add the new protein name column if it doesn't exist\n",
    "    if euk_hit_protein_name_col not in df_main.columns:\n",
    "        df_main[euk_hit_protein_name_col] = np.nan\n",
    "    else:\n",
    "        logging.info(f\"Column '{euk_hit_protein_name_col}' already exists. Values may be overwritten.\")\n",
    "        df_main[euk_hit_protein_name_col] = np.nan # Clear existing values before populating\n",
    "\n",
    "    # Get indices of Asgard proteins with hits\n",
    "    asgard_hit_indices = df_main[\n",
    "        (df_main[group_col] == 'Asgard') & (df_main[hit_flag_col] == True)\n",
    "    ].index\n",
    "\n",
    "    logging.info(f\"Processing {len(asgard_hit_indices):,} Asgard proteins with eukaryotic hits...\")\n",
    "    \n",
    "    update_count = 0\n",
    "    map_miss_count = 0\n",
    "    overwritten_org_count = 0\n",
    "    \n",
    "    # Diagnostic: Print first 5 Euk_Hit_SSEQID values to be looked up\n",
    "    if not asgard_hit_indices.empty:\n",
    "         logging.info(\"First 5 Euk_Hit_SSEQIDs from database to be looked up (for verification):\")\n",
    "         logging.info(df_main.loc[asgard_hit_indices[:5], euk_hit_sseqid_col].tolist())\n",
    "    \n",
    "    # Iterate only through relevant rows\n",
    "    for idx in asgard_hit_indices:\n",
    "        # Get the ID from the database; ensure it's a string and clean it\n",
    "        euk_sseqid_db = df_main.loc[idx, euk_hit_sseqid_col]\n",
    "        if pd.isna(euk_sseqid_db) or not isinstance(euk_sseqid_db, str):\n",
    "            continue \n",
    "        euk_sseqid_db = euk_sseqid_db.strip() # Clean whitespace\n",
    "        if not euk_sseqid_db:\n",
    "             continue\n",
    "             \n",
    "        # --- Removed per-line debug logging ---\n",
    "        # logging.debug(f\"Looking up DB ID: '{euk_sseqid_db}'\")\n",
    "\n",
    "        original_org_name = df_main.loc[idx, euk_hit_organism_col] # Get original name for comparison\n",
    "        \n",
    "        # Look up directly using the cleaned ID from the database\n",
    "        hit_info = euk_header_map.get(euk_sseqid_db) \n",
    "        \n",
    "        if hit_info:\n",
    "            new_genus_species = hit_info.get('Genus_Species', 'Not Found in FASTA')\n",
    "            new_protein_name = hit_info.get('Protein_Name', 'Not Found in FASTA')\n",
    "\n",
    "            # Update columns\n",
    "            # Check if we are overwriting a potentially useful existing organism name\n",
    "            if not pd.isna(original_org_name) and original_org_name not in ['Unknown', 'Unknown Organism'] and original_org_name != new_genus_species:\n",
    "                 # Log only if overwriting a non-generic name\n",
    "                 if original_org_name != 'Not Found in FASTA':\n",
    "                    logging.debug(f\"Overwriting existing organism '{original_org_name}' with '{new_genus_species}' for hit {euk_sseqid_db}\")\n",
    "                    overwritten_org_count += 1\n",
    "                 \n",
    "            df_main.loc[idx, euk_hit_organism_col] = new_genus_species\n",
    "            df_main.loc[idx, euk_hit_protein_name_col] = new_protein_name\n",
    "            update_count += 1\n",
    "        else:\n",
    "             map_miss_count += 1\n",
    "             # --- Removed per-line debug logging ---\n",
    "             # logging.debug(f\"Could not find header info for Euk_Hit_SSEQID: {euk_sseqid_db}\")\n",
    "             # Decide whether to clear existing info or leave it\n",
    "             df_main.loc[idx, euk_hit_protein_name_col] = 'Not Found in FASTA'\n",
    "             # Optionally clear organism name too if desired:\n",
    "             # df_main.loc[idx, euk_hit_organism_col] = 'Not Found in FASTA'\n",
    "\n",
    "\n",
    "        if (update_count + map_miss_count) % 25000 == 0 and (update_count + map_miss_count) > 0:\n",
    "             logging.info(f\"  Processed {update_count + map_miss_count:,} hits ({update_count:,} mapped, {map_miss_count:,} misses)...\")\n",
    "\n",
    "    logging.info(f\"Finished processing. Updated annotations for {update_count:,} hits.\")\n",
    "    if overwritten_org_count > 0:\n",
    "        logging.info(f\"  Note: Overwrote existing organism names from DIAMOND parsing with FASTA header info for {overwritten_org_count:,} hits.\")\n",
    "    if map_miss_count > 0:\n",
    "        logging.warning(f\"Could not find header information for {map_miss_count:,} Euk_Hit_SSEQIDs in the provided FASTA file.\")\n",
    "\n",
    "    # 4. Save Updated Database\n",
    "    logging.info(f\"Saving updated database to: {output_db_updated_path}\")\n",
    "    try:\n",
    "        # Ensure output directory exists\n",
    "        output_db_updated_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df_main.to_csv(output_db_updated_path, index=False)\n",
    "        logging.info(\"Successfully saved updated database.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save updated database: {e}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    logging.info(f\"--- Script finished in {end_time - start_time:.2f} seconds ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae38bea-9f07-4d60-bab1-b28709429ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "\"\"\"\n",
    "Updates the Euk_Hit_Organism column (to Genus species) and adds \n",
    "Euk_Hit_Protein_Name information to the main protein database by parsing \n",
    "headers from a eukaryotic proteome FASTA file.\n",
    "Version 1.4: Refined organism name parsing to Genus_species only.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "import sys\n",
    "import logging\n",
    "import re \n",
    "# Ensure Biopython is installed: pip install biopython\n",
    "try:\n",
    "    from Bio import SeqIO \n",
    "except ImportError:\n",
    "    print(\"ERROR: Biopython is required for this script. Please install it: pip install biopython\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# --- Setup Logging ---\n",
    "log_formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "logger = logging.getLogger()\n",
    "# Prevent adding handlers multiple times if script is re-run in interactive session\n",
    "if not logger.hasHandlers():\n",
    "    logger.setLevel(logging.INFO) \n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setFormatter(log_formatter)\n",
    "    logger.addHandler(console_handler)\n",
    "else:\n",
    "    logger.setLevel(logging.INFO) # Ensure level is INFO\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "# INPUT: Main database CSV (should contain Euk_Hit_SSEQID and Euk_Hit_Organism columns)\n",
    "# Make sure this is the correct starting point (e.g., v1.8 before adding FASTA annotations)\n",
    "db_with_hits_path = Path('proteome_database_combined_v1.9.csv') \n",
    "\n",
    "# INPUT: Eukaryotic proteome FASTA file with informative headers\n",
    "euk_fasta_path = Path('euk63_proteomes_final.fasta') \n",
    "\n",
    "# OUTPUT: Updated database CSV file\n",
    "# Adjusted filename to reflect the specific update\n",
    "output_db_updated_path = Path('proteome_database_combined_v2.0.csv') \n",
    "\n",
    "# --- Define Key Column Names ---\n",
    "protein_id_col = 'ProteinID'\n",
    "group_col = 'Group'\n",
    "hit_flag_col = 'Has_Euk_DIAMOND_Hit'\n",
    "euk_hit_sseqid_col = 'Euk_Hit_SSEQID' # The NCBI ID (e.g., XP_...) from DIAMOND\n",
    "euk_hit_organism_col = 'Euk_Hit_Organism' # Existing column to UPDATE\n",
    "\n",
    "# New column to add\n",
    "euk_hit_protein_name_col = 'Euk_Hit_Protein_Name'\n",
    "\n",
    "# --- Helper Function to Parse FASTA Headers (Refined Organism Parsing) ---\n",
    "def parse_euk_fasta_headers(fasta_file: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Parses a FASTA file to extract information from headers.\n",
    "    Extracts ID before the first pipe '|' or space ' '.\n",
    "    Extracts Genus species (first two words) for the organism name.\n",
    "    Assumes header format like: >ID|Genus species strain|Protein Name\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary mapping the extracted sequence ID (e.g., 'XP_020145405.1') to \n",
    "        {'Genus_Species': 'Genus species', 'Protein_Name': 'Protein Name'}\n",
    "    \"\"\"\n",
    "    logging.info(f\"Parsing FASTA headers from: {fasta_file}...\")\n",
    "    header_info = {}\n",
    "    count = 0\n",
    "    skipped_count = 0\n",
    "    \n",
    "    if not fasta_file.is_file():\n",
    "        logging.error(f\"FASTA file not found: {fasta_file}\")\n",
    "        return header_info\n",
    "\n",
    "    try:\n",
    "        for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "            count += 1\n",
    "            description = record.description \n",
    "            \n",
    "            # --- ID Extraction (same as v1.2) ---\n",
    "            first_pipe = description.find('|')\n",
    "            first_space = description.find(' ')\n",
    "            end_of_id = -1\n",
    "            if first_pipe != -1 and first_space != -1: end_of_id = min(first_pipe, first_space)\n",
    "            elif first_pipe != -1: end_of_id = first_pipe\n",
    "            elif first_space != -1: end_of_id = first_space\n",
    "            fasta_id_key = description[:end_of_id].strip() if end_of_id != -1 else description.strip()\n",
    "            \n",
    "            if not fasta_id_key:\n",
    "                 skipped_count += 1\n",
    "                 logging.warning(f\"Could not extract valid ID Key from header: {description[:100]}...\")\n",
    "                 continue\n",
    "\n",
    "            # --- Organism and Protein Name Parsing ---\n",
    "            genus_species_raw = \"Unknown Species\"\n",
    "            protein_name = \"Unknown Protein\"\n",
    "            parts = description.split('|')\n",
    "\n",
    "            if len(parts) >= 3: \n",
    "                # Assumes ID|Genus species strain etc|Protein Name ...\n",
    "                genus_species_raw = parts[1].strip()\n",
    "                protein_name = \" | \".join(p.strip() for p in parts[2:]) \n",
    "            elif len(parts) == 2:\n",
    "                # Might be ID|Protein Name or ID|Genus species strain etc\n",
    "                second_part = parts[1].strip()\n",
    "                if ' ' in second_part and not second_part.isupper(): \n",
    "                    genus_species_raw = second_part\n",
    "                    protein_name = \"Unknown Protein (Genus/Species only in header)\"\n",
    "                else:\n",
    "                    genus_species_raw = \"Unknown Species (Name only in header)\"\n",
    "                    protein_name = second_part\n",
    "            elif len(parts) == 1:\n",
    "                 id_len = len(fasta_id_key)\n",
    "                 first_space_after_id = description.find(' ', id_len)\n",
    "                 if first_space_after_id != -1:\n",
    "                     # Assume the rest is protein name, cannot reliably get species\n",
    "                     protein_name = description[first_space_after_id:].strip()\n",
    "                     genus_species_raw = \"Unknown Species (Space delimited header)\"\n",
    "                 else:\n",
    "                     protein_name = \"Unknown Protein (ID only header)\"\n",
    "                     genus_species_raw = \"Unknown Species (ID only header)\"\n",
    "\n",
    "            # --- Refine Genus_Species ---\n",
    "            genus_species_cleaned = \"Unknown Species\"\n",
    "            if genus_species_raw != \"Unknown Species\":\n",
    "                # Replace underscores, split by space, take first two words\n",
    "                temp_name = genus_species_raw.replace('_', ' ').split()\n",
    "                if len(temp_name) >= 2:\n",
    "                    genus_species_cleaned = f\"{temp_name[0]} {temp_name[1]}\"\n",
    "                elif len(temp_name) == 1:\n",
    "                    genus_species_cleaned = temp_name[0] # Keep genus if only one word\n",
    "                else:\n",
    "                     genus_species_cleaned = genus_species_raw # Fallback if split fails unexpectedly\n",
    "            # --- End Refine Genus_Species ---\n",
    "                 \n",
    "            header_info[fasta_id_key] = {\n",
    "                'Genus_Species': genus_species_cleaned, # Store the cleaned version\n",
    "                'Protein_Name': protein_name\n",
    "            }\n",
    "\n",
    "            if count % 50000 == 0:\n",
    "                logging.info(f\"  Processed {count:,} FASTA records...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error parsing FASTA file {fasta_file}: {e}\", exc_info=True) \n",
    "        \n",
    "    logging.info(f\"Finished parsing. Extracted info for {len(header_info):,} sequences.\")\n",
    "    if skipped_count > 0:\n",
    "        logging.warning(f\"Skipped {skipped_count} records due to missing/unparseable IDs.\")\n",
    "        \n",
    "    # Diagnostic: Print first 5 keys and their parsed Genus_Species\n",
    "    if header_info:\n",
    "        logging.info(\"First 5 keys and parsed Genus_Species from FASTA headers (for verification):\")\n",
    "        keys_sample = list(header_info.keys())[:5]\n",
    "        for key in keys_sample:\n",
    "             logging.info(f\"  Key: '{key}', Parsed Genus_Species: '{header_info[key]['Genus_Species']}'\")\n",
    "        \n",
    "    return header_info\n",
    "\n",
    "# --- Main Script Logic ---\n",
    "def main():\n",
    "    logging.info(\"--- Starting Eukaryotic Annotation Addition Script (v1.4 - Genus/Species Update) ---\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 1. Load Main Database\n",
    "    logging.info(f\"Loading main database: {db_with_hits_path}\")\n",
    "    if not db_with_hits_path.is_file():\n",
    "        logging.error(f\"Database file not found: {db_with_hits_path}\")\n",
    "        sys.exit(1)\n",
    "    try:\n",
    "        df_main = pd.read_csv(db_with_hits_path, low_memory=False)\n",
    "        logging.info(f\"Loaded database shape: {df_main.shape}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load database: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Ensure key columns exist\n",
    "    required_cols = [protein_id_col, group_col, hit_flag_col, euk_hit_sseqid_col, euk_hit_organism_col]\n",
    "    if not all(col in df_main.columns for col in required_cols):\n",
    "        missing = [col for col in required_cols if col not in df_main.columns]\n",
    "        logging.error(f\"Database is missing required columns: {missing}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # 2. Parse Eukaryotic FASTA Headers\n",
    "    euk_header_map = parse_euk_fasta_headers(euk_fasta_path)\n",
    "    if not euk_header_map:\n",
    "        logging.warning(\"FASTA header map is empty. Cannot add annotations.\")\n",
    "        # Save the original data if no map is generated\n",
    "        output_db_updated_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df_main.to_csv(output_db_updated_path, index=False)\n",
    "        logging.info(f\"Saved original database to {output_db_updated_path} as FASTA map was empty.\")\n",
    "        sys.exit(0) \n",
    "\n",
    "    # 3. Add New Columns and Populate\n",
    "    logging.info(f\"Preparing to update '{euk_hit_organism_col}' (with Genus species) and add '{euk_hit_protein_name_col}'\")\n",
    "    \n",
    "    # Add the new protein name column if it doesn't exist\n",
    "    if euk_hit_protein_name_col not in df_main.columns:\n",
    "        df_main[euk_hit_protein_name_col] = np.nan\n",
    "    else:\n",
    "        logging.info(f\"Column '{euk_hit_protein_name_col}' already exists. Values will be overwritten.\")\n",
    "        df_main[euk_hit_protein_name_col] = np.nan # Clear existing values before populating\n",
    "\n",
    "    # Get indices of Asgard proteins with hits\n",
    "    asgard_hit_indices = df_main[\n",
    "        (df_main[group_col] == 'Asgard') & (df_main[hit_flag_col] == True)\n",
    "    ].index\n",
    "\n",
    "    logging.info(f\"Processing {len(asgard_hit_indices):,} Asgard proteins with eukaryotic hits...\")\n",
    "    \n",
    "    update_count = 0\n",
    "    map_miss_count = 0\n",
    "    overwritten_org_count = 0\n",
    "    \n",
    "    # Diagnostic: Print first 5 Euk_Hit_SSEQID values to be looked up\n",
    "    if not asgard_hit_indices.empty:\n",
    "         logging.info(\"First 5 Euk_Hit_SSEQIDs from database to be looked up (for verification):\")\n",
    "         logging.info(df_main.loc[asgard_hit_indices[:5], euk_hit_sseqid_col].tolist())\n",
    "    \n",
    "    # Iterate only through relevant rows\n",
    "    for idx in asgard_hit_indices:\n",
    "        # Get the ID from the database; ensure it's a string and clean it\n",
    "        euk_sseqid_db = df_main.loc[idx, euk_hit_sseqid_col]\n",
    "        if pd.isna(euk_sseqid_db) or not isinstance(euk_sseqid_db, str):\n",
    "            continue \n",
    "        euk_sseqid_db = euk_sseqid_db.strip() \n",
    "        if not euk_sseqid_db:\n",
    "             continue\n",
    "             \n",
    "        original_org_name = df_main.loc[idx, euk_hit_organism_col] \n",
    "        \n",
    "        # Look up directly using the cleaned ID from the database\n",
    "        hit_info = euk_header_map.get(euk_sseqid_db) \n",
    "        \n",
    "        if hit_info:\n",
    "            # Use the CLEANED Genus_Species from the map\n",
    "            new_genus_species = hit_info.get('Genus_Species', 'Not Found in FASTA') \n",
    "            new_protein_name = hit_info.get('Protein_Name', 'Not Found in FASTA')\n",
    "\n",
    "            # Update columns\n",
    "            if not pd.isna(original_org_name) and original_org_name not in ['Unknown', 'Unknown Organism'] and original_org_name != new_genus_species:\n",
    "                 if original_org_name != 'Not Found in FASTA':\n",
    "                    logging.debug(f\"Overwriting existing organism '{original_org_name}' with '{new_genus_species}' for hit {euk_sseqid_db}\")\n",
    "                    overwritten_org_count += 1\n",
    "                 \n",
    "            # Update the Euk_Hit_Organism column with the cleaned Genus_species\n",
    "            df_main.loc[idx, euk_hit_organism_col] = new_genus_species \n",
    "            df_main.loc[idx, euk_hit_protein_name_col] = new_protein_name\n",
    "            update_count += 1\n",
    "        else:\n",
    "             map_miss_count += 1\n",
    "             # Keep existing Euk_Hit_Organism if lookup fails, but mark protein name as not found\n",
    "             df_main.loc[idx, euk_hit_protein_name_col] = 'Not Found in FASTA'\n",
    "\n",
    "\n",
    "        if (update_count + map_miss_count) % 25000 == 0 and (update_count + map_miss_count) > 0:\n",
    "             logging.info(f\"  Processed {update_count + map_miss_count:,} hits ({update_count:,} mapped, {map_miss_count:,} misses)...\")\n",
    "\n",
    "    logging.info(f\"Finished processing. Updated annotations for {update_count:,} hits.\")\n",
    "    if overwritten_org_count > 0:\n",
    "        logging.info(f\"  Note: Overwrote existing organism names from DIAMOND parsing with FASTA header info for {overwritten_org_count:,} hits.\")\n",
    "    if map_miss_count > 0:\n",
    "        logging.warning(f\"Could not find header information for {map_miss_count:,} Euk_Hit_SSEQIDs in the provided FASTA file.\")\n",
    "\n",
    "    # 4. Save Updated Database\n",
    "    logging.info(f\"Saving updated database to: {output_db_updated_path}\")\n",
    "    try:\n",
    "        # Ensure output directory exists\n",
    "        output_db_updated_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df_main.to_csv(output_db_updated_path, index=False)\n",
    "        logging.info(\"Successfully saved updated database.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save updated database: {e}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    logging.info(f\"--- Script finished in {end_time - start_time:.2f} seconds ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2bb6f4-939c-48bc-b052-bfa8b9d21862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Integrate Giant Virus (GV) Eukaryotic DIAMOND Hit Information\n",
    "\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "import sys\n",
    "import os\n",
    "import numpy as np # For NaN\n",
    "import re # For parsing\n",
    "import logging # For detailed logging\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Setup Logging ---\n",
    "# Ensures messages are printed to the notebook output\n",
    "log_formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "logger = logging.getLogger(\"GV_Euk_Hit_Integration\") # Use a specific logger name\n",
    "logger.handlers = [] # Clear existing handlers for this logger if re-running cell\n",
    "logger.setLevel(logging.INFO) \n",
    "console_handler = logging.StreamHandler(sys.stdout)\n",
    "console_handler.setFormatter(log_formatter)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# --- Configuration ---\n",
    "MAIN_DB_INPUT_PATH = 'proteome_database_v2.3.csv'\n",
    "GV_DIAMOND_RESULTS_PATH = \"giant_virus_diamond_results/gv_vs_euk_diamond_hits.tsv\" \n",
    "EUK_FASTA_PATH = 'euk63_proteomes_final.fasta' # Make sure this path is correct\n",
    "MAIN_DB_OUTPUT_PATH = 'proteome_database_v2.4_gv_euk_hits.csv'\n",
    "E_VALUE_THRESHOLD = 1e-10\n",
    "\n",
    "# Define column names for DIAMOND output (12 standard + qlen, slen)\n",
    "DIAMOND_COLS = ['qseqid_full', 'sseqid_full', 'pident', 'length', 'mismatch', 'gapopen',\n",
    "                'qstart', 'qend', 'sstart', 'send', 'evalue', 'bitscore', 'qlen', 'slen']\n",
    "\n",
    "# --- Helper Function to Parse Eukaryotic FASTA Headers ---\n",
    "def parse_euk_fasta_headers_for_hits(fasta_file: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Parses a FASTA file to extract information from headers.\n",
    "    Extracts ID before the first pipe '|' or space ' '.\n",
    "    Attempts to extract Genus species and Protein Name.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Parsing Eukaryotic FASTA headers from: {fasta_file} for hit annotation...\")\n",
    "    header_info = {}\n",
    "    count = 0\n",
    "    skipped_count = 0\n",
    "    \n",
    "    if not fasta_file.is_file():\n",
    "        logger.error(f\"Eukaryotic FASTA file not found: {fasta_file}\")\n",
    "        return header_info\n",
    "\n",
    "    try:\n",
    "        for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "            count += 1\n",
    "            description = record.description \n",
    "            \n",
    "            first_pipe_idx = description.find('|')\n",
    "            first_space_idx = description.find(' ')\n",
    "            end_of_id = -1\n",
    "            if first_pipe_idx != -1 and (first_space_idx == -1 or first_pipe_idx < first_space_idx):\n",
    "                end_of_id = first_pipe_idx\n",
    "            elif first_space_idx != -1:\n",
    "                end_of_id = first_space_idx\n",
    "            fasta_id_key = description[:end_of_id].strip() if end_of_id != -1 else description.strip()\n",
    "            \n",
    "            if not fasta_id_key:\n",
    "                 skipped_count += 1\n",
    "                 logger.warning(f\"Could not extract valid ID Key from header: {description[:100]}...\")\n",
    "                 continue\n",
    "\n",
    "            genus_species = \"Unknown Species\"\n",
    "            protein_name = \"Unknown Protein\"\n",
    "            name_part = description[len(fasta_id_key):].strip().lstrip('|').strip() # Part after ID\n",
    "\n",
    "            organism_match = re.search(r\"\\[(.*?)\\]$\", name_part) # Check for [Genus species] at the end\n",
    "            if organism_match:\n",
    "                genus_species = organism_match.group(1).strip()\n",
    "                protein_name = name_part[:organism_match.start()].strip()\n",
    "                if not protein_name: protein_name = \"Unknown Protein (Organism in brackets)\"\n",
    "            else:\n",
    "                pipe_parts = name_part.split('|')\n",
    "                if len(pipe_parts) >= 1: # Take first part as protein name if no brackets\n",
    "                    protein_name = pipe_parts[0].strip()\n",
    "                    if len(pipe_parts) > 1: # If more parts, could be organism or more name\n",
    "                        # This part is heuristic; NCBI headers vary.\n",
    "                        # If a part after the first clearly looks like a species name, use it.\n",
    "                        # For now, we prioritize [Genus species] format.\n",
    "                        # If you have specific common formats for organism in pipe-delimited headers, add logic here.\n",
    "                        pass \n",
    "\n",
    "\n",
    "            if genus_species != \"Unknown Species\":\n",
    "                gs_parts = genus_species.replace('_', ' ').split()\n",
    "                genus_species = \" \".join(gs_parts[:2]) if len(gs_parts) >= 2 else gs_parts[0] if gs_parts else \"Unknown Species\"\n",
    "            \n",
    "            header_info[fasta_id_key] = {\n",
    "                'Genus_Species': genus_species,\n",
    "                'Protein_Name': protein_name if protein_name else \"Unknown Protein\"\n",
    "            }\n",
    "\n",
    "            if count % 200000 == 0: \n",
    "                logger.info(f\"  Processed {count:,} Eukaryotic FASTA records...\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error parsing Eukaryotic FASTA file {fasta_file}: {e}\", exc_info=True)\n",
    "        \n",
    "    logger.info(f\"Finished Eukaryotic FASTA parsing. Extracted info for {len(header_info):,} sequences.\")\n",
    "    if skipped_count > 0: logger.warning(f\"Skipped {skipped_count} Eukaryotic FASTA records due to ID parsing issues.\")\n",
    "    return header_info\n",
    "\n",
    "# --- Main Processing Logic ---\n",
    "logger.info(f\"--- Starting GV Eukaryotic Hit Integration ---\")\n",
    "logger.info(f\"Reading main database: {MAIN_DB_INPUT_PATH}\")\n",
    "try:\n",
    "    df_main = pd.read_csv(MAIN_DB_INPUT_PATH, low_memory=False)\n",
    "    df_main['ProteinID'] = df_main['ProteinID'].astype(str)\n",
    "    if 'Original_Seq_Length' not in df_main.columns:\n",
    "        logger.warning(\"'Original_Seq_Length' not found, trying 'Length' for query coverage.\")\n",
    "        if 'Length' in df_main.columns:\n",
    "             df_main['Original_Seq_Length'] = df_main['Length']\n",
    "        else:\n",
    "             logger.error(\"Neither 'Original_Seq_Length' nor 'Length' found. Cannot calculate query coverage accurately for GVs.\")\n",
    "             df_main['Original_Seq_Length'] = np.nan\n",
    "    logger.info(f\"Main database loaded. Shape: {df_main.shape}\")\n",
    "except FileNotFoundError:\n",
    "    logger.error(f\"Main database file not found: {MAIN_DB_INPUT_PATH}\")\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading main database: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "logger.info(f\"Reading GV DIAMOND results: {GV_DIAMOND_RESULTS_PATH}\")\n",
    "if not Path(GV_DIAMOND_RESULTS_PATH).is_file():\n",
    "    logger.error(f\"GV DIAMOND results file not found: {GV_DIAMOND_RESULTS_PATH}. Please provide the correct path.\")\n",
    "    sys.exit(1)\n",
    "try:\n",
    "    df_gv_hits_raw = pd.read_csv(GV_DIAMOND_RESULTS_PATH, sep='\\t', header=None, names=DIAMOND_COLS)\n",
    "    # Parse GV ProteinID from qseqid_full (e.g., take part before first '|')\n",
    "    df_gv_hits_raw['qseqid'] = df_gv_hits_raw['qseqid_full'].astype(str).str.split('|').str[0]\n",
    "    df_gv_hits_raw['sseqid_full'] = df_gv_hits_raw['sseqid_full'].astype(str)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading GV DIAMOND results: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "logger.info(f\"  Read {len(df_gv_hits_raw)} raw GV DIAMOND alignments.\")\n",
    "df_gv_hits_filtered = df_gv_hits_raw[df_gv_hits_raw['evalue'] <= E_VALUE_THRESHOLD].copy()\n",
    "logger.info(f\"  Found {len(df_gv_hits_filtered)} alignments passing e-value <= {E_VALUE_THRESHOLD}.\")\n",
    "\n",
    "if df_gv_hits_filtered.empty:\n",
    "    logger.warning(\"No GV DIAMOND hits passed the e-value threshold. No GV eukaryotic hits will be added.\")\n",
    "    df_main.to_csv(MAIN_DB_OUTPUT_PATH, index=False, na_rep='NA')\n",
    "    logger.info(f\"Saved original database (no GV hits added) to: {MAIN_DB_OUTPUT_PATH}\")\n",
    "else:\n",
    "    df_gv_hits_filtered.sort_values(by=['qseqid', 'evalue', 'bitscore'], ascending=[True, True, False], inplace=True)\n",
    "    df_gv_best_hits = df_gv_hits_filtered.drop_duplicates(subset=['qseqid'], keep='first').copy()\n",
    "    logger.info(f\"  Identified {len(df_gv_best_hits)} unique GV proteins with significant eukaryotic hits.\")\n",
    "\n",
    "    euk_header_map = parse_euk_fasta_headers_for_hits(Path(EUK_FASTA_PATH))\n",
    "    if not euk_header_map:\n",
    "        logger.warning(\"Eukaryotic FASTA header map is empty. Organism and protein names for GV hits will be 'Unknown'.\")\n",
    "\n",
    "    gv_hit_data_list = []\n",
    "    for _, row in df_gv_best_hits.iterrows():\n",
    "        gv_protein_id = row['qseqid'] # Already parsed clean GV ID\n",
    "        euk_sseqid_full = row['sseqid_full']\n",
    "        euk_sseqid_clean = euk_sseqid_full.split('|')[0].split(' ')[0]\n",
    "\n",
    "        header_data = euk_header_map.get(euk_sseqid_clean, {})\n",
    "        euk_organism = header_data.get('Genus_Species', 'Unknown Organism')\n",
    "        euk_prot_name = header_data.get('Protein_Name', 'Unknown Protein Name')\n",
    "\n",
    "        gv_protein_entry = df_main[df_main['ProteinID'] == gv_protein_id]\n",
    "        q_len_from_db = np.nan\n",
    "        if not gv_protein_entry.empty and 'Original_Seq_Length' in gv_protein_entry.columns and pd.notna(gv_protein_entry['Original_Seq_Length'].iloc[0]):\n",
    "            q_len_from_db = gv_protein_entry['Original_Seq_Length'].iloc[0]\n",
    "        if pd.isna(q_len_from_db) or q_len_from_db == 0: # Fallback to DIAMOND qlen if not found or zero\n",
    "             q_len_from_db = row['qlen']\n",
    "\n",
    "\n",
    "        query_cov = (row['qend'] - row['qstart'] + 1) / q_len_from_db if q_len_from_db > 0 else 0\n",
    "        subject_cov = (row['send'] - row['sstart'] + 1) / row['slen'] if row['slen'] > 0 else 0\n",
    "        \n",
    "        gv_hit_data_list.append({\n",
    "            'ProteinID': gv_protein_id,\n",
    "            'Has_Euk_DIAMOND_Hit': True, # This will be the final column name\n",
    "            'Euk_Hit_SSEQID': euk_sseqid_clean,\n",
    "            'Euk_Hit_Organism': euk_organism,\n",
    "            'Euk_Hit_PIDENT': row['pident'],\n",
    "            'Euk_Hit_EVALUE': row['evalue'],\n",
    "            'Euk_Hit_Protein_Name': euk_prot_name,\n",
    "            'Euk_Hit_Qstart': row['qstart'],\n",
    "            'Euk_Hit_Qend': row['qend'],\n",
    "            'Euk_Hit_Sstart': row['sstart'],\n",
    "            'Euk_Hit_Send': row['send'],\n",
    "            'Euk_Hit_Slen_Diamond': row['slen'],\n",
    "            'Query_Coverage': query_cov,\n",
    "            'Subject_Coverage': subject_cov\n",
    "        })\n",
    "    \n",
    "    df_gv_hits_to_update = pd.DataFrame(gv_hit_data_list)\n",
    "\n",
    "    if not df_gv_hits_to_update.empty:\n",
    "        logger.info(f\"Updating {len(df_gv_hits_to_update)} GV proteins with their best eukaryotic hit details.\")\n",
    "        \n",
    "        # Ensure target Euk_Hit columns exist in df_main, creating them if necessary\n",
    "        euk_hit_cols = [\n",
    "            'Has_Euk_DIAMOND_Hit', 'Euk_Hit_SSEQID', 'Euk_Hit_Organism', \n",
    "            'Euk_Hit_PIDENT', 'Euk_Hit_EVALUE', 'Euk_Hit_Protein_Name',\n",
    "            'Euk_Hit_Qstart', 'Euk_Hit_Qend', 'Euk_Hit_Sstart', 'Euk_Hit_Send',\n",
    "            'Euk_Hit_Slen_Diamond', 'Query_Coverage', 'Subject_Coverage'\n",
    "        ]\n",
    "        for col in euk_hit_cols:\n",
    "            if col not in df_main.columns:\n",
    "                df_main[col] = np.nan if col != 'Has_Euk_DIAMOND_Hit' else False\n",
    "                logger.info(f\"Added missing column to main DataFrame: {col}\")\n",
    "        \n",
    "        # Set ProteinID as index for efficient update\n",
    "        df_main = df_main.set_index('ProteinID')\n",
    "        df_gv_hits_to_update = df_gv_hits_to_update.set_index('ProteinID')\n",
    "\n",
    "        # Update only GV rows\n",
    "        gv_indices = df_main[df_main['Group'] == 'GV'].index\n",
    "        common_indices = gv_indices.intersection(df_gv_hits_to_update.index)\n",
    "        \n",
    "        logger.info(f\"Found {len(common_indices)} GV proteins with hits to update in df_main.\")\n",
    "\n",
    "        for col in df_gv_hits_to_update.columns: # Iterate through columns in the hits dataframe\n",
    "            df_main.loc[common_indices, col] = df_gv_hits_to_update.loc[common_indices, col]\n",
    "\n",
    "        # For GV proteins that did NOT have a hit, ensure 'Has_Euk_DIAMOND_Hit' is False\n",
    "        # And other Euk_Hit fields are NaN\n",
    "        gv_without_hits_indices = gv_indices.difference(df_gv_hits_to_update.index)\n",
    "        if not gv_without_hits_indices.empty:\n",
    "            logger.info(f\"Setting 'Has_Euk_DIAMOND_Hit' to False for {len(gv_without_hits_indices)} GV proteins without hits.\")\n",
    "            df_main.loc[gv_without_hits_indices, 'Has_Euk_DIAMOND_Hit'] = False\n",
    "            for col in euk_hit_cols: # Clear other Euk_Hit fields for GVs without hits\n",
    "                if col != 'Has_Euk_DIAMOND_Hit':\n",
    "                    df_main.loc[gv_without_hits_indices, col] = np.nan\n",
    "        \n",
    "        df_main.reset_index(inplace=True) # Restore ProteinID as a column\n",
    "        \n",
    "        num_gv_hits_in_main = df_main[(df_main['Group'] == 'GV') & (df_main['Has_Euk_DIAMOND_Hit'] == True)].shape[0]\n",
    "        logger.info(f\"Total GV proteins now flagged with eukaryotic hits in main DB: {num_gv_hits_in_main}\")\n",
    "    else:\n",
    "        logger.info(\"No GV eukaryotic hits to merge after processing.\")\n",
    "        # Ensure 'Has_Euk_DIAMOND_Hit' is False for all GVs if no hits were processed\n",
    "        df_main.loc[df_main['Group'] == 'GV', 'Has_Euk_DIAMOND_Hit'] = df_main.loc[df_main['Group'] == 'GV', 'Has_Euk_DIAMOND_Hit'].fillna(False)\n",
    "\n",
    "    try:\n",
    "        df_main.to_csv(MAIN_DB_OUTPUT_PATH, index=False, na_rep='NA')\n",
    "        logger.info(f\"Successfully saved updated database to: {MAIN_DB_OUTPUT_PATH}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error writing updated database CSV: {e}\")\n",
    "\n",
    "logger.info(\"--- GV Eukaryotic Hit Integration Cell Finished ---\")\n",
    "\n",
    "# --- Example of how to run this cell ---\n",
    "# Make sure the paths are correctly set above.\n",
    "# Then, you would typically run this cell in your Jupyter notebook.\n",
    "# For standalone script execution, you might wrap the main logic in a function\n",
    "# and call it from an if __name__ == \"__main__\": block, passing paths as arguments.\n",
    "# Example (if this were a script, not a notebook cell):\n",
    "# if __name__ == \"__main__\":\n",
    "#     add_gv_eukaryotic_hits(\n",
    "#         main_db_path='proteome_database_v2.3.csv',\n",
    "#         gv_diamond_results_path='YOUR_GV_vs_EUK_DIAMOND_RESULTS.tsv', # !!! REPLACE !!!\n",
    "#         euk_fasta_path_str='euk63_proteomes_final.fasta',\n",
    "#         output_db_path='proteome_database_v2.4_gv_euk_hits.csv'\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98c97a2-5373-43ea-974e-b8cf31bc056a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Integrate Giant Virus (GV) Eukaryotic DIAMOND Hit Information (v2 - Standardized Organism Names)\n",
    "\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "import sys\n",
    "import os\n",
    "import numpy as np # For NaN\n",
    "import re # For parsing\n",
    "import logging # For detailed logging\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Setup Logging ---\n",
    "log_formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "logger = logging.getLogger(\"GV_Euk_Hit_Integration_V2\") \n",
    "logger.handlers = [] \n",
    "logger.setLevel(logging.INFO) \n",
    "console_handler = logging.StreamHandler(sys.stdout)\n",
    "console_handler.setFormatter(log_formatter)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# --- Configuration ---\n",
    "MAIN_DB_INPUT_PATH = 'proteome_database_v2.3.csv' # Input is v2.3\n",
    "GV_DIAMOND_RESULTS_PATH = \"giant_virus_diamond_results/gv_vs_euk_diamond_hits.tsv\" \n",
    "EUK_FASTA_PATH = 'euk63_proteomes_final.fasta' \n",
    "MAIN_DB_OUTPUT_PATH = 'proteome_database_v2.5_gv_euk_hits_std_org.csv' # Output will be v2.5\n",
    "E_VALUE_THRESHOLD = 1e-10\n",
    "\n",
    "DIAMOND_COLS = ['qseqid_full', 'sseqid_full', 'pident', 'length', 'mismatch', 'gapopen',\n",
    "                'qstart', 'qend', 'sstart', 'send', 'evalue', 'bitscore', 'qlen', 'slen']\n",
    "\n",
    "# --- Helper Function to Parse Eukaryotic FASTA Headers (Refined for Genus species) ---\n",
    "def parse_euk_fasta_headers_for_hits_std_org(fasta_file: Path) -> dict:\n",
    "    logger.info(f\"Parsing Eukaryotic FASTA headers from: {fasta_file} for hit annotation (std org)...\")\n",
    "    header_info = {}\n",
    "    count = 0; skipped_count = 0\n",
    "    if not fasta_file.is_file():\n",
    "        logger.error(f\"Eukaryotic FASTA file not found: {fasta_file}\")\n",
    "        return header_info\n",
    "    try:\n",
    "        for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "            count += 1\n",
    "            description = record.description\n",
    "            \n",
    "            first_pipe_idx = description.find('|')\n",
    "            first_space_idx = description.find(' ')\n",
    "            end_of_id = -1\n",
    "            if first_pipe_idx != -1 and (first_space_idx == -1 or first_pipe_idx < first_space_idx):\n",
    "                end_of_id = first_pipe_idx\n",
    "            elif first_space_idx != -1:\n",
    "                end_of_id = first_space_idx\n",
    "            fasta_id_key = description[:end_of_id].strip() if end_of_id != -1 else description.strip()\n",
    "            \n",
    "            if not fasta_id_key:\n",
    "                 skipped_count += 1; continue\n",
    "\n",
    "            genus_species_raw = \"Unknown Species\"\n",
    "            protein_name = \"Unknown Protein\"\n",
    "            remaining_description = description[len(fasta_id_key):].lstrip().lstrip('|').strip()\n",
    "\n",
    "            organism_match_in_brackets = re.search(r\"\\[(.*?)\\]$\", remaining_description)\n",
    "            if organism_match_in_brackets:\n",
    "                genus_species_raw = organism_match_in_brackets.group(1).strip()\n",
    "                protein_name = remaining_description[:organism_match_in_brackets.start()].strip()\n",
    "                if not protein_name: protein_name = \"Unknown Protein (Organism in brackets)\"\n",
    "            else:\n",
    "                parts_after_id = remaining_description.split('|')\n",
    "                if len(parts_after_id) >= 2: # Format: ID|Organism Info|Protein Name\n",
    "                    genus_species_raw = parts_after_id[0].strip()\n",
    "                    protein_name = \" | \".join(p.strip() for p in parts_after_id[1:])\n",
    "                elif len(parts_after_id) == 1 and parts_after_id[0]: # Format: ID|Protein Name (or ID|Organism if only one word)\n",
    "                    # Heuristic: if it contains spaces, assume it's an organism name attempt\n",
    "                    if ' ' in parts_after_id[0] or '_' in parts_after_id[0]:\n",
    "                        genus_species_raw = parts_after_id[0].strip()\n",
    "                        protein_name = \"Unknown Protein (Organism only after ID)\"\n",
    "                    else: # Assume it's protein name\n",
    "                        protein_name = parts_after_id[0].strip()\n",
    "                elif not parts_after_id and remaining_description: \n",
    "                     protein_name = remaining_description\n",
    "            \n",
    "            genus_species_cleaned = \"Unknown Species\"\n",
    "            if genus_species_raw and genus_species_raw != \"Unknown Species\":\n",
    "                temp_name_parts = genus_species_raw.replace('_', ' ').split()\n",
    "                if len(temp_name_parts) >= 2:\n",
    "                    genus_species_cleaned = f\"{temp_name_parts[0]} {temp_name_parts[1]}\"\n",
    "                elif len(temp_name_parts) == 1:\n",
    "                    genus_species_cleaned = temp_name_parts[0]\n",
    "            \n",
    "            header_info[fasta_id_key] = {\n",
    "                'Genus_Species': genus_species_cleaned,\n",
    "                'Protein_Name': protein_name if protein_name else \"Unknown Protein\"\n",
    "            }\n",
    "            if count % 200000 == 0: logger.info(f\"  Processed {count:,} Eukaryotic FASTA records...\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error parsing Eukaryotic FASTA file {fasta_file}: {e}\", exc_info=True)\n",
    "    logger.info(f\"Finished Eukaryotic FASTA parsing. Extracted info for {len(header_info):,} sequences (Skipped: {skipped_count}).\")\n",
    "    return header_info\n",
    "\n",
    "# --- Main Processing Logic (largely same as before, uses new helper) ---\n",
    "logger.info(f\"--- Starting GV Eukaryotic Hit Integration (v2) ---\")\n",
    "# ... (rest of the GV hit integration logic from the previous cell, ensuring it calls \n",
    "#      parse_euk_fasta_headers_for_hits_std_org and saves to MAIN_DB_OUTPUT_PATH)\n",
    "# The following is a condensed version of that logic:\n",
    "\n",
    "logger.info(f\"Reading main database: {MAIN_DB_INPUT_PATH}\")\n",
    "try:\n",
    "    df_main = pd.read_csv(MAIN_DB_INPUT_PATH, low_memory=False)\n",
    "    df_main['ProteinID'] = df_main['ProteinID'].astype(str)\n",
    "    if 'Original_Seq_Length' not in df_main.columns:\n",
    "        df_main['Original_Seq_Length'] = df_main.get('Length', pd.Series(np.nan, index=df_main.index))\n",
    "    logger.info(f\"Main database loaded. Shape: {df_main.shape}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading main database: {e}\"); sys.exit(1)\n",
    "\n",
    "logger.info(f\"Reading GV DIAMOND results: {GV_DIAMOND_RESULTS_PATH}\")\n",
    "if not Path(GV_DIAMOND_RESULTS_PATH).is_file():\n",
    "    logger.error(f\"GV DIAMOND results file not found: {GV_DIAMOND_RESULTS_PATH}.\"); sys.exit(1)\n",
    "try:\n",
    "    df_gv_hits_raw = pd.read_csv(GV_DIAMOND_RESULTS_PATH, sep='\\t', header=None, names=DIAMOND_COLS)\n",
    "    df_gv_hits_raw['qseqid'] = df_gv_hits_raw['qseqid_full'].astype(str).str.split('|').str[0]\n",
    "    df_gv_hits_raw['sseqid_full'] = df_gv_hits_raw['sseqid_full'].astype(str)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading GV DIAMOND results: {e}\"); sys.exit(1)\n",
    "\n",
    "df_gv_hits_filtered = df_gv_hits_raw[df_gv_hits_raw['evalue'] <= E_VALUE_THRESHOLD].copy()\n",
    "logger.info(f\"  Found {len(df_gv_hits_filtered)} GV DIAMOND alignments passing e-value.\")\n",
    "\n",
    "if df_gv_hits_filtered.empty:\n",
    "    logger.warning(\"No GV DIAMOND hits passed e-value. Saving original DB.\")\n",
    "    df_main.to_csv(MAIN_DB_OUTPUT_PATH, index=False, na_rep='NA')\n",
    "else:\n",
    "    df_gv_hits_filtered.sort_values(by=['qseqid', 'evalue', 'bitscore'], ascending=[True, True, False], inplace=True)\n",
    "    df_gv_best_hits = df_gv_hits_filtered.drop_duplicates(subset=['qseqid'], keep='first').copy()\n",
    "    logger.info(f\"  Identified {len(df_gv_best_hits)} unique GV proteins with Euk hits.\")\n",
    "\n",
    "    euk_header_map = parse_euk_fasta_headers_for_hits_std_org(Path(EUK_FASTA_PATH)) # Call refined parser\n",
    "\n",
    "    gv_hit_data_list = []\n",
    "    for _, row in df_gv_best_hits.iterrows():\n",
    "        gv_protein_id = row['qseqid']\n",
    "        euk_sseqid_full = row['sseqid_full']\n",
    "        euk_sseqid_clean = euk_sseqid_full.split('|')[0].split(' ')[0]\n",
    "        header_data = euk_header_map.get(euk_sseqid_clean, {})\n",
    "        euk_organism = header_data.get('Genus_Species', 'Unknown Organism') # This will now be \"Genus species\"\n",
    "        euk_prot_name = header_data.get('Protein_Name', 'Unknown Protein Name')\n",
    "        \n",
    "        q_len_from_db_series = df_main.loc[df_main['ProteinID'] == gv_protein_id, 'Original_Seq_Length']\n",
    "        q_len_from_db = q_len_from_db_series.iloc[0] if not q_len_from_db_series.empty and pd.notna(q_len_from_db_series.iloc[0]) else row['qlen']\n",
    "        if pd.isna(q_len_from_db) or q_len_from_db == 0: q_len_from_db = row['qlen']\n",
    "\n",
    "        query_cov = (row['qend'] - row['qstart'] + 1) / q_len_from_db if q_len_from_db > 0 else 0\n",
    "        subject_cov = (row['send'] - row['sstart'] + 1) / row['slen'] if row['slen'] > 0 else 0\n",
    "        \n",
    "        gv_hit_data_list.append({\n",
    "            'ProteinID': gv_protein_id, 'Has_Euk_DIAMOND_Hit': True,\n",
    "            'Euk_Hit_SSEQID': euk_sseqid_clean, 'Euk_Hit_Organism': euk_organism,\n",
    "            'Euk_Hit_PIDENT': row['pident'], 'Euk_Hit_EVALUE': row['evalue'],\n",
    "            'Euk_Hit_Protein_Name': euk_prot_name, 'Euk_Hit_Qstart': row['qstart'],\n",
    "            'Euk_Hit_Qend': row['qend'], 'Euk_Hit_Sstart': row['sstart'],\n",
    "            'Euk_Hit_Send': row['send'], 'Euk_Hit_Slen_Diamond': row['slen'],\n",
    "            'Query_Coverage': query_cov, 'Subject_Coverage': subject_cov\n",
    "        })\n",
    "    \n",
    "    df_gv_hits_to_update = pd.DataFrame(gv_hit_data_list)\n",
    "\n",
    "    if not df_gv_hits_to_update.empty:\n",
    "        logger.info(f\"Updating {len(df_gv_hits_to_update)} GV proteins with Euk hit details.\")\n",
    "        euk_hit_cols = ['Has_Euk_DIAMOND_Hit', 'Euk_Hit_SSEQID', 'Euk_Hit_Organism', \n",
    "                        'Euk_Hit_PIDENT', 'Euk_Hit_EVALUE', 'Euk_Hit_Protein_Name',\n",
    "                        'Euk_Hit_Qstart', 'Euk_Hit_Qend', 'Euk_Hit_Sstart', 'Euk_Hit_Send',\n",
    "                        'Euk_Hit_Slen_Diamond', 'Query_Coverage', 'Subject_Coverage']\n",
    "        for col in euk_hit_cols:\n",
    "            if col not in df_main.columns:\n",
    "                df_main[col] = np.nan if col != 'Has_Euk_DIAMOND_Hit' else False\n",
    "        \n",
    "        df_main = df_main.set_index('ProteinID')\n",
    "        df_gv_hits_to_update = df_gv_hits_to_update.set_index('ProteinID')\n",
    "        gv_indices = df_main[df_main['Group'] == 'GV'].index\n",
    "        common_indices = gv_indices.intersection(df_gv_hits_to_update.index)\n",
    "        \n",
    "        for col in df_gv_hits_to_update.columns:\n",
    "            df_main.loc[common_indices, col] = df_gv_hits_to_update.loc[common_indices, col]\n",
    "        \n",
    "        gv_without_hits_indices = gv_indices.difference(df_gv_hits_to_update.index)\n",
    "        if not gv_without_hits_indices.empty:\n",
    "            df_main.loc[gv_without_hits_indices, 'Has_Euk_DIAMOND_Hit'] = False\n",
    "            for col in euk_hit_cols:\n",
    "                if col != 'Has_Euk_DIAMOND_Hit':\n",
    "                    df_main.loc[gv_without_hits_indices, col] = np.nan\n",
    "        df_main.reset_index(inplace=True)\n",
    "        logger.info(f\"Total GV proteins now flagged with Euk hits: {df_main[(df_main['Group'] == 'GV') & (df_main['Has_Euk_DIAMOND_Hit'] == True)].shape[0]}\")\n",
    "    else:\n",
    "        logger.info(\"No GV Euk hits to merge after processing.\")\n",
    "        df_main.loc[df_main['Group'] == 'GV', 'Has_Euk_DIAMOND_Hit'] = df_main.loc[df_main['Group'] == 'GV', 'Has_Euk_DIAMOND_Hit'].fillna(False)\n",
    "\n",
    "    try:\n",
    "        df_main.to_csv(MAIN_DB_OUTPUT_PATH, index=False, na_rep='NA')\n",
    "        logger.info(f\"Successfully saved updated database to: {MAIN_DB_OUTPUT_PATH}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error writing updated database CSV: {e}\")\n",
    "\n",
    "logger.info(\"--- GV Eukaryotic Hit Integration Cell (v2) Finished ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1049141d-fd13-4fb0-91cc-2cc696570a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Merge Intra-OG APSI Data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# Input: Path to the main database CSV created by a previous cell\n",
    "# Adjust this path to match the output of your main assembly cell\n",
    "MAIN_DB_INPUT_PATH = \"proteome_database_v2.5.csv\" \n",
    "\n",
    "# Input: Path to the APSI values CSV file\n",
    "APSI_CSV_PATH = \"output_summary_data_hit_validation_phase1/intra_og_apsi_values.csv\" # Make sure this path is correct\n",
    "\n",
    "# Output: Path to save the database CSV after merging APSI data\n",
    "# You can overwrite the input file or save to a new file\n",
    "MAIN_DB_OUTPUT_PATH = \"proteome_database_v2.6.csv\" \n",
    "\n",
    "# Column name in the main database DataFrame that contains the Orthogroup ID\n",
    "MAIN_DB_OG_COLUMN = \"Orthogroup\" # Verify this matches your main DataFrame\n",
    "\n",
    "# Column name in the APSI CSV file that contains the Orthogroup ID\n",
    "APSI_DB_OG_COLUMN = \"Orthogroup\" # Verify this matches the APSI CSV file\n",
    "\n",
    "# --- Setup Logging (Optional, but good practice) ---\n",
    "logger = logging.getLogger(__name__) \n",
    "# Basic configuration if not already set up in the notebook\n",
    "if not logger.hasHandlers():\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# --- Main Logic ---\n",
    "logger.info(\"--- Starting APSI Merge Cell ---\")\n",
    "logger.info(f\"Reading main database from: {MAIN_DB_INPUT_PATH}\")\n",
    "\n",
    "try:\n",
    "    # Load the main database CSV\n",
    "    df_main = pd.read_csv(MAIN_DB_INPUT_PATH, low_memory=False)\n",
    "    logger.info(f\"Loaded {len(df_main):,} records from main database.\")\n",
    "    logger.info(f\"Main DB columns: {df_main.columns.tolist()}\")\n",
    "\n",
    "    # Check if the main OG column exists\n",
    "    if MAIN_DB_OG_COLUMN not in df_main.columns:\n",
    "        logger.error(f\"Required column '{MAIN_DB_OG_COLUMN}' not found in {MAIN_DB_INPUT_PATH}. Cannot merge APSI.\")\n",
    "        raise KeyError(f\"Column '{MAIN_DB_OG_COLUMN}' not found.\")\n",
    "\n",
    "    logger.info(f\"Reading APSI data from: {APSI_CSV_PATH}\")\n",
    "    # Load the APSI data CSV\n",
    "    df_apsi = pd.read_csv(APSI_CSV_PATH)\n",
    "    logger.info(f\"Loaded {len(df_apsi):,} records from APSI file.\")\n",
    "    logger.info(f\"APSI DB columns: {df_apsi.columns.tolist()}\")\n",
    "\n",
    "    # Check if the APSI OG column exists\n",
    "    if APSI_DB_OG_COLUMN not in df_apsi.columns:\n",
    "        logger.error(f\"Required column '{APSI_DB_OG_COLUMN}' not found in {APSI_CSV_PATH}. Cannot merge.\")\n",
    "        raise KeyError(f\"Column '{APSI_DB_OG_COLUMN}' not found.\")\n",
    "\n",
    "    # Prepare APSI DataFrame for merging\n",
    "    # Rename columns for clarity and to avoid potential conflicts\n",
    "    df_apsi_renamed = df_apsi.rename(columns={\n",
    "        'APSI': 'Intra_OG_APSI',\n",
    "        'Num_Sequences': 'Num_OG_Sequences',\n",
    "        APSI_DB_OG_COLUMN: 'APSI_Orthogroup_Key' # Use a temporary unique key name\n",
    "    })\n",
    "    \n",
    "    # Select only necessary columns from APSI data\n",
    "    apsi_cols_to_merge = ['APSI_Orthogroup_Key', 'Intra_OG_APSI', 'Num_OG_Sequences']\n",
    "    df_apsi_to_merge = df_apsi_renamed[apsi_cols_to_merge]\n",
    "\n",
    "    # Perform the merge\n",
    "    logger.info(f\"Merging APSI data into main database on '{MAIN_DB_OG_COLUMN}' <-> '{APSI_DB_OG_COLUMN}'\")\n",
    "    # Keep track of original columns to see if merge adds columns unexpectedly\n",
    "    original_cols = set(df_main.columns)\n",
    "    \n",
    "    df_merged = df_main.merge(\n",
    "        df_apsi_to_merge,\n",
    "        left_on=MAIN_DB_OG_COLUMN,\n",
    "        right_on='APSI_Orthogroup_Key',\n",
    "        how='left' # Keep all rows from the main database\n",
    "    )\n",
    "\n",
    "    # Check if merge was successful and drop the temporary key\n",
    "    if 'Intra_OG_APSI' in df_merged.columns:\n",
    "        logger.info(\"Merge successful. Added 'Intra_OG_APSI' and 'Num_OG_Sequences' columns.\")\n",
    "        # Drop the temporary key column used for merging\n",
    "        if 'APSI_Orthogroup_Key' in df_merged.columns:\n",
    "            df_merged.drop(columns=['APSI_Orthogroup_Key'], inplace=True)\n",
    "        \n",
    "        # Report merge statistics\n",
    "        apsi_merged_count = df_merged['Intra_OG_APSI'].notna().sum()\n",
    "        total_rows = len(df_merged)\n",
    "        logger.info(f\"Number of proteins with merged APSI values: {apsi_merged_count} / {total_rows} ({apsi_merged_count/total_rows:.2%})\")\n",
    "        \n",
    "        # Check if the number of rows changed unexpectedly\n",
    "        if len(df_merged) != len(df_main):\n",
    "             logger.warning(f\"Row count changed after merge! Original: {len(df_main)}, Merged: {len(df_merged)}\")\n",
    "             \n",
    "    else:\n",
    "        logger.warning(\"Merge completed, but 'Intra_OG_APSI' column was not added. Check column names and merge keys.\")\n",
    "        # Ensure placeholder columns exist if merge failed to add them\n",
    "        if 'Intra_OG_APSI' not in df_merged.columns: df_merged['Intra_OG_APSI'] = pd.NA\n",
    "        if 'Num_OG_Sequences' not in df_merged.columns: df_merged['Num_OG_Sequences'] = pd.NA\n",
    "\n",
    "\n",
    "    # Save the updated DataFrame\n",
    "    logger.info(f\"Saving updated database with APSI info to: {MAIN_DB_OUTPUT_PATH}\")\n",
    "    df_merged.to_csv(MAIN_DB_OUTPUT_PATH, index=False, na_rep='NA')\n",
    "    logger.info(\"Successfully saved updated database.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    logger.error(f\"File not found error: {e}. Please check input paths.\")\n",
    "except KeyError as e:\n",
    "    logger.error(f\"Column name error: {e}. Please check column names in configuration and CSV files.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"An unexpected error occurred during APSI merge: {e}\", exc_info=True) # Log traceback\n",
    "\n",
    "logger.info(\"--- APSI Merge Cell Finished ---\")\n",
    "\n",
    "# Display the first few rows and info of the merged dataframe (optional)\n",
    "# print(\"\\nMerged DataFrame Info:\")\n",
    "# df_merged.info()\n",
    "# print(\"\\nMerged DataFrame Head:\")\n",
    "# print(df_merged.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7520476-f1ac-4518-8d36-a3d28d1db35d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (proteome_env)",
   "language": "python",
   "name": "proteome_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
